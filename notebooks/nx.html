<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"arxiv_id": "0704.0047", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "", "label": "", "shape": "dot", "size": 10, "title": "Title: ,\n Arxiv ID: 0704.0047"}, {"arxiv_id": "1512.03385", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "deep residual learning for image recognition", "label": "deep residual learning for image recognition", "shape": "dot", "size": 10, "title": "Title: deep residual learning for image recognition,\n Arxiv ID: 1512.03385"}, {"arxiv_id": "1709.01507", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "squeeze-and-excitation networks", "label": "squeeze-and-excitation networks", "shape": "dot", "size": 10, "title": "Title: squeeze-and-excitation networks,\n Arxiv ID: 1709.01507"}, {"arxiv_id": "1608.06993", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "densely connected convolutional networks", "label": "densely connected convolutional networks", "shape": "dot", "size": 10, "title": "Title: densely connected convolutional networks,\n Arxiv ID: 1608.06993"}, {"arxiv_id": "1610.02391", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "grad-cam: visual explanations from deep networks via gradient-based localization", "label": "grad-cam: visual explanations from deep networks via gradient-based localization", "shape": "dot", "size": 10, "title": "Title: grad-cam: visual explanations from deep networks via gradient-based localization,\n Arxiv ID: 1610.02391"}, {"arxiv_id": "2305.10084", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture", "label": "CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture", "shape": "dot", "size": 10, "title": "Title: CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture,\n Arxiv ID: 2305.10084"}, {"arxiv_id": "2305.10415", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering", "label": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering", "shape": "dot", "size": 10, "title": "Title: PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering,\n Arxiv ID: 2305.10415"}, {"arxiv_id": "2305.10764", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", "label": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", "shape": "dot", "size": 10, "title": "Title: OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding,\n Arxiv ID: 2305.10764"}, {"arxiv_id": "2305.10794", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Multi-spectral Class Center Network for Face Manipulation Detection and Localization", "label": "Multi-spectral Class Center Network for Face Manipulation Detection and Localization", "shape": "dot", "size": 10, "title": "Title: Multi-spectral Class Center Network for Face Manipulation Detection and Localization,\n Arxiv ID: 2305.10794"}, {"arxiv_id": "2305.10830", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Constructing a personalized AI assistant for shear wall layout using Stable Diffusion", "label": "Constructing a personalized AI assistant for shear wall layout using Stable Diffusion", "shape": "dot", "size": 10, "title": "Title: Constructing a personalized AI assistant for shear wall layout using Stable Diffusion,\n Arxiv ID: 2305.10830"}, {"arxiv_id": "2305.10853", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "LDM3D: Latent Diffusion Model for 3D", "label": "LDM3D: Latent Diffusion Model for 3D", "shape": "dot", "size": 10, "title": "Title: LDM3D: Latent Diffusion Model for 3D,\n Arxiv ID: 2305.10853"}, {"arxiv_id": "2305.10855", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "TextDiffuser: Diffusion Models as Text Painters", "label": "TextDiffuser: Diffusion Models as Text Painters", "shape": "dot", "size": 10, "title": "Title: TextDiffuser: Diffusion Models as Text Painters,\n Arxiv ID: 2305.10855"}, {"arxiv_id": "2305.11520", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "label": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "shape": "dot", "size": 10, "title": "Title: Late-Constraint Diffusion Guidance for Controllable Image Synthesis,\n Arxiv ID: 2305.11520"}, {"arxiv_id": "2305.11870", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models", "label": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models", "shape": "dot", "size": 10, "title": "Title: Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models,\n Arxiv ID: 2305.11870"}, {"arxiv_id": "2305.12082", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "SneakyPrompt: Jailbreaking Text-to-image Generative Models", "label": "SneakyPrompt: Jailbreaking Text-to-image Generative Models", "shape": "dot", "size": 10, "title": "Title: SneakyPrompt: Jailbreaking Text-to-image Generative Models,\n Arxiv ID: 2305.12082"}, {"arxiv_id": "2305.12223", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "What Makes for Good Visual Tokenizers for Large Language Models?", "label": "What Makes for Good Visual Tokenizers for Large Language Models?", "shape": "dot", "size": 10, "title": "Title: What Makes for Good Visual Tokenizers for Large Language Models?,\n Arxiv ID: 2305.12223"}, {"arxiv_id": "2305.10079", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "FACE RECOGNITION USING SYNTHETIC FACE DATA", "label": "FACE RECOGNITION USING SYNTHETIC FACE DATA", "shape": "dot", "size": 10, "title": "Title: FACE RECOGNITION USING SYNTHETIC FACE DATA,\n Arxiv ID: 2305.10079"}, {"arxiv_id": "2302.05543", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "adding conditional control to text-to-image diffusion models", "label": "adding conditional control to text-to-image diffusion models", "shape": "dot", "size": 10, "title": "Title: adding conditional control to text-to-image diffusion models,\n Arxiv ID: 2302.05543"}, {"arxiv_id": "2103.13413", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "vision transformers for dense prediction", "label": "vision transformers for dense prediction", "shape": "dot", "size": 10, "title": "Title: vision transformers for dense prediction,\n Arxiv ID: 2103.13413"}, {"arxiv_id": "2305.09664", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Understanding 3D Object Interaction from a Single Image", "label": "Understanding 3D Object Interaction from a Single Image", "shape": "dot", "size": 10, "title": "Title: Understanding 3D Object Interaction from a Single Image,\n Arxiv ID: 2305.09664"}, {"arxiv_id": "2305.09880", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "A survey of the Vision Transformers and its CNN-Transformer based Variants", "label": "A survey of the Vision Transformers and its CNN-Transformer based Variants", "shape": "dot", "size": 10, "title": "Title: A survey of the Vision Transformers and its CNN-Transformer based Variants,\n Arxiv ID: 2305.09880"}, {"arxiv_id": "2305.11676", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Learning Global-aware Kernel for Image Harmonization", "label": "Learning Global-aware Kernel for Image Harmonization", "shape": "dot", "size": 10, "title": "Title: Learning Global-aware Kernel for Image Harmonization,\n Arxiv ID: 2305.11676"}, {"arxiv_id": "2012.09841", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "taming transformers for high-resolution image synthesis", "label": "taming transformers for high-resolution image synthesis", "shape": "dot", "size": 10, "title": "Title: taming transformers for high-resolution image synthesis,\n Arxiv ID: 2012.09841"}, {"arxiv_id": "2305.11337", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture", "label": "RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture", "shape": "dot", "size": 10, "title": "Title: RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture,\n Arxiv ID: 2305.11337"}, {"arxiv_id": "2112.10752", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "high-resolution image synthesis with latent diffusion models", "label": "high-resolution image synthesis with latent diffusion models", "shape": "dot", "size": 10, "title": "Title: high-resolution image synthesis with latent diffusion models,\n Arxiv ID: 2112.10752"}, {"arxiv_id": "1606.00373", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "deeper depth prediction with fully convolutional residual networks", "label": "deeper depth prediction with fully convolutional residual networks", "shape": "dot", "size": 10, "title": "Title: deeper depth prediction with fully convolutional residual networks,\n Arxiv ID: 1606.00373"}, {"arxiv_id": null, "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "ViDaS: Video Depth-aware Saliency Network", "label": "ViDaS: Video Depth-aware Saliency Network", "shape": "dot", "size": 10, "title": "Title: ViDaS: Video Depth-aware Saliency Network,\n Arxiv ID: None"}, {"arxiv_id": "2305.11488", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning", "label": "AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning", "shape": "dot", "size": 10, "title": "Title: AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning,\n Arxiv ID: 2305.11488"}, {"arxiv_id": "2103.00020", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "learning transferable visual models from natural language supervision", "label": "learning transferable visual models from natural language supervision", "shape": "dot", "size": 10, "title": "Title: learning transferable visual models from natural language supervision,\n Arxiv ID: 2103.00020"}, {"arxiv_id": "2305.10701", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models", "label": "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models", "shape": "dot", "size": 10, "title": "Title: Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models,\n Arxiv ID: 2305.10701"}, {"arxiv_id": "2305.10328", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Joint Denoising and Few-angle Reconstruction for Low-dose Cardiac SPECT Using a Dual-domain Iterative Network with Adaptive Data Consistency", "label": "Joint Denoising and Few-angle Reconstruction for Low-dose Cardiac SPECT Using a Dual-domain Iterative Network with Adaptive Data Consistency", "shape": "dot", "size": 10, "title": "Title: Joint Denoising and Few-angle Reconstruction for Low-dose Cardiac SPECT Using a Dual-domain Iterative Network with Adaptive Data Consistency,\n Arxiv ID: 2305.10328"}, {"arxiv_id": "1505.04597", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "u-net: convolutional networks for biomedical image segmentation", "label": "u-net: convolutional networks for biomedical image segmentation", "shape": "dot", "size": 10, "title": "Title: u-net: convolutional networks for biomedical image segmentation,\n Arxiv ID: 1505.04597"}, {"arxiv_id": "2112.10741", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "label": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "shape": "dot", "size": 10, "title": "Title: glide: towards photorealistic image generation and editing with text-guided diffusion models,\n Arxiv ID: 2112.10741"}, {"arxiv_id": "2305.10874", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "label": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "shape": "dot", "size": 10, "title": "Title: VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation,\n Arxiv ID: 2305.10874"}, {"arxiv_id": "2305.11577", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "label": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "shape": "dot", "size": 10, "title": "Title: LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model,\n Arxiv ID: 2305.11577"}, {"arxiv_id": null, "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "fleet. video diffusion models", "label": "fleet. video diffusion models", "shape": "dot", "size": 10, "title": "Title: fleet. video diffusion models,\n Arxiv ID: None"}, {"arxiv_id": "2305.12252", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model", "label": "Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model", "shape": "dot", "size": 10, "title": "Title: Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model,\n Arxiv ID: 2305.12252"}, {"arxiv_id": "2205.11487", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "photorealistic text-to-image diffusion models with deep language understanding", "label": "photorealistic text-to-image diffusion models with deep language understanding", "shape": "dot", "size": 10, "title": "Title: photorealistic text-to-image diffusion models with deep language understanding,\n Arxiv ID: 2205.11487"}, {"arxiv_id": "1405.0312", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "microsoft coco: common objects in context", "label": "microsoft coco: common objects in context", "shape": "dot", "size": 10, "title": "Title: microsoft coco: common objects in context,\n Arxiv ID: 1405.0312"}, {"arxiv_id": "2305.11213", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Information-Ordered Bottlenecks for Adaptive Semantic Compression", "label": "Information-Ordered Bottlenecks for Adaptive Semantic Compression", "shape": "dot", "size": 10, "title": "Title: Information-Ordered Bottlenecks for Adaptive Semantic Compression,\n Arxiv ID: 2305.11213"}, {"arxiv_id": "2305.11443", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Equivariant Multi-Modality Image Fusion", "label": "Equivariant Multi-Modality Image Fusion", "shape": "dot", "size": 10, "title": "Title: Equivariant Multi-Modality Image Fusion,\n Arxiv ID: 2305.11443"}, {"arxiv_id": "2305.09566", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Ray-Patch: An Efficient Querying for Light Field Transformers", "label": "Ray-Patch: An Efficient Querying for Light Field Transformers", "shape": "dot", "size": 10, "title": "Title: Ray-Patch: An Efficient Querying for Light Field Transformers,\n Arxiv ID: 2305.09566"}, {"arxiv_id": "2305.10247", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?", "label": "Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?", "shape": "dot", "size": 10, "title": "Title: Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?,\n Arxiv ID: 2305.10247"}, {"arxiv_id": "1907.01341", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer", "label": "towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer", "shape": "dot", "size": 10, "title": "Title: towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer,\n Arxiv ID: 1907.01341"}, {"arxiv_id": "2305.10722", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Discriminative Diffusion Models as Few-shot Vision and Language Learners", "label": "Discriminative Diffusion Models as Few-shot Vision and Language Learners", "shape": "dot", "size": 10, "title": "Title: Discriminative Diffusion Models as Few-shot Vision and Language Learners,\n Arxiv ID: 2305.10722"}, {"arxiv_id": "2305.09699", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Mobile User Interface Element Detection Via Adaptively Prompt Tuning", "label": "Mobile User Interface Element Detection Via Adaptively Prompt Tuning", "shape": "dot", "size": 10, "title": "Title: Mobile User Interface Element Detection Via Adaptively Prompt Tuning,\n Arxiv ID: 2305.09699"}, {"arxiv_id": "2305.11481", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation", "label": "CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation", "shape": "dot", "size": 10, "title": "Title: CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation,\n Arxiv ID: 2305.11481"}, {"arxiv_id": "2305.11818", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "MaGIC: Multi-modality Guided Image Completion", "label": "MaGIC: Multi-modality Guided Image Completion", "shape": "dot", "size": 10, "title": "Title: MaGIC: Multi-modality Guided Image Completion,\n Arxiv ID: 2305.11818"}, {"arxiv_id": "2305.11615", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization", "label": "SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization", "shape": "dot", "size": 10, "title": "Title: SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization,\n Arxiv ID: 2305.11615"}, {"arxiv_id": "2305.12354", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization", "label": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization", "shape": "dot", "size": 10, "title": "Title: Bi-ViT: Pushing the Limit of Vision Transformer Quantization,\n Arxiv ID: 2305.12354"}, {"arxiv_id": "2210.02303", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "imagen video: high definition video generation with diffusion models", "label": "imagen video: high definition video generation with diffusion models", "shape": "dot", "size": 10, "title": "Title: imagen video: high definition video generation with diffusion models,\n Arxiv ID: 2210.02303"}, {"arxiv_id": "2305.10868", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation", "label": "Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation", "shape": "dot", "size": 10, "title": "Title: Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation,\n Arxiv ID: 2305.10868"}, {"arxiv_id": "2305.10135", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Controllable Mind Visual Diffusion Model", "label": "Controllable Mind Visual Diffusion Model", "shape": "dot", "size": 10, "title": "Title: Controllable Mind Visual Diffusion Model,\n Arxiv ID: 2305.10135"}, {"arxiv_id": "2305.11173", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Going Denser with Open-Vocabulary Part Segmentation", "label": "Going Denser with Open-Vocabulary Part Segmentation", "shape": "dot", "size": 10, "title": "Title: Going Denser with Open-Vocabulary Part Segmentation,\n Arxiv ID: 2305.11173"}, {"arxiv_id": "2305.09810", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "SEMI-SUPERVISED OBJECT DETECTION FOR SORGHUM PANICLES IN UAV IMAGERY", "label": "SEMI-SUPERVISED OBJECT DETECTION FOR SORGHUM PANICLES IN UAV IMAGERY", "shape": "dot", "size": 10, "title": "Title: SEMI-SUPERVISED OBJECT DETECTION FOR SORGHUM PANICLES IN UAV IMAGERY,\n Arxiv ID: 2305.09810"}, {"arxiv_id": "2305.11175", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "label": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "shape": "dot", "size": 10, "title": "Title: VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks,\n Arxiv ID: 2305.11175"}, {"arxiv_id": "2305.12344", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles", "label": "YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles", "shape": "dot", "size": 10, "title": "Title: YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles,\n Arxiv ID: 2305.12344"}, {"arxiv_id": "2305.10469", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "Object Segmentation by Mining Cross-Modal Semantics", "label": "Object Segmentation by Mining Cross-Modal Semantics", "shape": "dot", "size": 10, "title": "Title: Object Segmentation by Mining Cross-Modal Semantics,\n Arxiv ID: 2305.10469"}, {"arxiv_id": "2305.11918", "color": "#97c2fc", "font": {"color": "#10000000"}, "id": "PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation", "label": "PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation", "shape": "dot", "size": 10, "title": "Title: PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation,\n Arxiv ID: 2305.11918"}]);
                  edges = new vis.DataSet([{"category": "Is Evidence For", "explanation": "The cited works, ResNet, ResNext, SEResNext, DenseNet, and EfficientNet, have been dominant in ISIC skin lesion classification challenges, providing a benchmark for comparison and validation of the proposed lesion-focused deep learning method.", "from": "", "title": "Category: Is Evidence For,\n Explanation: The cited works, ResNet, ResNext, SEResNext, DenseNet, and EfficientNet, have been dominant in ISIC skin lesion classification challenges, providing a benchmark for comparison and validation of the proposed lesion-focused deep learning method.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, SEResNext, contributes to the understanding of CNN architectures that have excelled in skin lesion classification challenges, supporting the rationale behind utilizing deep learning methods in the proposed lesion-focused approach.", "from": "", "title": "Category: Is Evidence For,\n Explanation: The cited work, SEResNext, contributes to the understanding of CNN architectures that have excelled in skin lesion classification challenges, supporting the rationale behind utilizing deep learning methods in the proposed lesion-focused approach.", "to": "squeeze-and-excitation networks", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, DenseNet, is part of the CNN architectures that have demonstrated high performance in skin lesion classification challenges, providing a basis for the selection and evaluation of deep learning models in the study.", "from": "", "title": "Category: Is Evidence For,\n Explanation: The cited work, DenseNet, is part of the CNN architectures that have demonstrated high performance in skin lesion classification challenges, providing a basis for the selection and evaluation of deep learning models in the study.", "to": "densely connected convolutional networks", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work introduces Grad-CAM (Gradient-weighted Class Activation Mapping) as a visualization technique, which is employed in the study to highlight the important regions in skin lesion images that influence the decisions of convolutional neural networks.", "from": "", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work introduces Grad-CAM (Gradient-weighted Class Activation Mapping) as a visualization technique, which is employed in the study to highlight the important regions in skin lesion images that influence the decisions of convolutional neural networks.", "to": "grad-cam: visual explanations from deep networks via gradient-based localization", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited work contributes to the understanding of weed species impacting crops globally, which is essential for the creation of the hierarchical taxonomic system for the CWD30 dataset.", "from": "", "title": "Category: Supporting Evidence,\n Explanation: The cited work contributes to the understanding of weed species impacting crops globally, which is essential for the creation of the hierarchical taxonomic system for the CWD30 dataset.", "to": "CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited works, BLIP-2 and the open-source version of Flamingo, serve as benchmarks for evaluating multimodal models on natural images. Their struggle to answer MedVQA questions highlights the challenging nature of the dataset and its relevance in the biomedical field.", "from": "", "title": "Category: Supporting Evidence,\n Explanation: The cited works, BLIP-2 and the open-source version of Flamingo, serve as benchmarks for evaluating multimodal models on natural images. Their struggle to answer MedVQA questions highlights the challenging nature of the dataset and its relevance in the biomedical field.", "to": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper utilizes GPT-4 as a tool to filter out uninformative raw texts, demonstrating a methodological reliance on the technology for text processing.", "from": "", "title": "Category: Methodological Basis,\n Explanation: The citing paper utilizes GPT-4 as a tool to filter out uninformative raw texts, demonstrating a methodological reliance on the technology for text processing.", "to": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", "width": 1}, {"category": "Data Source", "explanation": "The cited work, FaceSwap, is utilized as a manipulation method within the FF++ dataset, adding to the range of forgeries examined in the study.", "from": "", "title": "Category: Data Source,\n Explanation: The cited work, FaceSwap, is utilized as a manipulation method within the FF++ dataset, adding to the range of forgeries examined in the study.", "to": "Multi-spectral Class Center Network for Face Manipulation Detection and Localization", "width": 1}, {"category": "Methodological Basis", "explanation": "The open-source Large Language Model Meta AI (LLaMA) cited in this paper serves as the basis for individuals to develop personalized reasoning assistants, indicating the methodological approach adopted in the citing paper.", "from": "", "title": "Category: Methodological Basis,\n Explanation: The open-source Large Language Model Meta AI (LLaMA) cited in this paper serves as the basis for individuals to develop personalized reasoning assistants, indicating the methodological approach adopted in the citing paper.", "to": "Constructing a personalized AI assistant for shear wall layout using Stable Diffusion", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper adopts the use of TouchDesigner to project RGB color images onto a 3D spherical object, leveraging the software\u0027s capabilities for immersive visualization.; The citing paper utilizes TouchDesigner as a platform to create immersive and interactive multimedia experiences, showcasing the potential of the LDM3D model in bringing text prompts to life in vivid detail.", "from": "", "title": "Category: Methodological Basis,\n Explanation: The citing paper adopts the use of TouchDesigner to project RGB color images onto a 3D spherical object, leveraging the software\u0027s capabilities for immersive visualization.; The citing paper utilizes TouchDesigner as a platform to create immersive and interactive multimedia experiences, showcasing the potential of the LDM3D model in bringing text prompts to life in vivid detail.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Extension or Continuation", "explanation": "DeepFolyd leverages a large-scale language model to enhance text-spelling knowledge, which the citing paper further extends in the context of text rendering for image generation.; DeepFloyd is mentioned for generating higher resolution images, and the citing paper extends this by using different super-resolution modules to improve image quality further.; DeepFloyd\u0027s cascaded pixel-based diffusion modules are expanded upon in the citing paper to generate images of increasing resolution. The citing paper utilizes the frozen text encoders based on T5 Transformer from DeepFloyd and implements the models using Hugging Face diffusers.", "from": "", "title": "Category: Extension or Continuation,\n Explanation: DeepFolyd leverages a large-scale language model to enhance text-spelling knowledge, which the citing paper further extends in the context of text rendering for image generation.; DeepFloyd is mentioned for generating higher resolution images, and the citing paper extends this by using different super-resolution modules to improve image quality further.; DeepFloyd\u0027s cascaded pixel-based diffusion modules are expanded upon in the citing paper to generate images of increasing resolution. The citing paper utilizes the frozen text encoders based on T5 Transformer from DeepFloyd and implements the models using Hugging Face diffusers.", "to": "TextDiffuser: Diffusion Models as Text Painters", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited works serve as benchmarks for comparison in evaluating the performance of LCDG against existing competitors, providing a basis for demonstrating the superiority of the proposed method.", "from": "", "title": "Category: Supporting Evidence,\n Explanation: The cited works serve as benchmarks for comparison in evaluating the performance of LCDG against existing competitors, providing a basis for demonstrating the superiority of the proposed method.", "to": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited works on diffusion models for text-to-image generation contribute to the citing paper\u0027s discussion by demonstrating the ability to produce high-quality images based on textual input, with specific emphasis on enhancing efficiency and operating in a lower-dimensional latent space.", "from": "", "title": "Category: Extension or Continuation,\n Explanation: The cited works on diffusion models for text-to-image generation contribute to the citing paper\u0027s discussion by demonstrating the ability to produce high-quality images based on textual input, with specific emphasis on enhancing efficiency and operating in a lower-dimensional latent space.", "to": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited work, Midjourney, blocks the generation of non-PG-13 images, which sets a precedent for exploring different categories of safety filters beyond the ones discussed in the citing paper.", "from": "", "title": "Category: Extension or Continuation,\n Explanation: The cited work, Midjourney, blocks the generation of non-PG-13 images, which sets a precedent for exploring different categories of safety filters beyond the ones discussed in the citing paper.", "to": "SneakyPrompt: Jailbreaking Text-to-image Generative Models", "width": 1}, {"category": "Theoretical Foundation", "explanation": "The citing paper leverages the instruction-tuned Vicuna-7B as the Language Model (LLM), building upon the theoretical framework established in [26] to train the model using language modeling loss.", "from": "", "title": "Category: Theoretical Foundation,\n Explanation: The citing paper leverages the instruction-tuned Vicuna-7B as the Language Model (LLM), building upon the theoretical framework established in [26] to train the model using language modeling loss.", "to": "What Makes for Good Visual Tokenizers for Large Language Models?", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work introduces the IResNet50 architecture, which is utilized as the backbone for training the models in the citing paper, providing a strong foundation for the model architecture.; The Resnet 100 backbone is referenced as part of the experimental setup, providing supporting evidence for the methodology used in the citing paper.", "from": "FACE RECOGNITION USING SYNTHETIC FACE DATA", "title": "Category: Is Evidence For,\n Explanation: The cited work introduces the IResNet50 architecture, which is utilized as the backbone for training the models in the citing paper, providing a strong foundation for the model architecture.; The Resnet 100 backbone is referenced as part of the experimental setup, providing supporting evidence for the methodology used in the citing paper.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper highlights the initial stages of personalized synthetic data and the need for further research to investigate the combination of rendered data and diffusion models for face recognition tasks.", "from": "FACE RECOGNITION USING SYNTHETIC FACE DATA", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper highlights the initial stages of personalized synthetic data and the need for further research to investigate the combination of rendered data and diffusion models for face recognition tasks.", "to": "adding conditional control to text-to-image diffusion models", "width": 1}, {"category": "Data Source", "explanation": "The cited works represent datasets and images used for training state-of-the-art depth estimation models. The citing paper acknowledges the reliance on these external datasets to evaluate the depth estimation performance of their own model, which was trained on a smaller dataset.", "from": "vision transformers for dense prediction", "title": "Category: Data Source,\n Explanation: The cited works represent datasets and images used for training state-of-the-art depth estimation models. The citing paper acknowledges the reliance on these external datasets to evaluate the depth estimation performance of their own model, which was trained on a smaller dataset.", "to": "Understanding 3D Object Interaction from a Single Image", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited work by Ranftl et al. serves as the methodological basis for the design of the dense prediction transformer (DPT) in the citing paper. It outlines the use of a ViT as the encoder and a CNN as the decoder, which the citing paper adopts in its segmentation approach.; The cited work by Ranftl et al. (2021) is used as a basis for lateral-layer integration in the citing paper, potentially influencing the methodology employed in the research.", "from": "vision transformers for dense prediction", "title": "Category: Methodological Basis,\n Explanation: The cited work by Ranftl et al. serves as the methodological basis for the design of the dense prediction transformer (DPT) in the citing paper. It outlines the use of a ViT as the encoder and a CNN as the decoder, which the citing paper adopts in its segmentation approach.; The cited work by Ranftl et al. (2021) is used as a basis for lateral-layer integration in the citing paper, potentially influencing the methodology employed in the research.", "to": "A survey of the Vision Transformers and its CNN-Transformer based Variants", "width": 1}, {"category": "Data Source", "explanation": "The citing paper utilizes depth maps from DPT-Large as a data source for its analysis on 512 \u00d7 512 images from the COCO validation dataset.; The cited works, DPT-Large depth estimation model, provide highly accurate relative depth estimates for each pixel in an image, which were crucial in fine-tuning the LDM3D model to generate realistic and immersive 360\u00b0views.", "from": "vision transformers for dense prediction", "title": "Category: Data Source,\n Explanation: The citing paper utilizes depth maps from DPT-Large as a data source for its analysis on 512 \u00d7 512 images from the COCO validation dataset.; The cited works, DPT-Large depth estimation model, provide highly accurate relative depth estimates for each pixel in an image, which were crucial in fine-tuning the LDM3D model to generate realistic and immersive 360\u00b0views.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited works focus on the pathways structure design for feature fusion using linear classic methods of summation or concatenation, which serves as the basis for the methods adopted in the citing paper.", "from": "Learning Global-aware Kernel for Image Harmonization", "title": "Category: Is Methodological Basis For,\n Explanation: The cited works focus on the pathways structure design for feature fusion using linear classic methods of summation or concatenation, which serves as the basis for the methods adopted in the citing paper.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited works demonstrate the successful use of Transformer in computer vision, inspiring the citing paper to explore new approaches for feature fusion by applying attention mechanisms instead of linear methods.", "from": "Learning Global-aware Kernel for Image Harmonization", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited works demonstrate the successful use of Transformer in computer vision, inspiring the citing paper to explore new approaches for feature fusion by applying attention mechanisms instead of linear methods.", "to": "taming transformers for high-resolution image synthesis", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited works introduce nonlinear approaches for feature fusion using attention mechanisms, which motivates the citing paper to further develop a dynamic weighted fusion method for incorporating global information in image harmonization.", "from": "Learning Global-aware Kernel for Image Harmonization", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited works introduce nonlinear approaches for feature fusion using attention mechanisms, which motivates the citing paper to further develop a dynamic weighted fusion method for incorporating global information in image harmonization.", "to": "squeeze-and-excitation networks", "width": 1}, {"category": "Is Theoretical Foundation For", "explanation": "The cited diffusion models serve as the theoretical foundation for allowing user-provided text prompts to guide the image synthesis process in the context of 2D content generation. This theoretical framework is extended to the generation of 3D scenes in the citing paper.", "from": "RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture", "title": "Category: Is Theoretical Foundation For,\n Explanation: The cited diffusion models serve as the theoretical foundation for allowing user-provided text prompts to guide the image synthesis process in the context of 2D content generation. This theoretical framework is extended to the generation of 3D scenes in the citing paper.", "to": "high-resolution image synthesis with latent diffusion models", "width": 1}, {"category": "Is Theoretical Foundation For", "explanation": "The cited diffusion models serve as the theoretical foundation for allowing user-provided text prompts to guide the image synthesis process in the context of 2D content generation. This theoretical framework is extended to the generation of 3D scenes in the citing paper.", "from": "RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture", "title": "Category: Is Theoretical Foundation For,\n Explanation: The cited diffusion models serve as the theoretical foundation for allowing user-provided text prompts to guide the image synthesis process in the context of 2D content generation. This theoretical framework is extended to the generation of 3D scenes in the citing paper.", "to": "adding conditional control to text-to-image diffusion models", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited works have shown great performance in depth estimation using deep learning models based on convolutional neural networks, providing a foundational basis for the deep learning approach adopted in the citing paper.", "from": "deeper depth prediction with fully convolutional residual networks", "title": "Category: Supporting Evidence,\n Explanation: The cited works have shown great performance in depth estimation using deep learning models based on convolutional neural networks, providing a foundational basis for the deep learning approach adopted in the citing paper.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Data Source", "explanation": "The cited methods and technologies provide the means for robust depth extraction from RGB frames, which is crucial for the methodology discussed in the citing paper.", "from": "deeper depth prediction with fully convolutional residual networks", "title": "Category: Data Source,\n Explanation: The cited methods and technologies provide the means for robust depth extraction from RGB frames, which is crucial for the methodology discussed in the citing paper.", "to": "ViDaS: Video Depth-aware Saliency Network", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited works offer methods that partially solve the task-agnostic setting issue, but are not lightweight enough. The citing paper builds upon these methods to further improve efficiency in continual learning algorithms.", "from": "AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited works offer methods that partially solve the task-agnostic setting issue, but are not lightweight enough. The citing paper builds upon these methods to further improve efficiency in continual learning algorithms.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work introduces the cosine distance function as a method for optimizing the keys in the image embedding process, which is adopted in the citing paper for its effectiveness.", "from": "AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work introduces the cosine distance function as a method for optimizing the keys in the image embedding process, which is adopted in the citing paper for its effectiveness.", "to": "grad-cam: visual explanations from deep networks via gradient-based localization", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited work, Stable Diffusion, serves as a foundational model for text-to-image tasks, and the citing paper extends this by demonstrating how to inject a backdoor trigger into the model. By building upon the core components of Stable Diffusion, the citing paper showcases a new application and functionality of conditional diffusion models.", "from": "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited work, Stable Diffusion, serves as a foundational model for text-to-image tasks, and the citing paper extends this by demonstrating how to inject a backdoor trigger into the model. By building upon the core components of Stable Diffusion, the citing paper showcases a new application and functionality of conditional diffusion models.", "to": "high-resolution image synthesis with latent diffusion models", "width": 1}, {"category": "Is Data Source For", "explanation": "The citing paper relies on the pre-trained CLIP model from the cited work to distinguish whether the concept in generated images is successfully modified by the backdoor.", "from": "Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models", "title": "Category: Is Data Source For,\n Explanation: The citing paper relies on the pre-trained CLIP model from the cited work to distinguish whether the concept in generated images is successfully modified by the backdoor.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper adopts the densely-connected CNN module from the cited work for spatial feature extraction in the proposed ADC.", "from": "Joint Denoising and Few-angle Reconstruction for Low-dose Cardiac SPECT Using a Dual-domain Iterative Network with Adaptive Data Consistency", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper adopts the densely-connected CNN module from the cited work for spatial feature extraction in the proposed ADC.", "to": "densely connected convolutional networks", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work introduces the U-Net architecture, which is adopted by the citing paper for the Projection-domain methods (UNet-Proj and AttnUNet-Proj) in the study.", "from": "Joint Denoising and Few-angle Reconstruction for Low-dose Cardiac SPECT Using a Dual-domain Iterative Network with Adaptive Data Consistency", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work introduces the U-Net architecture, which is adopted by the citing paper for the Projection-domain methods (UNet-Proj and AttnUNet-Proj) in the study.", "to": "u-net: convolutional networks for biomedical image segmentation", "width": 1}, {"category": "Extension or Continuation", "explanation": "The diffusion models showcased in the cited works for generating detailed images based on input conditions are further extended in the citing paper to explore the use of depth estimates as an additional condition for depth-to-image generation.", "from": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "title": "Category: Extension or Continuation,\n Explanation: The diffusion models showcased in the cited works for generating detailed images based on input conditions are further extended in the citing paper to explore the use of depth estimates as an additional condition for depth-to-image generation.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited works have made significant progress in image generation using diffusion models, which the citing paper builds upon to achieve state-of-the-art results in text rendering compared to previous GAN-based approaches.", "from": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "title": "Category: Extension or Continuation,\n Explanation: The cited works have made significant progress in image generation using diffusion models, which the citing paper builds upon to achieve state-of-the-art results in text rendering compared to previous GAN-based approaches.", "to": "TextDiffuser: Diffusion Models as Text Painters", "width": 1}, {"category": "Extension or Continuation", "explanation": "GLIDE is referenced as a work that extends text-to-image multi-modality generation using diffusion models, which motivates further advancements in the citing paper.", "from": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "title": "Category: Extension or Continuation,\n Explanation: GLIDE is referenced as a work that extends text-to-image multi-modality generation using diffusion models, which motivates further advancements in the citing paper.", "to": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited works have laid the foundation for text-to-image diffusion models, and the citing paper builds upon these works to showcase the plausible generation ability of such models in various tasks.", "from": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "title": "Category: Extension or Continuation,\n Explanation: The cited works have laid the foundation for text-to-image diffusion models, and the citing paper builds upon these works to showcase the plausible generation ability of such models in various tasks.", "to": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited works represent the foundational models (Textto-Image models) that serve as the basis for the approach proposed in the citing paper for handling complex reference-guided synthesis tasks.", "from": "glide: towards photorealistic image generation and editing with text-guided diffusion models", "title": "Category: Methodological Basis,\n Explanation: The cited works represent the foundational models (Textto-Image models) that serve as the basis for the approach proposed in the citing paper for handling complex reference-guided synthesis tasks.", "to": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited works highlight how semantic objective features can interfere with forgery cues in face manipulation localization, leading to the need for semantic-agnostic features in the citing paper.; The cited works contribute to the existing image forgery localization methods, providing foundational data and techniques that are relevant to the face manipulation localization discussed in the citing paper.; The cited works provide foundational methods for image forgery localization, but they only focus on fake image datasets rather than real-fake mixed datasets, highlighting the gap that the citing paper aims to address.", "from": "Multi-spectral Class Center Network for Face Manipulation Detection and Localization", "title": "Category: Is Evidence For,\n Explanation: The cited works highlight how semantic objective features can interfere with forgery cues in face manipulation localization, leading to the need for semantic-agnostic features in the citing paper.; The cited works contribute to the existing image forgery localization methods, providing foundational data and techniques that are relevant to the face manipulation localization discussed in the citing paper.; The cited works provide foundational methods for image forgery localization, but they only focus on fake image datasets rather than real-fake mixed datasets, highlighting the gap that the citing paper aims to address.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Extension or Continuation", "explanation": "The task of generating multiple images simultaneously using diffusion models, as discussed in the cited works, is further extended in the citing paper to propose a model that generates RGB images and depth maps simultaneously.", "from": "fleet. video diffusion models", "title": "Category: Extension or Continuation,\n Explanation: The task of generating multiple images simultaneously using diffusion models, as discussed in the cited works, is further extended in the citing paper to propose a model that generates RGB images and depth maps simultaneously.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited works introduce the concept of space-time separable architectures in text-to-video generation, which the citing paper adopts to reduce computational costs and utilize pretrained image generation models.", "from": "fleet. video diffusion models", "title": "Category: Methodological Basis,\n Explanation: The cited works introduce the concept of space-time separable architectures in text-to-video generation, which the citing paper adopts to reduce computational costs and utilize pretrained image generation models.", "to": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited works on text-to-image diffusion models trained on internet-scale data have demonstrated significant performance in conditioned image generation, which forms the basis for the citing paper\u0027s approach to generating realistic HOI images.", "from": "Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model", "title": "Category: Is Evidence For,\n Explanation: The cited works on text-to-image diffusion models trained on internet-scale data have demonstrated significant performance in conditioned image generation, which forms the basis for the citing paper\u0027s approach to generating realistic HOI images.", "to": "photorealistic text-to-image diffusion models with deep language understanding", "width": 1}, {"category": "Is Theoretical Foundation For", "explanation": "The cited work, CLIP model, is leveraged in the citing paper to extract comprehensive semantic representations aligning with text descriptors, serving as a theoretical foundation for the development of scene-aware adaptors and interaction classifiers.", "from": "Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model", "title": "Category: Is Theoretical Foundation For,\n Explanation: The cited work, CLIP model, is leveraged in the citing paper to extract comprehensive semantic representations aligning with text descriptors, serving as a theoretical foundation for the development of scene-aware adaptors and interaction classifiers.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, the SOTA detector trained on COCO, is used as a benchmark to show the performance of detecting humans and objects in SynHOI, indicating the similarity of data distribution between SynHOI and COCO.", "from": "Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model", "title": "Category: Is Evidence For,\n Explanation: The cited work, the SOTA detector trained on COCO, is used as a benchmark to show the performance of detecting humans and objects in SynHOI, indicating the similarity of data distribution between SynHOI and COCO.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper builds upon the concept of creating and understanding semantic latent spaces in DNNs, especially in the context of multimodal models and zero-shot inference.", "from": "Information-Ordered Bottlenecks for Adaptive Semantic Compression", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper builds upon the concept of creating and understanding semantic latent spaces in DNNs, especially in the context of multimodal models and zero-shot inference.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Data Source For", "explanation": "The cited work, MS-COCO 2017 Captioning dataset, serves as the primary data source for the study conducted in the citing paper. The dataset is used to generate CLIP embeddings and evaluate the compression quality of IOB autoencoders.", "from": "Information-Ordered Bottlenecks for Adaptive Semantic Compression", "title": "Category: Is Data Source For,\n Explanation: The cited work, MS-COCO 2017 Captioning dataset, serves as the primary data source for the study conducted in the citing paper. The dataset is used to generate CLIP embeddings and evaluate the compression quality of IOB autoencoders.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work, U-Net, is chosen as the backbone for the mappings A i and A v in the self-supervised learning framework of the citing paper. The methods and architecture of U-Net are adopted for the end-to-end training paradigm in the framework.; The citing paper adopts the U-Net model as the basis for the learnable pseudo-sensing module, which models the mapping from the fused image to the source images in the image fusion process.", "from": "Equivariant Multi-Modality Image Fusion", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work, U-Net, is chosen as the backbone for the mappings A i and A v in the self-supervised learning framework of the citing paper. The methods and architecture of U-Net are adopted for the end-to-end training paradigm in the framework.; The citing paper adopts the U-Net model as the basis for the learnable pseudo-sensing module, which models the mapping from the fused image to the source images in the image fusion process.", "to": "u-net: convolutional networks for biomedical image segmentation", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work presents the Res-block structure, which is employed in the CNN block of the citing paper for processing input features in parallel with the Restormer block.", "from": "Equivariant Multi-Modality Image Fusion", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work presents the Res-block structure, which is employed in the CNN block of the citing paper for processing input features in parallel with the Restormer block.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper extends the research by introducing a novel side loss to address depth ambiguity in left/right side regions of the human body during 3D reconstruction. This builds upon the initial SMPL-X mesh as a strong geometric prior to prevent unrealistic side-views in the optimized mesh.", "from": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper extends the research by introducing a novel side loss to address depth ambiguity in left/right side regions of the human body during 3D reconstruction. This builds upon the initial SMPL-X mesh as a strong geometric prior to prevent unrealistic side-views in the optimized mesh.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited works set the current state of the art in image generation by introducing the use of perceptual losses and adversarial discriminators. The citing paper extends this research by highlighting how reducing the computational cost of LFTs enables the utilization of more complex loss functions, thereby improving training and inference processes.; The citing paper suggests the possibility of employing attention upsamplers with Ray-Patch querying, extending the discussion beyond the cited works to explore different decoder options.", "from": "Ray-Patch: An Efficient Querying for Light Field Transformers", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited works set the current state of the art in image generation by introducing the use of perceptual losses and adversarial discriminators. The citing paper extends this research by highlighting how reducing the computational cost of LFTs enables the utilization of more complex loss functions, thereby improving training and inference processes.; The citing paper suggests the possibility of employing attention upsamplers with Ray-Patch querying, extending the discussion beyond the cited works to explore different decoder options.", "to": "taming transformers for high-resolution image synthesis", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited works set the current state of the art in image generation by introducing the use of perceptual losses and adversarial discriminators. The citing paper extends this research by highlighting how reducing the computational cost of LFTs enables the utilization of more complex loss functions, thereby improving training and inference processes.; The citing paper suggests the possibility of employing attention upsamplers with Ray-Patch querying, extending the discussion beyond the cited works to explore different decoder options.", "from": "Ray-Patch: An Efficient Querying for Light Field Transformers", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited works set the current state of the art in image generation by introducing the use of perceptual losses and adversarial discriminators. The citing paper extends this research by highlighting how reducing the computational cost of LFTs enables the utilization of more complex loss functions, thereby improving training and inference processes.; The citing paper suggests the possibility of employing attention upsamplers with Ray-Patch querying, extending the discussion beyond the cited works to explore different decoder options.", "to": "high-resolution image synthesis with latent diffusion models", "width": 1}, {"category": "Is Data Source For", "explanation": "The cited works highlight the common use of pretrained weights from ImageNet and COCO in deep learning models, underscoring their limitations for domain-specific agricultural tasks due to their generic content.", "from": "CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture", "title": "Category: Is Data Source For,\n Explanation: The cited works highlight the common use of pretrained weights from ImageNet and COCO in deep learning models, underscoring their limitations for domain-specific agricultural tasks due to their generic content.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper adopts the U-Net architecture introduced in the cited work as a foundational method for image segmentation tasks.", "from": "CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper adopts the U-Net architecture introduced in the cited work as a foundational method for image segmentation tasks.", "to": "u-net: convolutional networks for biomedical image segmentation", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, ResNet-101, provides a well-established deep learning model that the citing paper utilizes for feature extraction and classification tasks.", "from": "CWD30: A Comprehensive and Holistic Dataset for Crop Weed Recognition in Precision Agriculture", "title": "Category: Is Evidence For,\n Explanation: The cited work, ResNet-101, provides a well-established deep learning model that the citing paper utilizes for feature extraction and classification tasks.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Theoretical Foundation For", "explanation": "The cited works in the field of deep learning within computer vision provide the theoretical foundation for leveraging deep feature analysis in copy-move forgery detection, which is utilized in the citing paper to advance the research in this area.", "from": "Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?", "title": "Category: Is Theoretical Foundation For,\n Explanation: The cited works in the field of deep learning within computer vision provide the theoretical foundation for leveraging deep feature analysis in copy-move forgery detection, which is utilized in the citing paper to advance the research in this area.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Data Source For", "explanation": "The Microsoft COCO dataset is referenced as another source used to compile the USCISI dataset in the citing paper. This dataset contributes to the citing paper by providing additional image samples for the research.", "from": "Can Deep Network Balance Copy-Move Forgery Detection and Distinguishment?", "title": "Category: Is Data Source For,\n Explanation: The Microsoft COCO dataset is referenced as another source used to compile the USCISI dataset in the citing paper. This dataset contributes to the citing paper by providing additional image samples for the research.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Data Source For", "explanation": "The cited methods and technologies provide the means for robust depth extraction from RGB frames, which is crucial for the methodology discussed in the citing paper.; The cited work, MIDAS, is one of the three different depth extraction methods used in the study for comparing and assessing the relationship between depth estimation and saliency.", "from": "ViDaS: Video Depth-aware Saliency Network", "title": "Category: Is Data Source For,\n Explanation: The cited methods and technologies provide the means for robust depth extraction from RGB frames, which is crucial for the methodology discussed in the citing paper.; The cited work, MIDAS, is one of the three different depth extraction methods used in the study for comparing and assessing the relationship between depth estimation and saliency.", "to": "towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer", "width": 1}, {"category": "Methodological Basis", "explanation": "The work by Saharia et al. likely introduces novel methods or techniques related to diffusion models, which the citing paper adopts or adapts in its own methodology for modeling distributions and generating images.", "from": "photorealistic text-to-image diffusion models with deep language understanding", "title": "Category: Methodological Basis,\n Explanation: The work by Saharia et al. likely introduces novel methods or techniques related to diffusion models, which the citing paper adopts or adapts in its own methodology for modeling distributions and generating images.", "to": "Discriminative Diffusion Models as Few-shot Vision and Language Learners", "width": 1}, {"category": "Supporting Evidence", "explanation": "These citations highlight the foundational role of image-language models like CLIP in achieving remarkable performance through large-scale image-text pretraining, which forms the basis for the citing paper\u0027s exploration of applying CLIP to 3D vision tasks.", "from": "photorealistic text-to-image diffusion models with deep language understanding", "title": "Category: Supporting Evidence,\n Explanation: These citations highlight the foundational role of image-language models like CLIP in achieving remarkable performance through large-scale image-text pretraining, which forms the basis for the citing paper\u0027s exploration of applying CLIP to 3D vision tasks.", "to": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", "width": 1}, {"category": "Extension or Continuation", "explanation": "The diffusion models showcased in the cited works for generating detailed images based on input conditions are further extended in the citing paper to explore the use of depth estimates as an additional condition for depth-to-image generation.", "from": "photorealistic text-to-image diffusion models with deep language understanding", "title": "Category: Extension or Continuation,\n Explanation: The diffusion models showcased in the cited works for generating detailed images based on input conditions are further extended in the citing paper to explore the use of depth estimates as an additional condition for depth-to-image generation.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Data Source", "explanation": "The citing paper acknowledges the dataset used in the DrawBenchText study, indicating the reliance on external data for the evaluation conducted in the current research.; The citing paper utilizes DrawBenchText for evaluation purposes and Microsoft Read API to detect and recognize texts in generated images, providing a basis for the assessment of the model\u0027s performance.", "from": "photorealistic text-to-image diffusion models with deep language understanding", "title": "Category: Data Source,\n Explanation: The citing paper acknowledges the dataset used in the DrawBenchText study, indicating the reliance on external data for the evaluation conducted in the current research.; The citing paper utilizes DrawBenchText for evaluation purposes and Microsoft Read API to detect and recognize texts in generated images, providing a basis for the assessment of the model\u0027s performance.", "to": "TextDiffuser: Diffusion Models as Text Painters", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited works represent the foundational models (Textto-Image models) that serve as the basis for the approach proposed in the citing paper for handling complex reference-guided synthesis tasks.", "from": "photorealistic text-to-image diffusion models with deep language understanding", "title": "Category: Methodological Basis,\n Explanation: The cited works represent the foundational models (Textto-Image models) that serve as the basis for the approach proposed in the citing paper for handling complex reference-guided synthesis tasks.", "to": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited works provide evidence that freezing the text encoder T leads to improved performance, which the citing paper follows by also freezing T in their study.", "from": "Mobile User Interface Element Detection Via Adaptively Prompt Tuning", "title": "Category: Is Evidence For,\n Explanation: The cited works provide evidence that freezing the text encoder T leads to improved performance, which the citing paper follows by also freezing T in their study.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited works present the idea of single-stage object detection methods, which the citing paper utilizes to directly predict the category and bounding box of objects at each location without the need for separate detection and classification stages.", "from": "Mobile User Interface Element Detection Via Adaptively Prompt Tuning", "title": "Category: Is Methodological Basis For,\n Explanation: The cited works present the idea of single-stage object detection methods, which the citing paper utilizes to directly predict the category and bounding box of objects at each location without the need for separate detection and classification stages.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Theoretical Foundation For", "explanation": "The citing paper leverages the ViT structure pre-trained through unsupervised manner in the cited work to demonstrate strong scalability and effectiveness in achieving finer-grained cross-modality feature alignment in the CM-MaskSD framework.", "from": "CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation", "title": "Category: Is Theoretical Foundation For,\n Explanation: The citing paper leverages the ViT structure pre-trained through unsupervised manner in the cited work to demonstrate strong scalability and effectiveness in achieving finer-grained cross-modality feature alignment in the CM-MaskSD framework.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Evidence For", "explanation": "The MSCOCO dataset is referenced as a foundational dataset from which RefCOCO dataset is derived, providing a basis for the citing paper\u0027s research on referring expression comprehension.", "from": "CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation", "title": "Category: Is Evidence For,\n Explanation: The MSCOCO dataset is referenced as a foundational dataset from which RefCOCO dataset is derived, providing a basis for the citing paper\u0027s research on referring expression comprehension.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper adopts the hybrid architecture of UNet and ViT bottleneck from the cited work as the basis for the generator in the UVCGAN model.", "from": "A survey of the Vision Transformers and its CNN-Transformer based Variants", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper adopts the hybrid architecture of UNet and ViT bottleneck from the cited work as the basis for the generator in the UVCGAN model.", "to": "u-net: convolutional networks for biomedical image segmentation", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited work, MAT, is used as another benchmark for comparison to validate the effectiveness of the MaGIC method proposed in the citing paper. By referencing MAT, the citing paper extends the evaluation of its approach against multiple existing image completion methods.", "from": "MaGIC: Multi-modality Guided Image Completion", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited work, MAT, is used as another benchmark for comparison to validate the effectiveness of the MaGIC method proposed in the citing paper. By referencing MAT, the citing paper extends the evaluation of its approach against multiple existing image completion methods.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The work by Ranftl et al. (2022) was extended in the citing paper to estimate model performances using quantitative metrics on completing masked RGB images, exploring new dimensions in the research.", "from": "MaGIC: Multi-modality Guided Image Completion", "title": "Category: Is Extension or Continuation Of,\n Explanation: The work by Ranftl et al. (2022) was extended in the citing paper to estimate model performances using quantitative metrics on completing masked RGB images, exploring new dimensions in the research.", "to": "towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited work suggests the implementation of the squeeze-and-excitation module, which is utilized in the citing paper to induce sparsity in the model structure for finding the target subnetwork.", "from": "squeeze-and-excitation networks", "title": "Category: Methodological Basis,\n Explanation: The cited work suggests the implementation of the squeeze-and-excitation module, which is utilized in the citing paper to induce sparsity in the model structure for finding the target subnetwork.", "to": "SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited work, Q-ViT, is mentioned for further reference in the discussion of low-bit training-aware quantization methods, providing additional context for the evaluation of the proposed method.", "from": "squeeze-and-excitation networks", "title": "Category: Supporting Evidence,\n Explanation: The cited work, Q-ViT, is mentioned for further reference in the discussion of low-bit training-aware quantization methods, providing additional context for the evaluation of the proposed method.", "to": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization", "width": 1}, {"category": "Extension or Continuation", "explanation": "The task of generating multiple images simultaneously using diffusion models, as discussed in the cited works, is further extended in the citing paper to propose a model that generates RGB images and depth maps simultaneously.", "from": "imagen video: high definition video generation with diffusion models", "title": "Category: Extension or Continuation,\n Explanation: The task of generating multiple images simultaneously using diffusion models, as discussed in the cited works, is further extended in the citing paper to propose a model that generates RGB images and depth maps simultaneously.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Data Source", "explanation": "The works Imagen Video and Phenaki are cited as adopting joint image-video training, which serves as a data source for knowledge acquisition in text-to-video generation discussed in the citing paper.", "from": "imagen video: high definition video generation with diffusion models", "title": "Category: Data Source,\n Explanation: The works Imagen Video and Phenaki are cited as adopting joint image-video training, which serves as a data source for knowledge acquisition in text-to-video generation discussed in the citing paper.", "to": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, CLIP, addresses the dataset scale issue of 2D images, which is relevant to the discussion of limited 3D training datasets in the citing paper. This work serves as an example of how advancements in 2D image datasets can inform and potentially improve 3D shape understanding.; These citations highlight the foundational role of image-language models like CLIP in achieving remarkable performance through large-scale image-text pretraining, which forms the basis for the citing paper\u0027s exploration of applying CLIP to 3D vision tasks.", "from": "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", "title": "Category: Is Evidence For,\n Explanation: The cited work, CLIP, addresses the dataset scale issue of 2D images, which is relevant to the discussion of limited 3D training datasets in the citing paper. This work serves as an example of how advancements in 2D image datasets can inform and potentially improve 3D shape understanding.; These citations highlight the foundational role of image-language models like CLIP in achieving remarkable performance through large-scale image-text pretraining, which forms the basis for the citing paper\u0027s exploration of applying CLIP to 3D vision tasks.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper adopts the latent diffusion models presented in the cited works to encode images into low-dimensional latent spaces for computational efficiency.; The cited works introduce the use of VAE networks to encode images into low-dimensional latent spaces for computational efficiency, which the citing paper adopts in its TextDiffuser model.", "from": "TextDiffuser: Diffusion Models as Text Painters", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper adopts the latent diffusion models presented in the cited works to encode images into low-dimensional latent spaces for computational efficiency.; The cited works introduce the use of VAE networks to encode images into low-dimensional latent spaces for computational efficiency, which the citing paper adopts in its TextDiffuser model.", "to": "high-resolution image synthesis with latent diffusion models", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work presents the U-Net model that is pre-trained to map latent features to character-level segmentation masks, which is crucial for guiding the model\u0027s focus on text regions during training.; The citing paper employs a manually trained U-Net for character-level segmentation annotations based on the method presented in the cited work.", "from": "TextDiffuser: Diffusion Models as Text Painters", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work presents the U-Net model that is pre-trained to map latent features to character-level segmentation masks, which is crucial for guiding the model\u0027s focus on text regions during training.; The citing paper employs a manually trained U-Net for character-level segmentation annotations based on the method presented in the cited work.", "to": "u-net: convolutional networks for biomedical image segmentation", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "Stable Diffusion (SD) utilizes the CLIP text encoder from the cited work to obtain embeddings of user prompts, which is then used in the diffusion process for computational efficiency in the citing paper.; The citing paper utilizes the pre-trained CLIP model to obtain embeddings for given prompts, demonstrating the adoption of methods and techniques from the cited work to process and analyze data.; CLIPScore, based on CLIP, is employed in the citing paper to calculate the cosine similarity between image and text representations, contributing to the evaluation of text rendering quality.", "from": "TextDiffuser: Diffusion Models as Text Painters", "title": "Category: Is Methodological Basis For,\n Explanation: Stable Diffusion (SD) utilizes the CLIP text encoder from the cited work to obtain embeddings of user prompts, which is then used in the diffusion process for computational efficiency in the citing paper.; The citing paper utilizes the pre-trained CLIP model to obtain embeddings for given prompts, demonstrating the adoption of methods and techniques from the cited work to process and analyze data.; CLIPScore, based on CLIP, is employed in the citing paper to calculate the cosine similarity between image and text representations, contributing to the evaluation of text rendering quality.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "ControlNet is used in the citing paper to control diffusion models by adding conditions through zero-convolution layers. The citing paper relies on the public pre-trained model released by ControlNet authors and implements it using Hugging Face diffusers.; ControlNet is referenced for using the Canny map of printed text images generated with the first stage model as conditions, indicating the adoption of specific methods or techniques in the citing paper.", "from": "TextDiffuser: Diffusion Models as Text Painters", "title": "Category: Is Methodological Basis For,\n Explanation: ControlNet is used in the citing paper to control diffusion models by adding conditions through zero-convolution layers. The citing paper relies on the public pre-trained model released by ControlNet authors and implements it using Hugging Face diffusers.; ControlNet is referenced for using the Canny map of printed text images generated with the first stage model as conditions, indicating the adoption of specific methods or techniques in the citing paper.", "to": "adding conditional control to text-to-image diffusion models", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, CLIP, is utilized for its semantic guidance in the few-shot classification task. It supports the citing paper\u0027s strategy of incorporating visual semantics to improve model performance and mitigate semantic-aliasing in semantic segmentation models.", "from": "Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation", "title": "Category: Is Evidence For,\n Explanation: The cited work, CLIP, is utilized for its semantic guidance in the few-shot classification task. It supports the citing paper\u0027s strategy of incorporating visual semantics to improve model performance and mitigate semantic-aliasing in semantic segmentation models.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Data Source For", "explanation": "The COCO dataset is used in the citing paper to evaluate the model\u0027s performance, with a focus on the 80 categories and the training/test samples available in this dataset.", "from": "Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation", "title": "Category: Is Data Source For,\n Explanation: The COCO dataset is used in the citing paper to evaluate the model\u0027s performance, with a focus on the 80 categories and the training/test samples available in this dataset.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work utilizes resnet101 for visual encoding, which the citing paper follows to build their visual encoder, maintaining consistency with previous methods.", "from": "Advancing Incremental Few-shot Semantic Segmentation via Semantic-guided Relation Alignment and Adaptation", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work utilizes resnet101 for visual encoding, which the citing paper follows to build their visual encoder, maintaining consistency with previous methods.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper builds upon the research presented in the works of Ho, Jain, and Abbeel (2020), Song, Meng, and Ermon (2020), and Rombach et al. (2022) by defining a fixed forward Markov diffusion process for learning the data distribution. It extends the concept introduced in these works to further develop the CMVDM model.", "from": "Controllable Mind Visual Diffusion Model", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper builds upon the research presented in the works of Ho, Jain, and Abbeel (2020), Song, Meng, and Ermon (2020), and Rombach et al. (2022) by defining a fixed forward Markov diffusion process for learning the data distribution. It extends the concept introduced in these works to further develop the CMVDM model.", "to": "high-resolution image synthesis with latent diffusion models", "width": 1}, {"category": "Is Data Source For", "explanation": "The cited work, the Generic Object Decoding (GOD) dataset, provides the fMRI and image data pairs used in the citing paper\u0027s experiments, serving as the foundational dataset for the research.; The GOD dataset is utilized in the citing paper to provide paired fMRI signals and images for the research, enabling the investigation of brain signal decoding data.", "from": "Controllable Mind Visual Diffusion Model", "title": "Category: Is Data Source For,\n Explanation: The cited work, the Generic Object Decoding (GOD) dataset, provides the fMRI and image data pairs used in the citing paper\u0027s experiments, serving as the foundational dataset for the research.; The GOD dataset is utilized in the citing paper to provide paired fMRI signals and images for the research, enabling the investigation of brain signal decoding data.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Evidence For", "explanation": "The COCO dataset is integrated into the BOLD5000 dataset, offering a collection of images that enrich the analysis of neural encoding of visual information in the citing paper.", "from": "Controllable Mind Visual Diffusion Model", "title": "Category: Is Evidence For,\n Explanation: The COCO dataset is integrated into the BOLD5000 dataset, offering a collection of images that enrich the analysis of neural encoding of visual information in the citing paper.", "to": "microsoft coco: common objects in context", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper builds upon the prior text-to-image arts by incorporating 2\u00d7 down/upsampling along the spatial dimension to establish a hierarchical structure, extending the research in the field.", "from": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper builds upon the prior text-to-image arts by incorporating 2\u00d7 down/upsampling along the spatial dimension to establish a hierarchical structure, extending the research in the field.", "to": "high-resolution image synthesis with latent diffusion models", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work (CLIPSIM) provides a metric for evaluating CLIP similarity, which is used in the citing paper to demonstrate the effectiveness of the swapped cross-attention mechanism in improving text-video alignment.", "from": "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", "title": "Category: Is Evidence For,\n Explanation: The cited work (CLIPSIM) provides a metric for evaluating CLIP similarity, which is used in the citing paper to demonstrate the effectiveness of the swapped cross-attention mechanism in improving text-video alignment.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited work is used in the few-shot setting by the citing paper, where they train their Discriminative Stable Diffusion model with only 5% of the data, extending the research into a new training scenario.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Extension or Continuation,\n Explanation: The cited work is used in the few-shot setting by the citing paper, where they train their Discriminative Stable Diffusion model with only 5% of the data, extending the research into a new training scenario.", "to": "Discriminative Diffusion Models as Few-shot Vision and Language Learners", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited work, OpenCV, is utilized in this paper to pixelate architectural floor plans and shear wall floor plans, providing the methodological basis for the data processing technique employed in the study.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Methodological Basis,\n Explanation: The cited work, OpenCV, is utilized in this paper to pixelate architectural floor plans and shear wall floor plans, providing the methodological basis for the data processing technique employed in the study.", "to": "Constructing a personalized AI assistant for shear wall layout using Stable Diffusion", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited work provides the methodology for converting RGB and depth maps into virtual geometric dimensions, which the citing paper adopts in its rendering engine within TouchDesigner.; The cited work, Stable Diffusion v1.4, serves as the basis for the fine-tuning process of the LDM3D model in the citing paper. The weights from the Stable Diffusion model are used as a starting point for the fine-tuning, indicating a methodological dependency between the two works.; The cited work introduces the technique of training an autoencoder to generate a lower-dimensional data representation, which the citing paper adopts for fine-tuning the diffusion model, simplifying training and increasing efficiency.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Methodological Basis,\n Explanation: The cited work provides the methodology for converting RGB and depth maps into virtual geometric dimensions, which the citing paper adopts in its rendering engine within TouchDesigner.; The cited work, Stable Diffusion v1.4, serves as the basis for the fine-tuning process of the LDM3D model in the citing paper. The weights from the Stable Diffusion model are used as a starting point for the fine-tuning, indicating a methodological dependency between the two works.; The cited work introduces the technique of training an autoencoder to generate a lower-dimensional data representation, which the citing paper adopts for fine-tuning the diffusion model, simplifying training and increasing efficiency.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper relies on Stable Diffusion, a text-to-image generation model, as the methodological basis for part-level image editing. The model is specifically fine-tuned for image in-painting tasks, enabling zero-shot in-painting inference in the study.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Methodological Basis,\n Explanation: The citing paper relies on Stable Diffusion, a text-to-image generation model, as the methodological basis for part-level image editing. The model is specifically fine-tuned for image in-painting tasks, enabling zero-shot in-painting inference in the study.", "to": "Going Denser with Open-Vocabulary Part Segmentation", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited works serve as benchmarks for comparison in evaluating the performance of LCDG against existing competitors, providing a basis for demonstrating the superiority of the proposed method.; The citing paper compares its LCDG method to the SD* solution for stroke condition, providing a benchmark for evaluation and validation of the proposed approach.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Supporting Evidence,\n Explanation: The cited works serve as benchmarks for comparison in evaluating the performance of LCDG against existing competitors, providing a basis for demonstrating the superiority of the proposed method.; The citing paper compares its LCDG method to the SD* solution for stroke condition, providing a benchmark for evaluation and validation of the proposed approach.", "to": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "width": 1}, {"category": "Theoretical Foundation", "explanation": "The cited work, StableDiffusion2.0 (SD), forms the theoretical foundation for the proposed approach, LeftRefill, which reformulates reference-based synthesis as a contextual inpainting process.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Theoretical Foundation,\n Explanation: The cited work, StableDiffusion2.0 (SD), forms the theoretical foundation for the proposed approach, LeftRefill, which reformulates reference-based synthesis as a contextual inpainting process.", "to": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "width": 1}, {"category": "Methodological Basis", "explanation": "The cited work introduces a public CLIP text encoder that is utilized by the citing paper as a shadow text encoder for local access, demonstrating the adoption of methods from the cited work.", "from": "high-resolution image synthesis with latent diffusion models", "title": "Category: Methodological Basis,\n Explanation: The cited work introduces a public CLIP text encoder that is utilized by the citing paper as a shadow text encoder for local access, demonstrating the adoption of methods from the cited work.", "to": "SneakyPrompt: Jailbreaking Text-to-image Generative Models", "width": 1}, {"category": "Data Source", "explanation": "The COCO dataset is referenced as the basis for selecting the detection networks in the citing paper, providing the performance benchmark for evaluating the chosen methods.", "from": "microsoft coco: common objects in context", "title": "Category: Data Source,\n Explanation: The COCO dataset is referenced as the basis for selecting the detection networks in the citing paper, providing the performance benchmark for evaluating the chosen methods.", "to": "SEMI-SUPERVISED OBJECT DETECTION FOR SORGHUM PANICLES IN UAV IMAGERY", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper uses the MS-COCO dataset as the basis for evaluating text-to-image generation performance, indicating a methodological reliance on this dataset for the study.", "from": "microsoft coco: common objects in context", "title": "Category: Methodological Basis,\n Explanation: The citing paper uses the MS-COCO dataset as the basis for evaluating text-to-image generation performance, indicating a methodological reliance on this dataset for the study.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Data Source", "explanation": "The citing paper utilizes the COCO dataset for object-level detection data, highlighting the reliance on external data sources for analysis and results.; The cited works offer object detection data including object boxes and categories, which are crucial for the object-level analysis carried out in the citing paper.", "from": "microsoft coco: common objects in context", "title": "Category: Data Source,\n Explanation: The citing paper utilizes the COCO dataset for object-level detection data, highlighting the reliance on external data sources for analysis and results.; The cited works offer object detection data including object boxes and categories, which are crucial for the object-level analysis carried out in the citing paper.", "to": "Going Denser with Open-Vocabulary Part Segmentation", "width": 1}, {"category": "Data Source", "explanation": "The citing paper relies on the COCO2017 dataset for conducting object detection tasks, acknowledging the dataset as a foundational element in its research.; The citing paper utilizes the COCO 2017 dataset for object detection tasks, indicating that the data source from COCO 2017 is fundamental for the experiments and analysis conducted in the citing paper.; COCO2017 dataset is used for training and evaluation in object detection and instance segmentation tasks, providing the necessary data for the experiments conducted in the citing paper.", "from": "microsoft coco: common objects in context", "title": "Category: Data Source,\n Explanation: The citing paper relies on the COCO2017 dataset for conducting object detection tasks, acknowledging the dataset as a foundational element in its research.; The citing paper utilizes the COCO 2017 dataset for object detection tasks, indicating that the data source from COCO 2017 is fundamental for the experiments and analysis conducted in the citing paper.; COCO2017 dataset is used for training and evaluation in object detection and instance segmentation tasks, providing the necessary data for the experiments conducted in the citing paper.", "to": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited work provides quantitative comparison results on the COCO 2017 validation set, which serves as supporting evidence for the efficiency and effectiveness of the LCDG approach discussed in the citing paper.", "from": "microsoft coco: common objects in context", "title": "Category: Supporting Evidence,\n Explanation: The cited work provides quantitative comparison results on the COCO 2017 validation set, which serves as supporting evidence for the efficiency and effectiveness of the LCDG approach discussed in the citing paper.", "to": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited works focus on feature disentanglement, which complements the non-structure-based methods discussed in the citing paper by exploring new dimensions in separating representations of different variables in data.", "from": "microsoft coco: common objects in context", "title": "Category: Extension or Continuation,\n Explanation: The cited works focus on feature disentanglement, which complements the non-structure-based methods discussed in the citing paper by exploring new dimensions in separating representations of different variables in data.", "to": "SFP: Spurious Feature-targeted Pruning for Out-of-Distribution Generalization", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited work provides a benchmark dataset for Image Captioning, which is used by the citing paper to evaluate the performance of their model.", "from": "microsoft coco: common objects in context", "title": "Category: Supporting Evidence,\n Explanation: The cited work provides a benchmark dataset for Image Captioning, which is used by the citing paper to evaluate the performance of their model.", "to": "What Makes for Good Visual Tokenizers for Large Language Models?", "width": 1}, {"category": "Data Source", "explanation": "The cited work refers to the COCO dataset, which serves as the primary data source for the experimental procedures conducted in the citing paper. The dataset is utilized for training the model and validating the results, forming a crucial component of the research methodology.", "from": "microsoft coco: common objects in context", "title": "Category: Data Source,\n Explanation: The cited work refers to the COCO dataset, which serves as the primary data source for the experimental procedures conducted in the citing paper. The dataset is utilized for training the model and validating the results, forming a crucial component of the research methodology.", "to": "YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles", "width": 1}, {"category": "Methodological Basis", "explanation": "ReActNet\u0027s replacement of PReLU and sign function with RPReLU and RSign is foundational to the citing paper\u0027s approach to enhancing the performance of binarized neural networks.", "from": "microsoft coco: common objects in context", "title": "Category: Methodological Basis,\n Explanation: ReActNet\u0027s replacement of PReLU and sign function with RPReLU and RSign is foundational to the citing paper\u0027s approach to enhancing the performance of binarized neural networks.", "to": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper adopts the use of convolutional U-Net architectures, as introduced by Ronneberger et al. (2015a), for implementing diffusion probabilistic models in image generation tasks.", "from": "u-net: convolutional networks for biomedical image segmentation", "title": "Category: Methodological Basis,\n Explanation: The citing paper adopts the use of convolutional U-Net architectures, as introduced by Ronneberger et al. (2015a), for implementing diffusion probabilistic models in image generation tasks.", "to": "Discriminative Diffusion Models as Few-shot Vision and Language Learners", "width": 1}, {"category": "Theoretical Foundation", "explanation": "The cited work, U-Net backbone architecture, forms the basis of the generative diffusion model in the LDM3D, primarily composed of 2D convolutional layers for training on the learned, low-dimensional, KL-regularized latent space.", "from": "u-net: convolutional networks for biomedical image segmentation", "title": "Category: Theoretical Foundation,\n Explanation: The cited work, U-Net backbone architecture, forms the basis of the generative diffusion model in the LDM3D, primarily composed of 2D convolutional layers for training on the learned, low-dimensional, KL-regularized latent space.", "to": "LDM3D: Latent Diffusion Model for 3D", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited work, ResNet, serves as the visual backbone that is frozen in the language-guided image tokenizer during the second stage of training, allowing for the continuation of model development with improved efficiency.", "from": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited work, ResNet, serves as the visual backbone that is frozen in the language-guided image tokenizer during the second stage of training, allowing for the continuation of model development with improved efficiency.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, ControlNet, is referenced to highlight its failure in learning the correct spatial correlation between reference images and masked targets, even with enhancements. This supports the claims made in the citing paper regarding the advantages of LeftRefill in inpainting.", "from": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "title": "Category: Is Evidence For,\n Explanation: The cited work, ControlNet, is referenced to highlight its failure in learning the correct spatial correlation between reference images and masked targets, even with enhancements. This supports the claims made in the citing paper regarding the advantages of LeftRefill in inpainting.", "to": "adding conditional control to text-to-image diffusion models", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper utilizes the CLIP score introduced in the cited work as a method to evaluate the similarity between generated images and the target, providing a quantitative measure for the performance of the model.; The cited work introduces the prompt embedding technique, which the citing paper adopts for cross-attention in their model. This methodological approach is crucial for enhancing the performance of generative tasks like Ref-inpainting.", "from": "LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper utilizes the CLIP score introduced in the cited work as a method to evaluate the similarity between generated images and the target, providing a quantitative measure for the performance of the model.; The cited work introduces the prompt embedding technique, which the citing paper adopts for cross-attention in their model. This methodological approach is crucial for enhancing the performance of generative tasks like Ref-inpainting.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Data Source For", "explanation": "The cited work, CLIP, provides the dataset of 400 million image-text pairs used for training the model discussed in the citing paper. This dataset forms the basis for the visual encoder utilized in the research.", "from": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering", "title": "Category: Is Data Source For,\n Explanation: The cited work, CLIP, provides the dataset of 400 million image-text pairs used for training the model discussed in the citing paper. This dataset forms the basis for the visual encoder utilized in the research.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The cited work [19] on residual networks organizes the convolutional layers in YOLOv2, which is then utilized in YOLOv3 for improved detection.", "from": "YOLOv3 with Spatial Pyramid Pooling for Object Detection with Unmanned Aerial Vehicles", "title": "Category: Is Extension or Continuation Of,\n Explanation: The cited work [19] on residual networks organizes the convolutional layers in YOLOv2, which is then utilized in YOLOv3 for improved detection.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper compares its approach with ResNet MLP, indicating that both are trained on the same data with the same loss functions. ResNet MLP is noted to perform reasonably well on certain aspects but struggles with bounding box localization, leading the citing paper to rely on a detection pipeline like Mask R-CNN.; ResNet MLP serves as the methodological basis for feature extraction using a ResNet-50 encoder, which is then utilized in the citing paper for recognizing articulated objects and their types.", "from": "Understanding 3D Object Interaction from a Single Image", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper compares its approach with ResNet MLP, indicating that both are trained on the same data with the same loss functions. ResNet MLP is noted to perform reasonably well on certain aspects but struggles with bounding box localization, leading the citing paper to rely on a detection pipeline like Mask R-CNN.; ResNet MLP serves as the methodological basis for feature extraction using a ResNet-50 encoder, which is then utilized in the citing paper for recognizing articulated objects and their types.", "to": "deep residual learning for image recognition", "width": 1}, {"category": "Is Evidence For", "explanation": "The cited work, CLIP, serves as a benchmark model pretrained on massive image-text pairs via contrastive learning, demonstrating the effectiveness of such approaches in visual encoder pretraining.", "from": "What Makes for Good Visual Tokenizers for Large Language Models?", "title": "Category: Is Evidence For,\n Explanation: The cited work, CLIP, serves as a benchmark model pretrained on massive image-text pairs via contrastive learning, demonstrating the effectiveness of such approaches in visual encoder pretraining.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper adopts the ResNet-50 backbone from the cited work to support the implementation of the Soft Teacher framework in the semi-supervised learning approach.", "from": "deep residual learning for image recognition", "title": "Category: Methodological Basis,\n Explanation: The citing paper adopts the ResNet-50 backbone from the cited work to support the implementation of the Soft Teacher framework in the semi-supervised learning approach.", "to": "SEMI-SUPERVISED OBJECT DETECTION FOR SORGHUM PANICLES IN UAV IMAGERY", "width": 1}, {"category": "Supporting Evidence", "explanation": "The cited work, ResNet, serves as a backbone model for the performance comparison in the citing paper, showcasing the superiority of the proposed model over existing counterparts with the same backbone.", "from": "deep residual learning for image recognition", "title": "Category: Supporting Evidence,\n Explanation: The cited work, ResNet, serves as a backbone model for the performance comparison in the citing paper, showcasing the superiority of the proposed model over existing counterparts with the same backbone.", "to": "Object Segmentation by Mining Cross-Modal Semantics", "width": 1}, {"category": "Methodological Basis", "explanation": "The citing paper adopts the image encoder based on convolutional neural networks like ResNet as a foundational method for encoding images before further processing.; The citing paper adopts the ResNet50 model as the basis for its detector, which is later scaled up to Swin-B for improved performance on various datasets.", "from": "deep residual learning for image recognition", "title": "Category: Methodological Basis,\n Explanation: The citing paper adopts the image encoder based on convolutional neural networks like ResNet as a foundational method for encoding images before further processing.; The citing paper adopts the ResNet50 model as the basis for its detector, which is later scaled up to Swin-B for improved performance on various datasets.", "to": "Going Denser with Open-Vocabulary Part Segmentation", "width": 1}, {"category": "Data Source", "explanation": "The citing paper utilizes image features extracted by the ResNet-152 model as a foundational element for its analysis.", "from": "deep residual learning for image recognition", "title": "Category: Data Source,\n Explanation: The citing paper utilizes image features extracted by the ResNet-152 model as a foundational element for its analysis.", "to": "PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation", "width": 1}, {"category": "Is Theoretical Foundation For", "explanation": "The text embeddings extracted from the text encoder in CLIP, as mentioned in the cited work, are used in the classification score calculation of the recognition head, enhancing the understanding of object classes through text representations.", "from": "Going Denser with Open-Vocabulary Part Segmentation", "title": "Category: Is Theoretical Foundation For,\n Explanation: The text embeddings extracted from the text encoder in CLIP, as mentioned in the cited work, are used in the classification score calculation of the recognition head, enhancing the understanding of object classes through text representations.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper extends the research by analyzing the impacts of varying search space sizes on different metrics, providing further insights into the relationship between search space size, re-use bypass rate, and online queries in SneakyPrompt.", "from": "SneakyPrompt: Jailbreaking Text-to-image Generative Models", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper extends the research by analyzing the impacts of varying search space sizes on different metrics, providing further insights into the relationship between search space size, re-use bypass rate, and online queries in SneakyPrompt.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work introduces an approach for global least-squares fitting in disparity space, which the citing paper adopts to align depth maps produced by different models to a reference model.", "from": "LDM3D: Latent Diffusion Model for 3D", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work introduces an approach for global least-squares fitting in disparity space, which the citing paper adopts to align depth maps produced by different models to a reference model.", "to": "towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The work cited demonstrates the use of depth estimates in diffusion models for image generation, which is further built upon in the citing paper to propose a diffusion model that generates RGB images and depth maps simultaneously.", "from": "LDM3D: Latent Diffusion Model for 3D", "title": "Category: Is Extension or Continuation Of,\n Explanation: The work cited demonstrates the use of depth estimates in diffusion models for image generation, which is further built upon in the citing paper to propose a diffusion model that generates RGB images and depth maps simultaneously.", "to": "adding conditional control to text-to-image diffusion models", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work presents the VAE architecture used in the KL-autoencoder of the LDM3D model, with modifications made to specific Conv2d layers to suit the input format of concatenated RGB images and depth maps.", "from": "LDM3D: Latent Diffusion Model for 3D", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work presents the VAE architecture used in the KL-autoencoder of the LDM3D model, with modifications made to specific Conv2d layers to suit the input format of concatenated RGB images and depth maps.", "to": "taming transformers for high-resolution image synthesis", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The frozen CLIP-text encoder from the cited work is utilized for text conditioning in the LDM3D model, with cross-attention mapping text prompts to various layers of the U-Net for generating high-quality images and depth maps.", "from": "LDM3D: Latent Diffusion Model for 3D", "title": "Category: Is Methodological Basis For,\n Explanation: The frozen CLIP-text encoder from the cited work is utilized for text conditioning in the LDM3D model, with cross-attention mapping text prompts to various layers of the U-Net for generating high-quality images and depth maps.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The citing paper utilizes the text embeddings extracted using a pre-trained CLIP model as a basis for the denoising process in the text-to-image conditional generation.", "from": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "title": "Category: Is Methodological Basis For,\n Explanation: The citing paper utilizes the text embeddings extracted using a pre-trained CLIP model as a basis for the denoising process in the text-to-image conditional generation.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Is Extension or Continuation Of", "explanation": "The citing paper extends the research on autoaggressive models by exploring new dimensions or variables in the context of image synthesis.; The cited works on conditional generative methods are further investigated in the citing paper to guide the synthesis process using information from other domains.", "from": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis", "title": "Category: Is Extension or Continuation Of,\n Explanation: The citing paper extends the research on autoaggressive models by exploring new dimensions or variables in the context of image synthesis.; The cited works on conditional generative methods are further investigated in the citing paper to guide the synthesis process using information from other domains.", "to": "taming transformers for high-resolution image synthesis", "width": 1}, {"category": "Is Methodological Basis For", "explanation": "The cited work introduces the method of fine-tuning the CLIP model, which the citing paper adopts to improve the model\u0027s performance by tuning only the last layer.", "from": "Discriminative Diffusion Models as Few-shot Vision and Language Learners", "title": "Category: Is Methodological Basis For,\n Explanation: The cited work introduces the method of fine-tuning the CLIP model, which the citing paper adopts to improve the model\u0027s performance by tuning only the last layer.", "to": "learning transferable visual models from natural language supervision", "width": 1}, {"category": "Extension or Continuation", "explanation": "The citing paper further investigates the effects of using the CLIP-B/32 image feature extractor, building upon the analysis conducted in the cited work.", "from": "learning transferable visual models from natural language supervision", "title": "Category: Extension or Continuation,\n Explanation: The citing paper further investigates the effects of using the CLIP-B/32 image feature extractor, building upon the analysis conducted in the cited work.", "to": "PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation", "width": 1}, {"category": "Extension or Continuation", "explanation": "The cited works are used to evaluate the robustness of the model against noisy measurements and as a basis for using pseudo-depth inputs in the RGB-D and camouflage object detection tasks.", "from": "towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer", "title": "Category: Extension or Continuation,\n Explanation: The cited works are used to evaluate the robustness of the model against noisy measurements and as a basis for using pseudo-depth inputs in the RGB-D and camouflage object detection tasks.", "to": "Object Segmentation by Mining Cross-Modal Semantics", "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>