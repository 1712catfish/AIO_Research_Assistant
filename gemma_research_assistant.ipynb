{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building Gemma Research Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/GemmaAIO-main-image.webp\" alt=\"main-image\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey everyone, welcome to this notebook where we're diving into something super cool: building an all-in-one research chatbot using the power of the Gemma Large Language Model! ðŸš€\n",
    "\n",
    "Here's the game plan:\n",
    "\n",
    "- First up, Section 1: We're kicking things off with a research paper query engine. Imagine being able to find any research paper with just a simple chat. Sounds handy, right?\n",
    "\n",
    "- Moving on to Section 2: We'll spice things up with a graph paper relationship engine. This is all about connecting the dots between different papers and seeing the bigger picture.\n",
    "\n",
    "- Section 3: We'll add a basic data science assistant to our toolkit. This chatbot will help with all those tricky data questions, from stats to machine learning.\n",
    "\n",
    "- Section 4 is for the coders: We're building an AI code assistant that's going to be like your coding sidekick, helping you solve problems and understand complex codes.\n",
    "\n",
    "- And for the grand finale, Section 5: We're bringing it all together with a combination module. This is where we make sure everything works in harmony, giving you a powerhouse tool for any research or coding project.\n",
    "\n",
    "So, let's roll up our sleeves and jump into this exciting project. An overview of this project is below: ðŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/RAG%20-%20Scientific%20Assistant%20-%20Frame%201.jpg\" alt=\"pipeline\" width=800/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Scientific Research Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're focusing on creating the first part of our chatbot: a tool that can search through a huge number of research papers on arXiv. The key to this tool is using embeddings, taken from paper abstracts. Think of these as unique IDs that sum up what each paper is about.\n",
    "\n",
    "When you ask the chatbot something, it uses these embeddings to look through the abstracts and find papers that really match what you're looking for, not just by keywords, but by the actual ideas and concepts you're interested in. This is more about understanding the meaning of your question and finding papers that really match.\n",
    "\n",
    "We'll go through everything: picking the right papers from arXiv, getting the abstracts ready, and choosing a way to turn these abstracts into embeddings. Then, we'll set up a smart search that can quickly find the best matches when you ask a question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Science-Paper-Search.jpg\" alt=\"science paper search\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2455227, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \n",
       "0    A fully differential calculation in perturba...           hep-ph  \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG  \n",
       "2    The evolution of Earth-Moon system is descri...   physics.gen-ph  \n",
       "3    We show that a determinant of Stirling cycle...          math.CO  \n",
       "4    In this paper we show how to compute the $\\L...  math.CA math.FA  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/matthewmaddock/nlp-arxiv-dataset-transformers-and-umap\n",
    "\n",
    "# This takes about 1 minute.\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['id', 'title', 'abstract', 'categories']\n",
    "data = []\n",
    "file_name = '../data/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\n",
    "with open(file_name, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "        data.append(lst)\n",
    "\n",
    "df_data = pd.DataFrame(data=data, columns=cols)\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of almost 2,5M papers on arxiv, that's too much! However, not all of them are about AI, so let's narrow down to the topics we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0704.0047</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>The intelligent acoustic emission locator is...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0704.0050</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Part I describes an intelligent acoustic emi...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0704.0304</td>\n",
       "      <td>The World as Evolving Information</td>\n",
       "      <td>This paper discusses the benefits of describ...</td>\n",
       "      <td>cs.IT cs.AI math.IT q-bio.PE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>0704.0671</td>\n",
       "      <td>Learning from compressed observations</td>\n",
       "      <td>The problem of statistical learning is to co...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0704.0954</td>\n",
       "      <td>Sensor Networks with Random Links: Topology De...</td>\n",
       "      <td>In a sensor network, in practice, the commun...</td>\n",
       "      <td>cs.IT cs.LG math.IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443613</th>\n",
       "      <td>quant-ph/0411140</td>\n",
       "      <td>Improved Bounds on Quantum Learning Algorithms</td>\n",
       "      <td>In this article we give several new results ...</td>\n",
       "      <td>quant-ph cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445483</th>\n",
       "      <td>quant-ph/0507231</td>\n",
       "      <td>Algebras of Measurements: the logical structur...</td>\n",
       "      <td>In Quantum Physics, a measurement is represe...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448330</th>\n",
       "      <td>quant-ph/0607111</td>\n",
       "      <td>`Plausibilities of plausibilities': an approac...</td>\n",
       "      <td>Probability-like parameters appearing in som...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450042</th>\n",
       "      <td>quant-ph/0702072</td>\n",
       "      <td>Markovian Entanglement Networks</td>\n",
       "      <td>Graphical models of probabilistic dependenci...</td>\n",
       "      <td>quant-ph cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452122</th>\n",
       "      <td>quant-ph/9802028</td>\n",
       "      <td>Analogue Quantum Computers for Data Analysis</td>\n",
       "      <td>Analogue computers use continuous properties...</td>\n",
       "      <td>quant-ph cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336892 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                              title  \\\n",
       "46              0704.0047  Intelligent location of simultaneously active ...   \n",
       "49              0704.0050  Intelligent location of simultaneously active ...   \n",
       "303             0704.0304                  The World as Evolving Information   \n",
       "670             0704.0671              Learning from compressed observations   \n",
       "953             0704.0954  Sensor Networks with Random Links: Topology De...   \n",
       "...                   ...                                                ...   \n",
       "2443613  quant-ph/0411140     Improved Bounds on Quantum Learning Algorithms   \n",
       "2445483  quant-ph/0507231  Algebras of Measurements: the logical structur...   \n",
       "2448330  quant-ph/0607111  `Plausibilities of plausibilities': an approac...   \n",
       "2450042  quant-ph/0702072                    Markovian Entanglement Networks   \n",
       "2452122  quant-ph/9802028       Analogue Quantum Computers for Data Analysis   \n",
       "\n",
       "                                                  abstract  \\\n",
       "46         The intelligent acoustic emission locator is...   \n",
       "49         Part I describes an intelligent acoustic emi...   \n",
       "303        This paper discusses the benefits of describ...   \n",
       "670        The problem of statistical learning is to co...   \n",
       "953        In a sensor network, in practice, the commun...   \n",
       "...                                                    ...   \n",
       "2443613    In this article we give several new results ...   \n",
       "2445483    In Quantum Physics, a measurement is represe...   \n",
       "2448330    Probability-like parameters appearing in som...   \n",
       "2450042    Graphical models of probabilistic dependenci...   \n",
       "2452122    Analogue computers use continuous properties...   \n",
       "\n",
       "                           categories  \n",
       "46                        cs.NE cs.AI  \n",
       "49                        cs.NE cs.AI  \n",
       "303      cs.IT cs.AI math.IT q-bio.PE  \n",
       "670               cs.IT cs.LG math.IT  \n",
       "953               cs.IT cs.LG math.IT  \n",
       "...                               ...  \n",
       "2443613                quant-ph cs.LG  \n",
       "2445483                quant-ph cs.AI  \n",
       "2448330                quant-ph cs.AI  \n",
       "2450042                quant-ph cs.AI  \n",
       "2452122                quant-ph cs.CV  \n",
       "\n",
       "[336892 rows x 4 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "topics = ['cs.AI', 'cs.CV', 'cs.IR', 'cs.LG', 'cs.CL']\n",
    "\n",
    "# Create a regular expression pattern that matches any of the topics\n",
    "# The pattern will look like 'cs.AI|cs.CV|cs.IR|cs.LG|cs.CL'\n",
    "pattern = '|'.join(topics)\n",
    "\n",
    "# Filter the DataFrame to include rows where the 'categories' column contains any of the topics\n",
    "# The na=False parameter makes sure that NaN values are treated as False\n",
    "df_filtered = df_data[df_data['categories'].str.contains(pattern, na=False)]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we down to about 330K papers. Now, let's clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                    0707.0705\n",
       "title         Optimal Solutions for Sparse Principal Compone...\n",
       "abstract        Given a sample covariance matrix, we examine...\n",
       "categories                                          cs.AI cs.LG\n",
       "Name: 13875, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.iloc[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturbati...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pe...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions\\n We ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is describe...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle n...</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\Lam...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \\\n",
       "0  A fully differential calculation in perturbati...           hep-ph   \n",
       "1  We describe a new algorithm, the $(k,\\ell)$-pe...    math.CO cs.CG   \n",
       "2  The evolution of Earth-Moon system is describe...   physics.gen-ph   \n",
       "3  We show that a determinant of Stirling cycle n...          math.CO   \n",
       "4  In this paper we show how to compute the $\\Lam...  math.CA math.FA   \n",
       "\n",
       "                                       prepared_text  \n",
       "0  Calculation of prompt diphoton production cros...  \n",
       "1  Sparsity-certifying Graph Decompositions\\n We ...  \n",
       "2  The evolution of the Earth-Moon system based o...  \n",
       "3  A determinant of Stirling cycle numbers counts...  \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(x):\n",
    "    \n",
    "    # Replace newline characters with a space\n",
    "    new_text = \" \".join([c.strip() for c in x.replace(\"\\n\", \"\").split()])\n",
    "    # Remove leading and trailing spaces\n",
    "    new_text = new_text.strip()\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "df_data['title'] = df_data['title'].apply(clean_text)\n",
    "df_data['abstract'] = df_data['abstract'].apply(clean_text)\n",
    "\n",
    "df_data['prepared_text'] = df_data['title'] + '\\n ' + df_data['abstract']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "arxiv_documents = [Document(text=prepared_text, doc_id=id) for prepared_text,id in list(zip(df_data['prepared_text'], df_data['id']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Creating Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VectorStoreIndex` is by far the most frequently used type of Index in llamaindex. This class takes your Documents and splits them up into Nodes. Then, it creates `vector_embeddings` of the text of every node. But what is `vector_embedding`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector embeddings are like turning the essence of your words into a mathematical sketch. Imagine every idea or concept in your text getting its unique numerical fingerprint. This is handy because even if two snippets of text use different words, if they're sharing the same idea, their numerical sketchesâ€”or embeddingsâ€”will be close neighbors in the numerical space. This magic is done using tools known as embedding models.\n",
    "\n",
    "Choosing the right embedding model is crucial. It's like picking the right artist to paint your portrait; you want the one who captures you best. A great place to start is the MTEB leaderboard, where the crÃ¨me de la crÃ¨me of embedding models are ranked. As we have quite a large dataset, the model size matters, we don't want to wait all day for the model to extract all the vector embeddings. When I last checked, the `BAAI/bge-small-en-v1.5` model was leading the pack, especially considering its size. It could be a solid choice if you're diving into the world of text embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import torch\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "Settings.llm = None\n",
    "# Create embed model\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have to find somewhere to store all of the embeddings extracted by the model, and that's why we need a `vector store`. There are many to choose from, in this tutorial, I will choose the `chroma` vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../DB/arxiv\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv_papers\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part takes quite a lot of time! So I precomputed the embedding and store them into chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     arxiv_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Loading from arxiv vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/arxiv\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_arxiv_papers\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "deep image synthesis from intuitive user input: a review and perspectives\n",
      " In many applications of computer graphics, art and design, it is desirablefor a user to provide intuitive non-image input, such as text, sketch, stroke,graph or layout, and have a computer system automatically generatephoto-realistic images that adhere to the input content. While classic worksthat allow such automatic image content generation have followed a framework ofimage retrieval and composition, recent advances in deep generative models suchas generative adversarial networks (GANs), variational autoencoders (VAEs), andflow-based methods have enabled more powerful and versatile image generationtasks. This paper reviews recent works for image synthesis given intuitive userinput, covering advances in input versatility, image generation methodology,benchmark datasets, and evaluation metrics. This motivates new perspectives oninput representation and interactivity, cross pollination between major imagegeneration paradigms, and evaluation and comparison of generation methods.\n",
      "\n",
      "paint it black: generating paintings from text descriptions\n",
      " Two distinct tasks - generating photorealistic pictures from given textprompts and transferring the style of a painting to a real image to make itappear as though it were done by an artist, have been addressed many times, andseveral approaches have been proposed to accomplish them. However, theintersection of these two, i.e., generating paintings from a given caption, isa relatively unexplored area with little data available. In this paper, we haveexplored two distinct strategies and have integrated them together. Firststrategy is to generate photorealistic images and then apply style transfer andthe second strategy is to train an image generation model on real images withcaptions and then fine-tune it on captioned paintings later. These two modelsare evaluated using different metrics as well as a user study is conducted toget human feedback on the produced results.\n",
      "\n",
      "semantic draw engineering for text-to-image creation\n",
      " Text-to-image generation is conducted through Generative Adversarial Networks(GANs) or transformer models. However, the current challenge lies in accuratelygenerating images based on textual descriptions, especially in scenarios wherethe content and theme of the target image are ambiguous. In this paper, wepropose a method that utilizes artificial intelligence models for thematiccreativity, followed by a classification modeling of the actual paintingprocess. The method involves converting all visual elements into quantifiabledata structures before creating images. We evaluate the effectiveness of thisapproach in terms of semantic accuracy, image reproducibility, andcomputational efficiency, in comparison with existing image generationalgorithms.\n",
      "\n",
      "systematic analysis of image generation using gans\n",
      " Generative Adversarial Networks have been crucial in the developments made inunsupervised learning in recent times. Exemplars of image synthesis from textor other images, these networks have shown remarkable improvements overconventional methods in terms of performance. Trained on the adversarialtraining philosophy, these networks aim to estimate the potential distributionfrom the real data and then use this as input to generate the synthetic data.Based on this fundamental principle, several frameworks can be generated thatare paragon implementations in several real-life applications such as artsynthesis, generation of high resolution outputs and synthesis of images fromhuman drawn sketches, to name a few. While theoretically GANs present betterresults and prove to be an improvement over conventional methods in manyfactors, the implementation of these frameworks for dedicated applicationsremains a challenge. This study explores and presents a taxonomy of theseframeworks and their use in various image to image synthesis and text to imagesynthesis applications. The basic GANs, as well as a variety of different nicheframeworks, are critically analyzed. The advantages of GANs for imagegeneration over conventional methods as well their disadvantages amongst otherframeworks are presented. The future applications of GANs in industries such ashealthcare, art and entertainment are also discussed.\n",
      "\n",
      "text-guided image-and-shape editing and generation: a short survey\n",
      " Image and shape editing are ubiquitous among digital artworks. Graphicsalgorithms facilitate artists and designers to achieve desired editing intentswithout going through manually tedious retouching. In the recent advance ofmachine learning, artists' editing intents can even be driven by text, using avariety of well-trained neural networks. They have seen to be receiving anextensive success on such as generating photorealistic images, artworks andhuman poses, stylizing meshes from text, or auto-completion given image andshape priors. In this short survey, we provide an overview over 50 papers onstate-of-the-art (text-guided) image-and-shape generation techniques. We startwith an overview on recent editing algorithms in the introduction. Then, weprovide a comprehensive review on text-guided editing techniques for 2D and 3Dindependently, where each of its sub-section begins with a brief backgroundintroduction. We also contextualize editing algorithms under recent implicitneural representations. Finally, we conclude the survey with the discussionover existing methods and potential research ideas.\n",
      "\n",
      "text-to-image cross-modal generation: a systematic review\n",
      " We review research on generating visual data from text from the angle of\"cross-modal generation.\" This point of view allows us to draw parallelsbetween various methods geared towards working on input text and producingvisual output, without limiting the analysis to narrow sub-areas. It alsoresults in the identification of common templates in the field, which are thencompared and contrasted both within pools of similar methods and across linesof research. We provide a breakdown of text-to-image generation into variousflavors of image-from-text methods, video-from-text methods, image editing,self-supervised and graph-based approaches. In this discussion, we focus onresearch papers published at 8 leading machine learning conferences in theyears 2016-2022, also incorporating a number of relevant papers not matchingthe outlined search criteria. The conducted review suggests a significantincrease in the number of papers published in the area and highlights researchgaps and potential lines of investigation. To our knowledge, this is the firstreview to systematically look at text-to-image generation from the perspectiveof \"cross-modal generation.\"\n",
      "\n",
      "a taxonomy of prompt modifiers for text-to-image generation\n",
      " Text-to-image generation has seen an explosion of interest since 2021. Today,beautiful and intriguing digital images and artworks can be synthesized fromtextual inputs (\"prompts\") with deep generative models. Online communitiesaround text-to-image generation and AI generated art have quickly emerged. Thispaper identifies six types of prompt modifiers used by practitioners in theonline community based on a 3-month ethnographic study. The novel taxonomy ofprompt modifiers provides researchers a conceptual starting point forinvestigating the practice of text-to-image generation, but may also helppractitioners of AI generated art improve their images. We further outline howprompt modifiers are applied in the practice of \"prompt engineering.\" Wediscuss research opportunities of this novel creative practice in the field ofHuman-Computer Interaction (HCI). The paper concludes with a discussion ofbroader implications of prompt engineering from the perspective of Human-AIInteraction (HAI) in future applications beyond the use case of text-to-imagegeneration and AI generated art.\n",
      "\n",
      "figgen: text to scientific figure generation\n",
      " The generative modeling landscape has experienced tremendous growth in recentyears, particularly in generating natural images and art. Recent techniqueshave shown impressive potential in creating complex visual compositions whiledelivering impressive realism and quality. However, state-of-the-art methodshave been focusing on the narrow domain of natural images, while otherdistributions remain unexplored. In this paper, we introduce the problem oftext-to-figure generation, that is creating scientific figures of papers fromtext descriptions. We present FigGen, a diffusion-based approach fortext-to-figure as well as the main challenges of the proposed task. Code andmodels are available at https://github.com/joanrod/figure-diffusion\n",
      "\n",
      "a novel sampling scheme for text- and image-conditional image synthesis in quantized latent spaces\n",
      " Recent advancements in the domain of text-to-image synthesis have culminatedin a multitude of enhancements pertaining to quality, fidelity, and diversity.Contemporary techniques enable the generation of highly intricate visuals whichrapidly approach near-photorealistic quality. Nevertheless, as progress isachieved, the complexity of these methodologies increases, consequentlyintensifying the comprehension barrier between individuals within the field andthose external to it. In an endeavor to mitigate this disparity, we propose a streamlined approachfor text-to-image generation, which encompasses both the training paradigm andthe sampling process. Despite its remarkable simplicity, our method yieldsaesthetically pleasing images with few sampling iterations, allows forintriguing ways for conditioning the model, and imparts advantages absent instate-of-the-art techniques. To demonstrate the efficacy of this approach inachieving outcomes comparable to existing works, we have trained a one-billionparameter text-conditional model, which we refer to as \"Paella\". In theinterest of fostering future exploration in this field, we have made our sourcecode and models publicly accessible for the research community.\n",
      "\n",
      "text-to-image diffusion models in generative ai: a survey\n",
      " This survey reviews text-to-image diffusion models in the context thatdiffusion models have emerged to be popular for a wide range of generativetasks. As a self-contained work, this survey starts with a brief introductionof how a basic diffusion model works for image synthesis, followed by howcondition or guidance improves learning. Based on that, we present a review ofstate-of-the-art methods on text-conditioned image synthesis, i.e.,text-to-image. We further summarize applications beyond text-to-imagegeneration: text-guided creative generation and text-guided image editing.Beyond the progress made so far, we discuss existing challenges and promisingfuture directions.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What are some papers about image generation?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(paper_query_engine.query(\"What are some papers about image generation?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Graph-based paper relationship search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we dive into constructing a knowledge graph about the relationships of papers. This graph could be used for interactive visualization, searching relationships between papers (e.g. How is paper A related to paper B), or search for a specific relationship in a paper (e.g. What are works that paper A based on?). The steps of constructing this knowledge graph are:\n",
    "\n",
    "\n",
    "- Step 1: arXiv Data Extraction: The process starts with academic papers from the arXiv database, which undergo OCR (Optical Character Recognition) and PDF parsing, which organizes the content into structured data such as the title, abstract, sections, and references of the papers. \n",
    "\n",
    "- Step 2: Text Splitter: The text in each section is then processed by a Text Splitter, which split the paper section into smaller chunks, which could be easier for LLMs to process.  \n",
    "\n",
    "- Step 3 GPT-3.5 Processing: Gemma couldn't generate the knowledge graph out-of-the-box. So we need knowledge distillation from a bigger model, which I choose GPT-3.5. The structured data is passed to GPT-3.5 to extract citation relationships, such as \"Data Source\", \"Extension\", or \"Theoretical Foundation\", etc. Each relationship is paired with a dense explanation. I extracted a total of ~300 papers, which cost around 4$.\n",
    "\n",
    "- Step 4 Training Gemma - 7B: The distilled knowledge data are then used to train Gemma-7b. Then I use this model to generate citation relationships for as many papers as I can. In total, I extracted 7k papers, with around 150K triplets! Crazy!!\n",
    "\n",
    "- Step 5 Graph Store: Finally, a Graph Store is created containing 7K papers and 586K triplets. This could then be used for searching relationships or visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Graph-Paper-Search.jpg\" alt=\"graph-search\" width=1200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Download pre-extracted citation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "parsed_article = load_dataset(\"BachNgoH/ParsedArxivPapers\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = parsed_article.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for article in parsed_article:\n",
    "    if article['citation_data'] != None:\n",
    "        article['citation_data'] = json.loads(article['citation_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Data Source',\n",
       "  'Citation': '(Ristani et al. 2016)',\n",
       "  'Explanation': 'The cited work by Ristani et al. (2016) is used to highlight the challenges associated with collecting and labeling training data for AutoRetail Checkout (ARC) with deep learning, emphasizing the need for a new approach to address these issues.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ruiz, Schulter, and Chandraker 2019)',\n",
       "  'Explanation': 'The cited work provides a method for addressing the domain gap between rendered and real data, which the citing paper adopts to improve the scalability of data rendering using graphic engines.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Tremblay et al. 2018)',\n",
       "  'Explanation': 'The cited work presents a method for training deep learning models using rendered data, which the citing paper utilizes to improve the training process and enhance the model performance.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Sun and Zheng 2019)',\n",
       "  'Explanation': 'The cited work offers a method for addressing the domain gap in training data by introducing a new approach to data collection and processing, which the citing paper adopts to improve the data quality and model performance.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Kar et al. 2019)',\n",
       "  'Explanation': 'The cited work by Kar et al. provides a method for training set optimization, which the citing paper adopts to improve the process of human involvement in the process of training set creation.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ruiz, Schulter, and Chandraker 2019)',\n",
       "  'Explanation': 'The cited work by Ruiz, Schulter, and Chandraker provides a method for reducing the necessity for extensive human involvement in the process of training set creation, which the citing paper builds upon to improve the process.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al. 2020)',\n",
       "  'Explanation': 'The cited work by Yao et al. presents a method for learning film attribute distributions, which the citing paper uses to enhance the realism of rendered data and improve the specificity of the training set for a target validation/testing.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al. 2020)',\n",
       "  'Explanation': 'The cited work by Yao et al. is a prime example of advancements in training set optimization that the citing paper leverages to improve the process of training set creation.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al. 2022)',\n",
       "  'Explanation': 'The cited work by Yao et al. is a continuation of the advancements in training set optimization presented in the previous work by the same authors, which the citing paper builds upon to further improve the process of training set creation.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Yao et al. 2020)',\n",
       "  'Explanation': 'The cited work by Yao et al. is a data source for the training set optimization methods presented in the citing paper, as it provides the film attribute distributions that are used to improve the realism of rendered data and the specificity of the training set for a target validation/testing.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Corbiere et al. 2017)',\n",
       "  'Explanation': 'The cited work provides a dataset for dress retrieval, which the citing paper uses as a reference for the data they are working with in their research.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Zhan et al. 2021)',\n",
       "  'Explanation': 'The cited work provides a dataset for product retrieval, which the citing paper uses as a reference for the data they are working with in their research.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Liu et al.',\n",
       "  'Explanation': 'The cited work provides a dataset for MEP retrieval, which the citing paper uses as a reference for the data they are working with in their research.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Wei et al. 2019)',\n",
       "  'Explanation': 'The cited work provides the RPC dataset, which is a real-world dataset that the citing paper uses in their research to study the domain gap between rendered data and real-world data.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Farahani and Hekmatfar 2009)',\n",
       "  'Explanation': 'The cited work provides a K-center problem algorithm that the citing paper adopts to optimize the cover radius in the problem reduction process.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Williamson and Shmoys 2011)',\n",
       "  'Explanation': 'The cited work provides a 2-approximation algorithm that the citing paper uses to iteratively find optimal samples in the dataset to be selected for the furthest point sampling method.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Eldar et al. 1997)',\n",
       "  'Explanation': 'The cited work introduces the furthest point sampling method, which the citing paper utilizes to select the most representative samples from a dataset in a specific process.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Zhang et al. 2018)',\n",
       "  'Explanation': 'The cited work by Zhang et al. (2018) is used in the citing paper to calculate image differences in feature space, which serves as a methodological basis for optimizing the feature extraction function in the feature space.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al. 2022)',\n",
       "  'Explanation': 'The cited work provides the inspiration for the per-image optimization method used in the citing paper, which is adapted from attribute descent to obtain digital twins.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Wright 2015)',\n",
       "  'Explanation': 'The cited work introduces the concept of coordinate descent, which the citing paper uses for per-image optimization in the process of obtaining digital twins.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Nguyen, Phan, and Nguyen, 2022)',\n",
       "  'Explanation': 'The cited work provides the detection-tracking-counting framework that the citing paper adopts for training the ARC model in the task model section.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Chen et al., 2016)',\n",
       "  'Explanation': 'The cited work introduces the concept of InfoGAN, which the citing paper incorporates into their research for digital twin acquisition through neural rendering techniques.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Liu et al., 2019)',\n",
       "  'Explanation': 'The cited work provides the soft rasterizer, a differentiable renderer that the citing paper utilizes in their research for digital twin acquisition.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Rombach et al., 2022)',\n",
       "  'Explanation': 'The cited work introduces the concept of LDM (Latent Diffusion Models), which the citing paper extends by incorporating it into their research for digital twin acquisition through neural rendering techniques.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Wang et al., 2012)',\n",
       "  'Explanation': 'The cited work by Wang et al. (2012) introduces the SSIM loss function, which the citing paper adopts in their digital twin creation process to measure the similarity between images.'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Wang et al., 2012)',\n",
       "  'Explanation': 'The cited work by Wang et al. (2012) provides a method for measuring image similarity using the SSIM loss function, which the citing paper uses to assess the quality of the digital twins created.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Wang et al., 2012)',\n",
       "  'Explanation': 'The cited work by Wang et al. (2012) serves as a data source for the SSIM loss function, which the citing paper utilizes in their digital twin creation process to measure image similarity.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Kar et al. 2019)',\n",
       "  'Explanation': 'The cited work by Kar et al. is used to inform the use of reinforcement learning (RL) in training models to improve task accuracy in the context of leveraging rendered data for automatic retail systems.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Devaranjan, Kar, and Fidler 2020)',\n",
       "  'Explanation': 'The work by Devaranjan, Kar, and Fidler is cited to highlight the use of reinforcement learning (RL) in optimizing rendering attributes to improve task accuracy in the training process of models leveraging rendered data.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ruiz, Schulter, and Chandraker 2019)',\n",
       "  'Explanation': 'The work by Ruiz, Schulter, and Chandraker is referenced to discuss the use of reinforcement learning (RL) in improving task accuracy by optimizing rendering attributes in the training process of models leveraging rendered data.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Xue, Mao, and Zheng 2021)',\n",
       "  'Explanation': 'The work by Xue, Mao, and Zheng is cited to highlight the use of reinforcement learning (RL) in improving task accuracy by optimizing rendering attributes in the training process of models leveraging rendered data.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al. 2020)',\n",
       "  'Explanation': 'The work by Yao et al. is mentioned to discuss the use of a pruned greedy search called attribute descent to optimize attribute optimization in the training process of models leveraging rendered data due to challenges in obtaining attribute gradients.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Yao et al. (2022) provides a detailed explanation of the physical significance of various attributes in rendered images, which the citing paper uses to constrain the rotation attributes in their research on digital twins.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[Yao et al., 2022]',\n",
       "  'Explanation': 'The cited work by Yao et al. (2022) provides a methodological basis for the search space definition in the citing paper, as the authors maintain a consistent approach to ensure experimental fairness in the context of coordinate descent.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ruiz, Schulter, and Chandraker 2019)',\n",
       "  'Explanation': 'The cited work introduces the Learning to Simulate (LTS) approach, which the citing paper adopts as a method for optimizing controllable attributes in a reinforcement learning setting.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Yao et al. 2022)',\n",
       "  'Explanation': 'The cited work presents the attribute descent search strategy, which the citing paper uses to optimize the distribution parameters in a gradient-free search process.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Shi et al. 2019)',\n",
       "  'Explanation': 'The cited work by Shi et al. provides the methodological basis for the creation of digital twins for product images in the citing paper, by adapting the original approach to create digital twins for face images.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Chen et al. 2016)',\n",
       "  'Explanation': 'The cited work introduces the concept of InfoGAN, which the citing paper adopts in the second stage of their research to generate product images with desired attributes.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Rombach et al. 2022)',\n",
       "  'Explanation': 'The cited work presents the LDM model, which the citing paper fine-tunes to create a pre-trained stable diffusion model for their research on retail products.'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Radford et al. 2021)',\n",
       "  'Explanation': 'The cited work introduces the CLIP encoder, which the citing paper modifies by removing and replacing it with a lightweight attribute encoder to adapt the LDM model to their research on retail products.'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Ronneberger, Fischer, and Brox 2015)',\n",
       "  'Explanation': 'The cited work presents the U-Net architecture, which the citing paper utilizes in their LDM training to adapt the model to their research on retail products.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Jocher et al. 2022)',\n",
       "  'Explanation': 'The cited work, YOLOv5, is used as a method for detecting and classifying target products within video content in the experiment conducted in the citing paper.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Lin et al. 2014)',\n",
       "  'Explanation': 'The cited work, COCO, is used as a dataset for fine-tuning the YOLOv5 model in the detection stage of the experiment.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Gao et al. 2019)',\n",
       "  'Explanation': 'The Res2Net model is one of the three models used in the classification process in the detection stage of the experiment.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Liu et al. 2021)',\n",
       "  'Explanation': 'The Swin-Transformer model is one of the three models used in the classification process in the detection stage of the experiment.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ding et al. 2021)',\n",
       "  'Explanation': 'The RepVGG model is one of the three models used in the classification process in the detection stage of the experiment.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Zhang et al.',\n",
       "  'Explanation': 'The Byte-Track algorithm is used in the tracking stage of the experiment to establish continuity in tracking identical products across frames.'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_article[5000]['citation_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the number of annotated papers for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_article = [x for x in parsed_article if x['citation_data'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated Papers:  7243\n"
     ]
    }
   ],
   "source": [
    "print(\"Annotated Papers: \", len(annotated_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Parsing generated data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my observation, there are 2 main citation styles in AI papers, Author-year style and Numeric style:\n",
    "\n",
    "Example of Author-year style:\n",
    "- (Bassignana and Plank, 2022a) \n",
    "- (Liu et al., 2021)\n",
    "- (KÃ¶ksal and Ã–zgÃ¼r, 2020)\n",
    "\n",
    "Example of Numeric style:\n",
    "- [1], [2], [3]\n",
    "- [2, 56, 67]\n",
    "- [7 - 9]\n",
    "\n",
    "Therefore, we need different strategy to handle each style of citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.1 Handle Author-Year citation style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling this citation style can be quite frustrating. Initially, we must separate combined citations like (Liu et al., 2021; Littell et al., 201) into individual entries. Then, we need to identify the first author and publication year. Subsequently, we have to locate the corresponding reference within our reference list based on the author's name and publication year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(Cohn et al., 1996)',\n",
       " '(Settles, 2009)',\n",
       " '(Dasgupta, 2011)',\n",
       " '(Gururangan et al., 2020)',\n",
       " '(Houlsby et al., 2019)',\n",
       " '(Pfeiffer et al., 2023)',\n",
       " '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)',\n",
       " '(Toneva et al., 2019)',\n",
       " '(Ein-Dor et al., 2020)',\n",
       " '(Margatina et al., 2021)',\n",
       " '(Shelmanov et al., 2021)',\n",
       " '(Karamcheti et al., 2021)',\n",
       " '(SchrÃ¶der et al., 2022)',\n",
       " '(Mosbach et al., 2021)',\n",
       " '(Zhang et al., 2021)',\n",
       " '(Dodge et al., 2020)',\n",
       " '(GrieÃŸhaber et al., 2020)',\n",
       " '(Yuan et al., 2020)',\n",
       " '(Yu et al., 2022)',\n",
       " '(Margatina et al., 2022)',\n",
       " '(JukiÄ‡ and Å najder, 2023)',\n",
       " '(Ansell et al., 2021)',\n",
       " '(Lee et al., 2022)',\n",
       " '(ParoviÄ‡ et al., 2022)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(He et al., 2021)',\n",
       " '(Kim et al., 2021)',\n",
       " '(Pang and Lee, 2004)',\n",
       " '(Li and Roth, 2002)',\n",
       " '(Socher et al., 2013)',\n",
       " '(Zhang et al., 2015)',\n",
       " '(Houlsby et al., 2019)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Hu et al., 2022)',\n",
       " '(Mao et al., 2022)',\n",
       " '(Devlin et al., 2019)',\n",
       " '(Lewis and Gale, 1994)',\n",
       " '(Gal and Ghahramani, 2016)',\n",
       " '(Srivastava et al., 2014)',\n",
       " '(Sener and Savarese, 2018)',\n",
       " '(SchrÃ¶der et al., 2022)',\n",
       " '(JukiÄ‡ and Å najder, 2023)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(He et al., 2021)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(He et al., 2021)',\n",
       " '(Toneva et al., 2019)',\n",
       " '(He et al., 2021)',\n",
       " '(Li and Liang, 2021)',\n",
       " '(Mao et al., 2022)',\n",
       " '(Stephenson et al., 2021)',\n",
       " '(Baldock et al., 2021)',\n",
       " '(Pfeiffer et al., 2020)']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse annotated articles\n",
    "import re\n",
    "\n",
    "# Function to normalize author names for comparison\n",
    "def normalize_author_name(name):\n",
    "    # Convert to lowercase and remove middle initials\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"\\s+[a-z]\\.\", \"\", name)  # Remove middle initials\n",
    "    return name\n",
    "\n",
    "\n",
    "citation_names = [c['Citation'] for c in annotated_article[0]['citation_data']]\n",
    "citation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined function to identify and normalize the first author from a citation\n",
    "def identify_and_normalize_first_author(citation_authors):\n",
    "    # Check for 'et al.' and 'and' to find the first author\n",
    "    if 'et al.' in citation_authors:\n",
    "        first_author = citation_authors.split('et al.')[0].strip()\n",
    "    elif ' and ' in citation_authors:\n",
    "        first_author = citation_authors.rsplit(' and ', 1)[0].split(',')[0].strip()\n",
    "    else:\n",
    "        first_author = citation_authors.split(',')[0].strip()\n",
    "    # Normalize the first author's name for comparison\n",
    "    return first_author.lower()\n",
    "\n",
    "\n",
    "# Function to split and parse citations in cases of citation \n",
    "# like (Culotta and Sorensen 2004; Bunescu and Mooney 2005; Ittoo and Bouma 2013)\n",
    "def split_and_parse_citation(citation):\n",
    "\n",
    "    # Remove outer parentheses\n",
    "    citation = citation.strip(\"()\")\n",
    "    # Split on semicolon if it's present, indicating multiple citations within one\n",
    "    if ';' in citation:\n",
    "        sub_citations = citation.split(';')\n",
    "    else:\n",
    "        sub_citations = [citation]\n",
    "    \n",
    "    # Parse each sub-citation for author names and year\n",
    "    for sub_citation in sub_citations:\n",
    "        # Splitting based on the last occurrence of space which is assumed to be before the year\n",
    "        *authors, year = sub_citation.rsplit(' ', 1)\n",
    "        authors = ' '.join(authors)  # Joining back the authors in case there are multiple names\n",
    "        parsed_citation = {'Author': identify_and_normalize_first_author(authors), 'Year': year}\n",
    "    \n",
    "    return parsed_citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': 'Alan Ansell; Maria Edoardo; Jonas Ponti; Sebastian Pfeiffer; Goran Ruder; Ivan GlavaÅ¡; Anna VuliÄ‡;  Korhonen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b0',\n",
       "  'title': 'MAD-G: Multilingual adapter generation for efficient cross-lingual transfer',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Robert Baldock; Hartmut Maennel; Behnam Neyshabur',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b1',\n",
       "  'title': 'Deep learning through the lens of example difficulty',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Curran Associates; Inc ',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b2',\n",
       "  'title': '',\n",
       "  'year': ''},\n",
       " {'authors': 'Zoubin David A Cohn; Michael I Ghahramani;  Jordan',\n",
       "  'journal': 'Journal of artificial intelligence research',\n",
       "  'ref_id': 'b3',\n",
       "  'title': 'Active learning with statistical models',\n",
       "  'year': '1996'},\n",
       " {'authors': 'Sanjoy Dasgupta',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b4',\n",
       "  'title': 'Two faces of active learning',\n",
       "  'year': '2009'},\n",
       " {'authors': 'Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b5',\n",
       "  'title': 'BERT: Pre-training of deep bidirectional transformers for language understanding',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Jesse Dodge; Gabriel Ilharco; Roy Schwartz; Ali Farhadi; Hannaneh Hajishirzi; Noah Smith',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b6',\n",
       "  'title': 'Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Liat Ein-Dor; Alon Halfon; Ariel Gera; Eyal Shnarch; Lena Dankin; Leshem Choshen; Marina Danilevsky; Ranit Aharonov; Yoav Katz; Noam Slonim',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b7',\n",
       "  'title': 'Active Learning for BERT: An Empirical Study',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Yarin Gal; Zoubin Ghahramani',\n",
       "  'journal': 'PMLR',\n",
       "  'ref_id': 'b8',\n",
       "  'title': 'Dropout as a bayesian approximation: Representing model uncertainty in deep learning',\n",
       "  'year': '2016'},\n",
       " {'authors': 'Daniel Gissin; Shai Shalev-Shwartz',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b9',\n",
       "  'title': 'Discriminative active learning',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Daniel GrieÃŸhaber; Johannes Maucher; Ngoc Thang Vu',\n",
       "  'journal': 'International Committee on Computational Linguistics',\n",
       "  'ref_id': 'b10',\n",
       "  'title': 'Fine-tuning BERT for low-resource natural language understanding via active learning',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Suchin Gururangan; Ana MarasoviÄ‡; Swabha Swayamdipta; Kyle Lo; Iz Beltagy; Doug Downey; Noah A Smith',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b11',\n",
       "  'title': \"Don't stop pretraining: Adapt language models to domains and tasks\",\n",
       "  'year': '2020'},\n",
       " {'authors': 'Junxian He; Chunting Zhou; Xuezhe Ma; Taylor Berg-Kirkpatrick; Graham Neubig',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b12',\n",
       "  'title': 'Towards a unified view of parameter-efficient transfer learning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Ruidan He; Linlin Liu; Hai Ye; Qingyu Tan; Bosheng Ding; Liying Cheng; Jiawei Low; Lidong Bing; Luo Si',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b13',\n",
       "  'title': 'On the effectiveness of adapter-based tuning for pretrained language model adaptation',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b14',\n",
       "  'title': 'Parameter-efficient transfer learning for NLP',\n",
       "  'year': '2019'},\n",
       " {'authors': ' Pmlr', 'journal': '', 'ref_id': 'b15', 'title': '', 'year': ''},\n",
       " {'authors': 'J Edward; Yelong Hu; Phillip Shen; Zeyuan Wallis; Yuanzhi Allen-Zhu; Shean Li; Lu Wang; Weizhu Wang;  Chen',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b16',\n",
       "  'title': 'LoRA: Low-rank adaptation of large language models',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Josip JukiÄ‡; Jan Å najder',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b17',\n",
       "  'title': 'Smooth sailing: Improving active learning for pre-trained language models with representation smoothness analysis',\n",
       "  'year': '2023'},\n",
       " {'authors': 'Siddharth Karamcheti; Ranjay Krishna; Li Fei-Fei; Christopher Manning',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b18',\n",
       "  'title': 'Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Rabeeh Karimi Mahabadi; Sebastian Ruder; Mostafa Dehghani; James Henderson',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b19',\n",
       "  'title': 'Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Seungwon Kim; Alex Shum; Nathan Susanj; Jonathan Hilgart',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b20',\n",
       "  'title': 'Revisiting pretraining with adapters',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Simon Kornblith; Mohammad Norouzi; Honglak Lee; Geoffrey Hinton',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b21',\n",
       "  'title': 'Similarity of neural network representations revisited',\n",
       "  'year': '2019'},\n",
       " {'authors': ' Pmlr', 'journal': '', 'ref_id': 'b22', 'title': '', 'year': ''},\n",
       " {'authors': 'Jaeseong Lee; Seung-Won Hwang; Taesup Kim',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b23',\n",
       "  'title': 'FAD-X: Fusing adapters for cross-lingual transfer to low-resource languages',\n",
       "  'year': '2022'},\n",
       " {'authors': 'D David; William A Lewis;  Gale',\n",
       "  'journal': 'Springer',\n",
       "  'ref_id': 'b24',\n",
       "  'title': 'A sequential algorithm for training text classifiers',\n",
       "  'year': '1994'},\n",
       " {'authors': 'Lisa Xiang; Percy Li;  Liang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b25',\n",
       "  'title': 'Prefix-tuning: Optimizing continuous prompts for generation',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Xin Li; Dan Roth',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b26',\n",
       "  'title': 'Learning question classifiers',\n",
       "  'year': '2002'},\n",
       " {'authors': 'Yuning Mao; Lambert Mathias; Rui Hou; Amjad Almahairi; Hao Ma; Jiawei Han; Scott Yih; Madian Khabsa',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b27',\n",
       "  'title': 'UniPELT: A unified framework for parameter-efficient language model tuning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Loic Barrault; Nikolaos Aletras',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b28',\n",
       "  'title': 'On the importance of effectively adapting pretrained language models for active learning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Giorgos Vernikos; LoÃ¯c Barrault; Nikolaos Aletras',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b29',\n",
       "  'title': 'Active learning by acquiring contrastive examples',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Marius Mosbach; Maksym Andriushchenko; Dietrich Klakow',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b30',\n",
       "  'title': 'On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Bo Pang; Lillian Lee',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b31',\n",
       "  'title': 'A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts',\n",
       "  'year': '2004'},\n",
       " {'authors': 'Marinela ParoviÄ‡; Goran GlavaÅ¡; Ivan VuliÄ‡; Anna Korhonen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b32',\n",
       "  'title': 'BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Jonas Pfeiffer; Andreas RÃ¼cklÃ©; Clifton Poth; Aishwarya Kamath; Ivan VuliÄ‡; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b33',\n",
       "  'title': 'AdapterHub: A framework for adapting transformers',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Jonas Pfeiffer; Sebastian Ruder; Ivan VuliÄ‡; Maria Edoardo;  Ponti',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b34',\n",
       "  'title': 'Modular deep learning',\n",
       "  'year': '2023'},\n",
       " {'authors': 'Christopher SchrÃ¶der; Andreas Niekler; Martin Potthast',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b35',\n",
       "  'title': 'Revisiting uncertainty-based query strategies for active learning with transformers',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Ozan Sener; Silvio Savarese',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b36',\n",
       "  'title': 'Active learning for convolutional neural networks: A core-set approach',\n",
       "  'year': '2018'},\n",
       " {'authors': 'Burr Settles',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b37',\n",
       "  'title': 'Active learning literature survey',\n",
       "  'year': '2009'},\n",
       " {'authors': 'Artem Shelmanov; Dmitri Puzyrev; Lyubov Kupriyanova; Denis Belyakov; Daniil Larionov; Nikita Khromov; Olga Kozlova; Ekaterina Artemova; V Dmitry; Alexander Dylov;  Panchenko',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b38',\n",
       "  'title': 'Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Richard Socher; John Bauer; Christopher D Manning; Andrew Y Ng',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b39',\n",
       "  'title': 'Parsing with compositional vector grammars',\n",
       "  'year': '2013'},\n",
       " {'authors': 'Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov',\n",
       "  'journal': 'Journal of Machine Learning Research',\n",
       "  'ref_id': 'b40',\n",
       "  'title': 'Dropout: A simple way to prevent neural networks from overfitting',\n",
       "  'year': '2014'},\n",
       " {'authors': 'Cory Stephenson; Suchismita Padhy; Abhinav Ganesh; Yue Hui; Hanlin Tang; Sueyeon Chung',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b41',\n",
       "  'title': 'On the geometry of generalization and memorization in deep neural networks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Mariya Toneva; Alessandro Sordoni; Remi Tachet Des Combes; Adam Trischler; Yoshua Bengio; Geoffrey J Gordon',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b42',\n",
       "  'title': 'An empirical study of example forgetting during deep neural network learning',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Yue Yu; Lingkai Kong; Jieyu Zhang; Rongzhi Zhang; Chao Zhang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b43',\n",
       "  'title': 'AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Michelle Yuan; Hsuan-Tien Lin; Jordan Boyd-Graber',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b44',\n",
       "  'title': 'Cold-start active learning through selfsupervised language modeling',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Tianyi Zhang; Felix Wu; Arzoo Katiyar; Kilian Q Weinberger; Yoav Artzi',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b45',\n",
       "  'title': 'Revisiting few-sample BERT fine-tuning',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Xiang Zhang; Junbo Zhao; Yann Lecun',\n",
       "  'journal': 'Advances in neural information processing systems',\n",
       "  'ref_id': 'b46',\n",
       "  'title': 'Character-level convolutional networks for text classification',\n",
       "  'year': '2015'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = annotated_article[0]['references']\n",
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize and extract the first author's name\n",
    "def get_first_author(authors_str):\n",
    "    first_author = authors_str.split(';')[0].strip()\n",
    "    # Normalize the first author's name for comparison\n",
    "    return first_author.lower()\n",
    "\n",
    "# Generalized regular expression for detecting years in various date formats and standalone years\n",
    "\n",
    "# Function to detect various year patterns and extract the year\n",
    "def extract_years(string):\n",
    "    general_year_pattern = re.compile(r'(?:\\b|\\D)(\\d{4})(?:\\b|\\D)')\n",
    "    # Find all matches for the general year pattern\n",
    "\n",
    "    matches = general_year_pattern.findall(string)\n",
    "    # Add all unique years found in this string\n",
    "    year = matches[0] if matches else None\n",
    "    return year\n",
    "\n",
    "# Function to match citations with references\n",
    "def match_citations_with_references(citation, references):\n",
    "    match = None\n",
    "    citation_first_author = citation['Author']\n",
    "    citation_year = citation['Year'].strip()\n",
    "    for ref in references:\n",
    "        ref_first_author = get_first_author(ref['authors'])\n",
    "        ref_year = extract_years(ref['year']) if ref['year'] is not None else None\n",
    "        # Check for match by first author and year\n",
    "        if citation_first_author in ref_first_author: #and (citation_year == ref_year or ref_year is None):\n",
    "            match = {\n",
    "                'ref_id': ref['ref_id']\n",
    "            }\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with the first sample\n",
    "for citation in annotated_article[0]['citation_data']:\n",
    "    parsed_name = split_and_parse_citation(citation['Citation'])\n",
    "    match = match_citations_with_references(parsed_name, references)\n",
    "    citation['ref_id'] = match['ref_id'] if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Methodological Basis',\n",
       "  'Citation': '(Cohn et al., 1996)',\n",
       "  'Explanation': 'The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.',\n",
       "  'ref_id': 'b3'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Settles, 2009)',\n",
       "  'Explanation': 'The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.',\n",
       "  'ref_id': 'b37'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Dasgupta, 2011)',\n",
       "  'Explanation': 'The cited work highlights the importance of label complexity in active learning and the need to reduce it for efficient model training, which the citing paper addresses in its research on efficient finetuning methods for PLMs in low-resource settings.',\n",
       "  'ref_id': 'b4'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Gururangan et al., 2020)',\n",
       "  'Explanation': 'The cited work introduces the concept of task-adaptive pre-training (TAPT), which the citing paper adopts in their research to further reduce the label complexity in AL research.',\n",
       "  'ref_id': 'b11'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Houlsby et al., 2019)',\n",
       "  'Explanation': 'The cited work introduces the concept of adapters as compact modules for fine-tuning PLMs, which the citing paper extends by discussing the use of adapters for parameter-efficient fine-tuning (PEFT) in AL research.',\n",
       "  'ref_id': 'b14'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Pfeiffer et al., 2023)',\n",
       "  'Explanation': 'The cited work discusses the use of modular learning in PEFT, which the citing paper references as a method for parameter-efficient fine-tuning in AL research.',\n",
       "  'ref_id': 'b34'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)',\n",
       "  'Explanation': 'The cited works have revealed that PEFT methods outperform full fine-tuning in low-resource settings, which is a key finding that supports the claims made in the citing paper about the potential benefits of PEFT in this context.',\n",
       "  'ref_id': 'b19'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Toneva et al., 2019)',\n",
       "  'Explanation': 'The cited work by Toneva et al. (2019) provides a method for analyzing the properties of PEFT and FFT, which the citing paper uses to understand the reason for the improved performance of PEFT in low-resource AL scenarios.',\n",
       "  'ref_id': 'b42'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Ein-Dor et al., 2020)',\n",
       "  'Explanation': 'The cited work by Ein-Dor et al. (2020) provides a conventional approach for integrating PLMs with AL, which the citing paper adopts in their research to investigate the use of PEFT techniques in low-resource settings.',\n",
       "  'ref_id': 'b7'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Margatina et al., 2021)',\n",
       "  'Explanation': 'The cited work by Margatina et al. (2021) also contributes to the research on combining PLMs with AL, providing a method for fine-tuning the model in each AL step.',\n",
       "  'ref_id': 'b29'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Shelmanov et al., 2021)',\n",
       "  'Explanation': 'The cited work by Shelmanov et al. (2021) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.',\n",
       "  'ref_id': 'b38'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Karamcheti et al., 2021)',\n",
       "  'Explanation': 'The cited work by Karamcheti et al. (2021) also contributes to the research on combining PLMs with AL, by exploring the use of fine-tuning in each AL step.',\n",
       "  'ref_id': 'b18'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(SchrÃ¶der et al., 2022)',\n",
       "  'Explanation': 'The cited work by SchrÃ¶der et al. (2022) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.',\n",
       "  'ref_id': 'b35'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Mosbach et al., 2021)',\n",
       "  'Explanation': 'The cited work by Mosbach et al. (2021) extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.',\n",
       "  'ref_id': 'b30'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Zhang et al., 2021)',\n",
       "  'Explanation': 'The cited work by Zhang et al. (2021) also extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.',\n",
       "  'ref_id': 'b46'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Dodge et al., 2020)',\n",
       "  'Explanation': 'The cited work by Dodge et al. (2020) provides a data source for the research on fine-tuning in low-resource settings, by discussing the sensitivity of the process to weight initialization and data ordering.',\n",
       "  'ref_id': 'b6'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(GrieÃŸhaber et al., 2020)',\n",
       "  'Explanation': 'The cited work by GrieÃŸhaber et al. (2020) provides evidence that the choice of training regime is more critical than the choice of the AL method in improving AL performance.',\n",
       "  'ref_id': 'b10'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Yuan et al., 2020)',\n",
       "  'Explanation': 'The cited work by Yuan et al. (2020) further supports the claim that the training regime is more important than the AL method in enhancing AL performance.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Yu et al., 2022)',\n",
       "  'Explanation': 'The cited work by Yu et al. (2022) provides additional evidence that the training regime is a critical factor in improving AL performance.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Margatina et al., 2022)',\n",
       "  'Explanation': 'The cited work by Margatina et al. (2022) extends the research on the effectiveness of TAPT in enhancing AL performance by providing further insights and data.',\n",
       "  'ref_id': 'b29'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(JukiÄ‡ and Å najder, 2023)',\n",
       "  'Explanation': 'The cited work by JukiÄ‡ and Å najder (2023) continues the research on TAPT by exploring new dimensions and variables in enhancing AL performance.',\n",
       "  'ref_id': 'b17'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Ansell et al., 2021)',\n",
       "  'Explanation': 'The cited work by Ansell et al. (2021) provides evidence on the effectiveness of cross-lingual transfer for low-resource languages in the context of adapters.',\n",
       "  'ref_id': 'b0'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Lee et al., 2022)',\n",
       "  'Explanation': 'The cited work by Lee et al. (2022) further supports the research on the use of adapters in low-resource settings for cross-lingual transfer.',\n",
       "  'ref_id': 'b23'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(ParoviÄ‡ et al., 2022)',\n",
       "  'Explanation': 'The cited work by ParoviÄ‡ et al. (2022) provides additional insights on the use of adapters in low-resource settings for cross-lingual transfer.',\n",
       "  'ref_id': 'b32'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work by Li and Liang (2021) supports the research on the use of adapters in monolingual settings with scarce data.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Mao et al. (2022) further supports the research on the use of adapters in monolingual settings with scarce data.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': 'The cited work by He et al. (2021) provides evidence on the stability and generalization capabilities of adapter-based tuning in monolingual settings with scarce data.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Supporting Evidence',\n",
       "  'Citation': '(Kim et al., 2021)',\n",
       "  'Explanation': 'The cited work by Kim et al. (2021) provides evidence that the benefits of integrating TAPT with adapters tend to taper off as the amount of data increases, which is relevant to the discussion in the citing paper about the limitations of using adapters in low-resource setups.',\n",
       "  'ref_id': 'b20'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Pang and Lee, 2004)',\n",
       "  'Explanation': 'The cited work by Pang and Lee serves as the data source for the SUBJ dataset used in the citing paper for the single-text classification task.',\n",
       "  'ref_id': 'b31'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Li and Roth, 2002)',\n",
       "  'Explanation': 'The cited work by Li and Roth is the data source for the TREC dataset used in the single-text classification task in the citing paper.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Socher et al., 2013)',\n",
       "  'Explanation': 'The cited work by Socher et al. is the data source for the SST dataset used in the single-text classification task in the citing paper.',\n",
       "  'ref_id': 'b39'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Zhang et al., 2015)',\n",
       "  'Explanation': 'The cited work by Zhang et al. is the data source for the AGN dataset used in the single-text classification task in the citing paper.',\n",
       "  'ref_id': 'b46'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Houlsby et al., 2019)',\n",
       "  'Explanation': 'The cited work introduces the concept of trainable bottleneck layers in Transformer layers, which the citing paper adopts in the development of the Adapter PEFT technique.',\n",
       "  'ref_id': 'b14'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work presents the Prefix-tuning PEFT technique, which the citing paper incorporates in the development of the UniPELT method by adding new parameters in the multi-head attention blocks of Transformer layers.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Hu et al., 2022)',\n",
       "  'Explanation': 'The cited work introduces the LoRA PEFT technique, which the citing paper incorporates in the development of the UniPELT method by representing an additive method that incorporates trainable low-rank decomposition matrices in the layers of a pre-trained model.',\n",
       "  'ref_id': None},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work presents the UniPELT PEFT method, which the citing paper considers as a combination of multiple PEFT approaches, including LoRA, Prefix-tuning, and Adapter, in a single unified setup with gating mechanisms for effective activation.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Devlin et al., 2019)',\n",
       "  'Explanation': 'The cited work by Devlin et al. (2019) provides the base PLM (BERT) that the citing paper uses as the foundation for their research on adapters.',\n",
       "  'ref_id': 'b5'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Lewis and Gale, 1994)',\n",
       "  'Explanation': 'The cited work by Lewis and Gale (1994) provides the maximum entropy (ENT) strategy for sampling instances in the field of uncertainty strategies, which the citing paper adopts as a method for instance selection.',\n",
       "  'ref_id': None},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Gal and Ghahramani, 2016)',\n",
       "  'Explanation': 'The cited work by Gal and Ghahramani (2016) introduces the Monte Carlo dropout (MC) method for instance selection based on the stochasticity of forward passes with dropout layers, which the citing paper utilizes in the field of uncertainty strategies.',\n",
       "  'ref_id': 'b8'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Srivastava et al., 2014)',\n",
       "  'Explanation': 'The cited work by Srivastava et al. (2014) presents the use of dropout layers in forward passes, which the citing paper references in the context of the Monte Carlo dropout (MC) method for instance selection in the field of uncertainty strategies.',\n",
       "  'ref_id': 'b40'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Sener and Savarese, 2018)',\n",
       "  'Explanation': 'The cited work by Sener and Savarese (2018) introduces the core-set (CS) method for instance selection in the field of learning representations of the acquisition model, which the citing paper adopts as a method for encouraging instance diversity.',\n",
       "  'ref_id': 'b36'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(SchrÃ¶der et al., 2022)',\n",
       "  'Explanation': 'The cited work provides a recommendation for using AUC as a suitable approximation of AL feasibility, which the citing paper adopts in their research to evaluate the performance of AL methods.',\n",
       "  'ref_id': 'b35'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(JukiÄ‡ and Å najder, 2023)',\n",
       "  'Explanation': 'The cited work also recommends using AUC as a summary numeric score in AL, which the citing paper adopts in their research to evaluate the performance of AL methods.',\n",
       "  'ref_id': 'b17'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work by Li and Liang provides the basis for the use of adapters in low-resource settings in the citing paper.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Mao et al. contributes to the understanding of the use of adapters in low-resource settings in the citing paper.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': 'The cited work by He et al. further builds upon the research on the use of adapters in low-resource settings in the citing paper.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The citing paper extends the research on the use of adapters in low-resource settings by conducting a more nuanced analysis and comparing multiple adapter variants with FFT under the passive learning setup.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The citing paper further extends the research on the use of adapters in low-resource settings by generating detailed learning curves to facilitate the comparison of multiple adapters with FFT in the passive learning setup.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Extension or Continuation',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': \"The citing paper continues the research on the use of adapters in low-resource settings by looking into how the models' performance changes as the training set increases.\",\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Toneva et al., 2019)',\n",
       "  'Explanation': 'The cited work by Toneva et al. (2019) provides a methodology for analyzing forgetting dynamics in training examples, which the citing paper adopts to study the occurrence of forgetting events in adapters and their impact on AL data selection.',\n",
       "  'ref_id': 'b42'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(He et al., 2021)',\n",
       "  'Explanation': 'The cited work by He et al. (2021) provides the inspiration for the layerwise examination of similarity in the citing paper, which is used to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.',\n",
       "  'ref_id': 'b44'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Li and Liang, 2021)',\n",
       "  'Explanation': 'The cited work by Li and Liang (2021) is used to bolster the findings of the citing paper by exploring the stability of representations in scenarios with limited resources.',\n",
       "  'ref_id': 'b26'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Mao et al., 2022)',\n",
       "  'Explanation': 'The cited work by Mao et al. (2022) contributes to the analysis of the stability of representations in the citing paper, providing insights into the use of adapters in scenarios with limited resources.',\n",
       "  'ref_id': 'b27'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Stephenson et al., 2021)',\n",
       "  'Explanation': 'The data source cited by Stephenson et al. (2021) is used to draw inspiration for the layerwise examination of similarity in the citing paper, which is conducted to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.',\n",
       "  'ref_id': 'b41'},\n",
       " {'Category': 'Data Source',\n",
       "  'Citation': '(Baldock et al., 2021)',\n",
       "  'Explanation': 'The data source cited by Baldock et al. (2021) is used in the citing paper to support the claim that different layers of networks specialize in different features, with earlier layers acquiring more generalized knowledge and deeper layers focusing on task-specific information.',\n",
       "  'ref_id': 'b1'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '(Pfeiffer et al., 2020)',\n",
       "  'Explanation': 'The cited work provides the implementation of adapters used in the citing paper, which serves as a methodological basis for the research conducted in the citing paper.',\n",
       "  'ref_id': 'b34'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[0]['citation_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'authors': 'Xin Li; Dan Roth',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b26',\n",
       "  'title': 'Learning question classifiers',\n",
       "  'year': '2002'},\n",
       " {'authors': 'Yuning Mao; Lambert Mathias; Rui Hou; Amjad Almahairi; Hao Ma; Jiawei Han; Scott Yih; Madian Khabsa',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b27',\n",
       "  'title': 'UniPELT: A unified framework for parameter-efficient language model tuning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Loic Barrault; Nikolaos Aletras',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b28',\n",
       "  'title': 'On the importance of effectively adapting pretrained language models for active learning',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Katerina Margatina; Giorgos Vernikos; LoÃ¯c Barrault; Nikolaos Aletras',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b29',\n",
       "  'title': 'Active learning by acquiring contrastive examples',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Marius Mosbach; Maksym Andriushchenko; Dietrich Klakow',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b30',\n",
       "  'title': 'On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Bo Pang; Lillian Lee',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b31',\n",
       "  'title': 'A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts',\n",
       "  'year': '2004'},\n",
       " {'authors': 'Marinela ParoviÄ‡; Goran GlavaÅ¡; Ivan VuliÄ‡; Anna Korhonen',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b32',\n",
       "  'title': 'BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Jonas Pfeiffer; Andreas RÃ¼cklÃ©; Clifton Poth; Aishwarya Kamath; Ivan VuliÄ‡; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b33',\n",
       "  'title': 'AdapterHub: A framework for adapting transformers',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Jonas Pfeiffer; Sebastian Ruder; Ivan VuliÄ‡; Maria Edoardo;  Ponti',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b34',\n",
       "  'title': 'Modular deep learning',\n",
       "  'year': '2023'},\n",
       " {'authors': 'Christopher SchrÃ¶der; Andreas Niekler; Martin Potthast',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b35',\n",
       "  'title': 'Revisiting uncertainty-based query strategies for active learning with transformers',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Ozan Sener; Silvio Savarese',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b36',\n",
       "  'title': 'Active learning for convolutional neural networks: A core-set approach',\n",
       "  'year': '2018'},\n",
       " {'authors': 'Burr Settles',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b37',\n",
       "  'title': 'Active learning literature survey',\n",
       "  'year': '2009'},\n",
       " {'authors': 'Artem Shelmanov; Dmitri Puzyrev; Lyubov Kupriyanova; Denis Belyakov; Daniil Larionov; Nikita Khromov; Olga Kozlova; Ekaterina Artemova; V Dmitry; Alexander Dylov;  Panchenko',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b38',\n",
       "  'title': 'Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Richard Socher; John Bauer; Christopher D Manning; Andrew Y Ng',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b39',\n",
       "  'title': 'Parsing with compositional vector grammars',\n",
       "  'year': '2013'},\n",
       " {'authors': 'Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov',\n",
       "  'journal': 'Journal of Machine Learning Research',\n",
       "  'ref_id': 'b40',\n",
       "  'title': 'Dropout: A simple way to prevent neural networks from overfitting',\n",
       "  'year': '2014'},\n",
       " {'authors': 'Cory Stephenson; Suchismita Padhy; Abhinav Ganesh; Yue Hui; Hanlin Tang; Sueyeon Chung',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b41',\n",
       "  'title': 'On the geometry of generalization and memorization in deep neural networks',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Mariya Toneva; Alessandro Sordoni; Remi Tachet Des Combes; Adam Trischler; Yoshua Bengio; Geoffrey J Gordon',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b42',\n",
       "  'title': 'An empirical study of example forgetting during deep neural network learning',\n",
       "  'year': '2019'},\n",
       " {'authors': 'Yue Yu; Lingkai Kong; Jieyu Zhang; Rongzhi Zhang; Chao Zhang',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b43',\n",
       "  'title': 'AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models',\n",
       "  'year': '2022'},\n",
       " {'authors': 'Michelle Yuan; Hsuan-Tien Lin; Jordan Boyd-Graber',\n",
       "  'journal': 'Association for Computational Linguistics',\n",
       "  'ref_id': 'b44',\n",
       "  'title': 'Cold-start active learning through selfsupervised language modeling',\n",
       "  'year': '2020'},\n",
       " {'authors': 'Tianyi Zhang; Felix Wu; Arzoo Katiyar; Kilian Q Weinberger; Yoav Artzi',\n",
       "  'journal': '',\n",
       "  'ref_id': 'b45',\n",
       "  'title': 'Revisiting few-sample BERT fine-tuning',\n",
       "  'year': '2021'},\n",
       " {'authors': 'Xiang Zhang; Junbo Zhao; Yann Lecun',\n",
       "  'journal': 'Advances in neural information processing systems',\n",
       "  'ref_id': 'b46',\n",
       "  'title': 'Character-level convolutional networks for text classification',\n",
       "  'year': '2015'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[26:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to group the citation data by ref_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b3': [{'Category': 'Methodological Basis', 'Citation': '(Cohn et al., 1996)', 'Explanation': 'The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.'}], 'b37': [{'Category': 'Methodological Basis', 'Citation': '(Settles, 2009)', 'Explanation': 'The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.'}], 'b4': [{'Category': 'Methodological Basis', 'Citation': '(Dasgupta, 2011)', 'Explanation': 'The cited work highlights the importance of label complexity in active learning and the need to reduce it for efficient model training, which the citing paper addresses in its research on efficient finetuning methods for PLMs in low-resource settings.'}], 'b11': [{'Category': 'Methodological Basis', 'Citation': '(Gururangan et al., 2020)', 'Explanation': 'The cited work introduces the concept of task-adaptive pre-training (TAPT), which the citing paper adopts in their research to further reduce the label complexity in AL research.'}], 'b14': [{'Category': 'Extension or Continuation', 'Citation': '(Houlsby et al., 2019)', 'Explanation': 'The cited work introduces the concept of adapters as compact modules for fine-tuning PLMs, which the citing paper extends by discussing the use of adapters for parameter-efficient fine-tuning (PEFT) in AL research.'}, {'Category': 'Methodological Basis', 'Citation': '(Houlsby et al., 2019)', 'Explanation': 'The cited work introduces the concept of trainable bottleneck layers in Transformer layers, which the citing paper adopts in the development of the Adapter PEFT technique.'}], 'b34': [{'Category': 'Data Source', 'Citation': '(Pfeiffer et al., 2023)', 'Explanation': 'The cited work discusses the use of modular learning in PEFT, which the citing paper references as a method for parameter-efficient fine-tuning in AL research.'}, {'Category': 'Methodological Basis', 'Citation': '(Pfeiffer et al., 2020)', 'Explanation': 'The cited work provides the implementation of adapters used in the citing paper, which serves as a methodological basis for the research conducted in the citing paper.'}], 'b19': [{'Category': 'Supporting Evidence', 'Citation': '(He et al., 2021;Li and Liang, 2021;Karimi Mahabadi et al., 2021)', 'Explanation': 'The cited works have revealed that PEFT methods outperform full fine-tuning in low-resource settings, which is a key finding that supports the claims made in the citing paper about the potential benefits of PEFT in this context.'}], 'b42': [{'Category': 'Supporting Evidence', 'Citation': '(Toneva et al., 2019)', 'Explanation': 'The cited work by Toneva et al. (2019) provides a method for analyzing the properties of PEFT and FFT, which the citing paper uses to understand the reason for the improved performance of PEFT in low-resource AL scenarios.'}, {'Category': 'Methodological Basis', 'Citation': '(Toneva et al., 2019)', 'Explanation': 'The cited work by Toneva et al. (2019) provides a methodology for analyzing forgetting dynamics in training examples, which the citing paper adopts to study the occurrence of forgetting events in adapters and their impact on AL data selection.'}], 'b7': [{'Category': 'Methodological Basis', 'Citation': '(Ein-Dor et al., 2020)', 'Explanation': 'The cited work by Ein-Dor et al. (2020) provides a conventional approach for integrating PLMs with AL, which the citing paper adopts in their research to investigate the use of PEFT techniques in low-resource settings.'}], 'b29': [{'Category': 'Methodological Basis', 'Citation': '(Margatina et al., 2021)', 'Explanation': 'The cited work by Margatina et al. (2021) also contributes to the research on combining PLMs with AL, providing a method for fine-tuning the model in each AL step.'}, {'Category': 'Extension or Continuation', 'Citation': '(Margatina et al., 2022)', 'Explanation': 'The cited work by Margatina et al. (2022) extends the research on the effectiveness of TAPT in enhancing AL performance by providing further insights and data.'}], 'b38': [{'Category': 'Methodological Basis', 'Citation': '(Shelmanov et al., 2021)', 'Explanation': 'The cited work by Shelmanov et al. (2021) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.'}], 'b18': [{'Category': 'Methodological Basis', 'Citation': '(Karamcheti et al., 2021)', 'Explanation': 'The cited work by Karamcheti et al. (2021) also contributes to the research on combining PLMs with AL, by exploring the use of fine-tuning in each AL step.'}], 'b35': [{'Category': 'Methodological Basis', 'Citation': '(SchrÃ¶der et al., 2022)', 'Explanation': 'The cited work by SchrÃ¶der et al. (2022) further adds to the research on integrating PLMs with AL, by discussing the use of fine-tuning in each AL step.'}, {'Category': 'Methodological Basis', 'Citation': '(SchrÃ¶der et al., 2022)', 'Explanation': 'The cited work provides a recommendation for using AUC as a suitable approximation of AL feasibility, which the citing paper adopts in their research to evaluate the performance of AL methods.'}], 'b30': [{'Category': 'Extension or Continuation', 'Citation': '(Mosbach et al., 2021)', 'Explanation': 'The cited work by Mosbach et al. (2021) extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.'}], 'b46': [{'Category': 'Extension or Continuation', 'Citation': '(Zhang et al., 2021)', 'Explanation': 'The cited work by Zhang et al. (2021) also extends the research on fine-tuning in low-resource settings, by discussing the instability of the process and its impact on AL.'}, {'Category': 'Data Source', 'Citation': '(Zhang et al., 2015)', 'Explanation': 'The cited work by Zhang et al. is the data source for the AGN dataset used in the single-text classification task in the citing paper.'}], 'b6': [{'Category': 'Data Source', 'Citation': '(Dodge et al., 2020)', 'Explanation': 'The cited work by Dodge et al. (2020) provides a data source for the research on fine-tuning in low-resource settings, by discussing the sensitivity of the process to weight initialization and data ordering.'}], 'b10': [{'Category': 'Supporting Evidence', 'Citation': '(GrieÃŸhaber et al., 2020)', 'Explanation': 'The cited work by GrieÃŸhaber et al. (2020) provides evidence that the choice of training regime is more critical than the choice of the AL method in improving AL performance.'}], 'b44': [{'Category': 'Supporting Evidence', 'Citation': '(Yuan et al., 2020)', 'Explanation': 'The cited work by Yuan et al. (2020) further supports the claim that the training regime is more important than the AL method in enhancing AL performance.'}, {'Category': 'Supporting Evidence', 'Citation': '(Yu et al., 2022)', 'Explanation': 'The cited work by Yu et al. (2022) provides additional evidence that the training regime is a critical factor in improving AL performance.'}, {'Category': 'Supporting Evidence', 'Citation': '(He et al., 2021)', 'Explanation': 'The cited work by He et al. (2021) provides evidence on the stability and generalization capabilities of adapter-based tuning in monolingual settings with scarce data.'}, {'Category': 'Methodological Basis', 'Citation': '(He et al., 2021)', 'Explanation': 'The cited work by He et al. further builds upon the research on the use of adapters in low-resource settings in the citing paper.'}, {'Category': 'Extension or Continuation', 'Citation': '(He et al., 2021)', 'Explanation': \"The citing paper continues the research on the use of adapters in low-resource settings by looking into how the models' performance changes as the training set increases.\"}, {'Category': 'Methodological Basis', 'Citation': '(He et al., 2021)', 'Explanation': 'The cited work by He et al. (2021) provides the inspiration for the layerwise examination of similarity in the citing paper, which is used to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.'}], 'b17': [{'Category': 'Extension or Continuation', 'Citation': '(JukiÄ‡ and Å najder, 2023)', 'Explanation': 'The cited work by JukiÄ‡ and Å najder (2023) continues the research on TAPT by exploring new dimensions and variables in enhancing AL performance.'}, {'Category': 'Methodological Basis', 'Citation': '(JukiÄ‡ and Å najder, 2023)', 'Explanation': 'The cited work also recommends using AUC as a summary numeric score in AL, which the citing paper adopts in their research to evaluate the performance of AL methods.'}], 'b0': [{'Category': 'Supporting Evidence', 'Citation': '(Ansell et al., 2021)', 'Explanation': 'The cited work by Ansell et al. (2021) provides evidence on the effectiveness of cross-lingual transfer for low-resource languages in the context of adapters.'}], 'b23': [{'Category': 'Supporting Evidence', 'Citation': '(Lee et al., 2022)', 'Explanation': 'The cited work by Lee et al. (2022) further supports the research on the use of adapters in low-resource settings for cross-lingual transfer.'}], 'b32': [{'Category': 'Supporting Evidence', 'Citation': '(ParoviÄ‡ et al., 2022)', 'Explanation': 'The cited work by ParoviÄ‡ et al. (2022) provides additional insights on the use of adapters in low-resource settings for cross-lingual transfer.'}], 'b26': [{'Category': 'Supporting Evidence', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work by Li and Liang (2021) supports the research on the use of adapters in monolingual settings with scarce data.'}, {'Category': 'Data Source', 'Citation': '(Li and Roth, 2002)', 'Explanation': 'The cited work by Li and Roth is the data source for the TREC dataset used in the single-text classification task in the citing paper.'}, {'Category': 'Methodological Basis', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work presents the Prefix-tuning PEFT technique, which the citing paper incorporates in the development of the UniPELT method by adding new parameters in the multi-head attention blocks of Transformer layers.'}, {'Category': 'Methodological Basis', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work by Li and Liang provides the basis for the use of adapters in low-resource settings in the citing paper.'}, {'Category': 'Extension or Continuation', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The citing paper extends the research on the use of adapters in low-resource settings by conducting a more nuanced analysis and comparing multiple adapter variants with FFT under the passive learning setup.'}, {'Category': 'Methodological Basis', 'Citation': '(Li and Liang, 2021)', 'Explanation': 'The cited work by Li and Liang (2021) is used to bolster the findings of the citing paper by exploring the stability of representations in scenarios with limited resources.'}], 'b27': [{'Category': 'Supporting Evidence', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work by Mao et al. (2022) further supports the research on the use of adapters in monolingual settings with scarce data.'}, {'Category': 'Methodological Basis', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work presents the UniPELT PEFT method, which the citing paper considers as a combination of multiple PEFT approaches, including LoRA, Prefix-tuning, and Adapter, in a single unified setup with gating mechanisms for effective activation.'}, {'Category': 'Methodological Basis', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work by Mao et al. contributes to the understanding of the use of adapters in low-resource settings in the citing paper.'}, {'Category': 'Extension or Continuation', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The citing paper further extends the research on the use of adapters in low-resource settings by generating detailed learning curves to facilitate the comparison of multiple adapters with FFT in the passive learning setup.'}, {'Category': 'Methodological Basis', 'Citation': '(Mao et al., 2022)', 'Explanation': 'The cited work by Mao et al. (2022) contributes to the analysis of the stability of representations in the citing paper, providing insights into the use of adapters in scenarios with limited resources.'}], 'b20': [{'Category': 'Supporting Evidence', 'Citation': '(Kim et al., 2021)', 'Explanation': 'The cited work by Kim et al. (2021) provides evidence that the benefits of integrating TAPT with adapters tend to taper off as the amount of data increases, which is relevant to the discussion in the citing paper about the limitations of using adapters in low-resource setups.'}], 'b31': [{'Category': 'Data Source', 'Citation': '(Pang and Lee, 2004)', 'Explanation': 'The cited work by Pang and Lee serves as the data source for the SUBJ dataset used in the citing paper for the single-text classification task.'}], 'b39': [{'Category': 'Data Source', 'Citation': '(Socher et al., 2013)', 'Explanation': 'The cited work by Socher et al. is the data source for the SST dataset used in the single-text classification task in the citing paper.'}], None: [{'Category': 'Methodological Basis', 'Citation': '(Hu et al., 2022)', 'Explanation': 'The cited work introduces the LoRA PEFT technique, which the citing paper incorporates in the development of the UniPELT method by representing an additive method that incorporates trainable low-rank decomposition matrices in the layers of a pre-trained model.'}, {'Category': 'Methodological Basis', 'Citation': '(Lewis and Gale, 1994)', 'Explanation': 'The cited work by Lewis and Gale (1994) provides the maximum entropy (ENT) strategy for sampling instances in the field of uncertainty strategies, which the citing paper adopts as a method for instance selection.'}], 'b5': [{'Category': 'Methodological Basis', 'Citation': '(Devlin et al., 2019)', 'Explanation': 'The cited work by Devlin et al. (2019) provides the base PLM (BERT) that the citing paper uses as the foundation for their research on adapters.'}], 'b8': [{'Category': 'Methodological Basis', 'Citation': '(Gal and Ghahramani, 2016)', 'Explanation': 'The cited work by Gal and Ghahramani (2016) introduces the Monte Carlo dropout (MC) method for instance selection based on the stochasticity of forward passes with dropout layers, which the citing paper utilizes in the field of uncertainty strategies.'}], 'b40': [{'Category': 'Methodological Basis', 'Citation': '(Srivastava et al., 2014)', 'Explanation': 'The cited work by Srivastava et al. (2014) presents the use of dropout layers in forward passes, which the citing paper references in the context of the Monte Carlo dropout (MC) method for instance selection in the field of uncertainty strategies.'}], 'b36': [{'Category': 'Methodological Basis', 'Citation': '(Sener and Savarese, 2018)', 'Explanation': 'The cited work by Sener and Savarese (2018) introduces the core-set (CS) method for instance selection in the field of learning representations of the acquisition model, which the citing paper adopts as a method for encouraging instance diversity.'}], 'b41': [{'Category': 'Data Source', 'Citation': '(Stephenson et al., 2021)', 'Explanation': 'The data source cited by Stephenson et al. (2021) is used to draw inspiration for the layerwise examination of similarity in the citing paper, which is conducted to analyze the effect of PEFT and FFT on AL selection with respect to their layerwise similarity to the base model.'}], 'b1': [{'Category': 'Data Source', 'Citation': '(Baldock et al., 2021)', 'Explanation': 'The data source cited by Baldock et al. (2021) is used in the citing paper to support the claim that different layers of networks specialize in different features, with earlier layers acquiring more generalized knowledge and deeper layers focusing on task-specific information.'}]}\n"
     ]
    }
   ],
   "source": [
    "# Function to regroup citations by ref_id\n",
    "def regroup_citations_by_ref_id(citations):\n",
    "    grouped_citations = {}\n",
    "    for citation in citations:\n",
    "        if 'ref_id' in citation.keys():\n",
    "            ref_id = citation['ref_id']\n",
    "            # Create a copy of the citation without the ref_id\n",
    "            citation_copy = {k: v for k, v in citation.items() if k != 'ref_id'}\n",
    "            # Append the citation to the list associated with its ref_id\n",
    "            if ref_id in grouped_citations:\n",
    "                grouped_citations[ref_id].append(citation_copy)\n",
    "            else:\n",
    "                grouped_citations[ref_id] = [citation_copy]\n",
    "    return grouped_citations\n",
    "\n",
    "\n",
    "# Regroup the citationb list by ref_id\n",
    "grouped_citations = regroup_citations_by_ref_id(annotated_article[0]['citation_data'])\n",
    "print(grouped_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the steps together into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_citation_author_year(article):\n",
    "    for citation in article['citation_data']:\n",
    "        try:\n",
    "            parsed_name = split_and_parse_citation(citation['Citation'])\n",
    "            match = match_citations_with_references(parsed_name, article['references'])\n",
    "            citation['ref_id'] = match['ref_id'] if match else None\n",
    "        except:\n",
    "            citation['ref_id'] = None\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a grouped citation data for author-year citation style, let's start solving cases with numeric-style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.2 Handle Numeric Citation Style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This style of citation seems simple at first, but there are many edge cases that we have to deal with. From my observation, there are 3 main types:\n",
    "\n",
    "- Singular citations such as [1] or [4]: These are processed conventionally, where the reference ID equals the citation number minus one.\n",
    "- Lists, for instance [1, 4, 6]: In this scenario, the citations are split into individual entries: [1], [4], and [6].\n",
    "- Ranges, like [1 - 5]: Here, the citation is divided into separate entries: [1], [2], [3], [4], [5].\n",
    "- Mixed ranges, such as [1] - [5]: These are split into distinct citations: [1] and [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_numeric_citations(citations):\n",
    "    # Helper function to parse ranges and individual numbers\n",
    "    def parse_part(part):\n",
    "        if '-' in part:  # Handle ranges\n",
    "            start, end = map(int, part.split('-'))\n",
    "            return list(range(start, end + 1))\n",
    "        else:  # Handle individual numbers\n",
    "            return [int(part)]\n",
    "\n",
    "    # Initialize the result list\n",
    "    result = []\n",
    "\n",
    "    # Find all parts of the input that match the patterns\n",
    "    parts = re.findall(r'\\[([^]]+)]', citations)\n",
    "    \n",
    "    for part in parts:\n",
    "        # For each part, remove spaces, split by commas and extend the result list\n",
    "        for subpart in part.replace(' ', '').split(','):\n",
    "            try:\n",
    "                result.extend(parse_part(subpart))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return [f\"[{num}]\" for num in result]\n",
    "\n",
    "# Function to apply citation splitting to a list of citation entries\n",
    "def split_citations_in_entries(citation_entries):\n",
    "    expanded_citation_entries = []\n",
    "    for entry in citation_entries:\n",
    "        try:\n",
    "        # Use the split_citations function to get a list of individual citations from the Citation field\n",
    "            split_citations_list = split_numeric_citations(entry['Citation'])\n",
    "            for citation in split_citations_list:\n",
    "                # Create a new citation entry for each split citation, keeping other fields the same\n",
    "                \n",
    "                new_entry = {\n",
    "                    'Citation': citation,\n",
    "                    'Category': entry['Category'],\n",
    "                    'Explanation': entry['Explanation']\n",
    "                }\n",
    "                expanded_citation_entries.append(new_entry)\n",
    "        except:\n",
    "            continue\n",
    "    return expanded_citation_entries\n",
    "\n",
    "\n",
    "def match_numeric_citation(citations):\n",
    "    for citation in citations:\n",
    "        # Regular expression to find single numbers inside square brackets\n",
    "        pattern = re.compile(r'\\[\\(?(?P<number>\\d+)\\)?\\]')\n",
    "        try:\n",
    "            #Find all matches in the text and convert them to integers\n",
    "            reference_num = [int(match.group('number')) for match in pattern.finditer(citation['Citation'])][0]\n",
    "            citation['ref_id'] = f\"b{reference_num -1}\"\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return citations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Category': 'Methodological Basis',\n",
       "  'Citation': '[29,72]',\n",
       "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[52]',\n",
       "  'Explanation': 'The cited work uses auxiliary maps as input to guide the training process, which the citing paper adopts in its research on saliency object detection.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[46,14]',\n",
       "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[58,60]',\n",
       "  'Explanation': 'The cited works provide a method of using multiple heads in a single encoder to learn different semantic information, which the citing paper adopts as a basis for their research on saliency map prediction.'},\n",
       " {'Category': 'Methodological Basis',\n",
       "  'Citation': '[74]',\n",
       "  'Explanation': 'The cited work highlights the inefficiency of using multiple branches in parallel for saliency map prediction, which the citing paper uses to guide their method design for efficient saliency map prediction.'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[108]['citation_data'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_article[108]['citation_data'] = split_citations_in_entries(annotated_article[108]['citation_data'])\n",
    "annotated_article[108]['citation_data'] =  match_numeric_citation(annotated_article[108]['citation_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Citation': '[29]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b28'},\n",
       " {'Citation': '[72]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works provide a method of using the features learned by predicting different auxiliary maps to assist in predicting saliency maps, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b71'},\n",
       " {'Citation': '[52]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited work uses auxiliary maps as input to guide the training process, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b51'},\n",
       " {'Citation': '[46]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b45'},\n",
       " {'Citation': '[14]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works introduce a boundary-aware loss to make the models pay more attention to edge pixels, which the citing paper adopts in its research on saliency object detection.',\n",
       "  'ref_id': 'b13'},\n",
       " {'Citation': '[58]',\n",
       "  'Category': 'Methodological Basis',\n",
       "  'Explanation': 'The cited works provide a method of using multiple heads in a single encoder to learn different semantic information, which the citing paper adopts as a basis for their research on saliency map prediction.',\n",
       "  'ref_id': 'b57'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[108]['citation_data'][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, single citation like [14], [58] will be parsed normaly. But for citation like [29, 72], they will get split to 2 separated citations [29] and [72].\n",
    "\n",
    "Now, let's combine the steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proprocess_citation_numeric(article):\n",
    "    \n",
    "    article['citation_data'] = split_citations_in_entries(article['citation_data'])\n",
    "    article['citation_data'] = match_numeric_citation(article['citation_data'])\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.3 Process 2 citation style**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to detect the citation style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_citation_style(text):\n",
    "    # Pattern to match numeric citations like [1], [1, 2], [1-6], [1, 2-6], [1, 2, 3-6], etc.\n",
    "    numeric_pattern = re.compile(r'\\[\\d+(-\\d+)?(,\\s*\\d+(-\\d+)?)*\\]')\n",
    "    # Pattern for \"Author-Year\" citations like (Author, Year)\n",
    "    author_year_pattern = re.compile(r'\\([A-Za-z]+,\\s*\\d{4}\\)')\n",
    "\n",
    "    # Check for numeric citation style\n",
    "    if numeric_pattern.search(text):\n",
    "        return \"Numeric\"\n",
    "    # Check for author-year citation style\n",
    "    elif author_year_pattern.search(text):\n",
    "        return \"Author-Year\"\n",
    "    else:\n",
    "        return \"Author-Year\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author-Year'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_citation_style(\"(Amin et al., 2019)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Numeric'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_citation_style(\"[1,6]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7243/7243 [00:12<00:00, 586.61it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for article in tqdm(annotated_article, total=len(annotated_article)):\n",
    "    references = article['references']\n",
    "    if len(article['citation_data']) == 0:\n",
    "        continue\n",
    "    citation_style = detect_citation_style(article['citation_data'][0][\"Citation\"])\n",
    "    # try:\n",
    "    if citation_style == \"Author-Year\":\n",
    "        article = preprocess_citation_author_year(article)\n",
    "    elif citation_style == \"Numeric\":\n",
    "        article = proprocess_citation_numeric(article)\n",
    "    else:\n",
    "        print(f\"Uncertain citation style: {citation_style}\")\n",
    "        continue\n",
    "    # except Exception as e:\n",
    "    #     print(article['citation_data'])\n",
    "    #     print(e)\n",
    "    #     break\n",
    "\n",
    "    grouped_citations = regroup_citations_by_ref_id(article['citation_data'])\n",
    "    article['grouped_citations'] = grouped_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b14': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work by Liu et al. introduces a fusion forget gate to control the flow of information between multimodal sequences, which the citing paper adopts as a method to address the problem of redundancy and noise in video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work proposed a multistage fusion network with a fusion forget gate module for controlling redundant information in multimodal long sequences, which the citing paper adopts in their video multimodal summarization research.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2018b)',\n",
       "   'Explanation': 'The cited work introduces the LMF model, which the citing paper references for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2018b)',\n",
       "   'Explanation': 'The cited work by Liu et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work by Liu et al. provides a method for multimodal summarization that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2018b)',\n",
       "   'Explanation': 'The cited work introduces the LMF model as an advanced version of the TFN model, which the citing paper may have used in their research for multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Liu et al., 2020)',\n",
       "   'Explanation': 'The cited work presents the multistage fusion network with the fusion forget gate module, which the citing paper adopts to control the flow of redundant information between multimodal long sequences.'}],\n",
       " 'b16': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Nagrani et al., 2021)',\n",
       "   'Explanation': 'The cited work by Nagrani et al. (2021) introduces the concept of a bottleneck module, which the citing paper adopts to restrict redundant and noisy information in video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Nagrani et al., 2021)',\n",
       "   'Explanation': 'The cited work by Nagrani et al. provides a foundational understanding of the redundant and noisy nature of audio and visual inputs in video, which the citing paper leverages in their research to focus on removing noise and preserving critical information.'}],\n",
       " 'b4': [{'Category': 'Data Source',\n",
       "   'Citation': '(Gutmann and HyvÃ¤rinen, 2010)',\n",
       "   'Explanation': 'The cited work by Gutmann and HyvÃ¤rinen (2010) provides the noise-contrastive estimation framework, which the citing paper utilizes to maximize mutual information between fusion results and unimodal inputs.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Gutmann and HyvÃ¤rinen, 2010)',\n",
       "   'Explanation': 'The cited work introduces the noise-contrastive estimation framework, which the citing paper adopts to produce the InfoNCE loss for measuring the lower bound of mutual information in the fusion process.'}],\n",
       " 'b36': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2016)',\n",
       "   'Explanation': 'The cited work, MOSI, is a dataset used in the experiments conducted in the citing paper to evaluate the performance of the proposed model in the field of multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2018b)',\n",
       "   'Explanation': 'The cited work, MOSEI, is another dataset used in the experiments of the citing paper to further assess the performance of the model in the same field of multimodal sentiment analysis.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Li et al., 2017)',\n",
       "   'Explanation': 'The cited work introduced a multimodal summarization dataset in Chinese and English, which the citing paper utilizes in their research on video multimodal summarization.'},\n",
       "  {'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Zadeh et al., 2016)',\n",
       "   'Explanation': 'The cited work provides the MOSI dataset, which is used in the citing paper for video multimodal sentiment analysis.'},\n",
       "  {'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Zadeh et al., 2018b)',\n",
       "   'Explanation': 'The cited work provides the MOSEI dataset, which is used in the citing paper for video multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2017)',\n",
       "   'Explanation': 'The cited work presents the TFN model, which the citing paper uses as a method for text generation.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2017)',\n",
       "   'Explanation': 'The cited work by Zadeh et al. also serves as a methodological basis for the comparison in the citing paper, as it provides a method for multimodal sentiment analysis that is used to compare against the performance of DBF.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Zadeh et al., 2017)',\n",
       "   'Explanation': 'The cited work presents the TFN model based on tensor outer product to capture multiple-modal interactions, which the citing paper uses in their research for multimodal sentiment analysis.'}],\n",
       " 'b22': [{'Category': 'Data Source',\n",
       "   'Citation': '(Sanabria et al., 2018)',\n",
       "   'Explanation': \"The cited work, How2, is a benchmark dataset used in the experiments of the citing paper to evaluate the model's performance in the field of multimodal summarization.\"},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Sanabria et al., 2018)',\n",
       "   'Explanation': 'The cited work proposed the How2 dataset of short instructional videos with summaries, which the citing paper uses in their study of video multimodal summarization.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Sanabria et al., 2018)',\n",
       "   'Explanation': 'The cited work provides the How2 dataset, which is a large-scale collection of instructional videos and their corresponding transcripts, that the citing paper uses as a benchmark for the summary task.'}],\n",
       " None: [{'Category': 'Methodological Basis',\n",
       "   'Explanation': 'The cited work in the last sentence of the response is not provided in the format of a citation number. However, it is worth noting that the cited work is likely to be a methodological basis for the model proposed in the citing paper, as it is used to demonstrate the effectiveness of the model in reducing noise and retaining key information in video multimodal fusion.'}],\n",
       " 'b28': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Vaswani et al., 2017)',\n",
       "   'Explanation': 'The cited work by Vaswani et al. (2017) introduced the concept of Transformer, which has influenced the design of more recent models in the field of video multimodal fusion.'}],\n",
       " 'b37': [{'Category': 'Extension or Continuation',\n",
       "   'Citation': '(Zhang et al., 2019)',\n",
       "   'Explanation': 'The cited work by Zhang et al. (2019) built upon the use of cross attention mechanism in the decoder of Transformer to perform multimodal translation tasks, which is further extended in the field of video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work builds a hierarchical mutual information maximization guided model that the citing paper uses to improve the fusion outcome and performance in the downstream multimodal sentiment analysis task.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work introduces the concept of mutual information in information theory, which the citing paper utilizes to estimate the relationship between pairs of variables and capture modality-invariant cues in fusion results.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': 'Facet1',\n",
       "   'Explanation': 'The Facet1 model is cited as the source of the facial expression features used in the citing paper for sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work by Han et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Han et al., 2021)',\n",
       "   'Explanation': 'The cited work presents the MMIM model that hierarchically maximizes the mutual information in unimodal input pairs and between multimodal fusion result and unimodal input, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b30': [{'Category': 'Extension or Continuation',\n",
       "   'Citation': '(Wu et al., 2021)',\n",
       "   'Explanation': 'The cited work by Wu et al. (2021) proposed a text-centric multimodal fusion shared private framework for multimodal fusion, which builds upon the use of crossmodal prediction and sentiment regression parts in the field of video multimodal fusion.'}],\n",
       " 'b25': [{'Category': 'Data Source',\n",
       "   'Citation': '(Sun et al., 2019)',\n",
       "   'Explanation': 'The cited work by Sun et al. (2019) introduced the use of three pre-training tasks in video-language pre-training, which is a data source for the field of video multimodal fusion.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Sun et al., 2020)',\n",
       "   'Explanation': 'The cited work introduces the ICCN model, which the citing paper uses in their research for text generation.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Sun et al., 2020)',\n",
       "   'Explanation': 'The cited work by Sun et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Sun et al., 2020)',\n",
       "   'Explanation': 'The cited work introduces the ICCN model with an adversarial encoder-decoder classifier framework to learn a modality-invariant embedding space, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b32': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work first introduced pre-trained language models in multimodal summarization and experimented with the optimal injection layer of visual features, which the citing paper builds upon in their video multimodal summarization research.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021b)',\n",
       "   'Explanation': 'The cited work introduces a multi-label training scheme that the citing paper adopts to generate extra unimodal labels and train the model concurrently with the main task.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work provides a set of evaluation metrics for summarization, which the citing paper adopts to assess the performance of the generated abstractive summaries in the How2 dataset.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021b)',\n",
       "   'Explanation': 'The cited work by Yu et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work by Yu et al. provides a method for multimodal summarization that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021b)',\n",
       "   'Explanation': 'The cited work introduces the Self-MM model with a label generation module based on self-supervised learning strategy to acquire independent unimodal supervision, which the citing paper may have used in their research for multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Yu et al., 2021a)',\n",
       "   'Explanation': 'The cited work introduces the BART-based and vision guided model for multimodal summarization task, which the citing paper uses as a reference for incorporating visual information in the model.'}],\n",
       " 'b15': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Luo et al., 2021)',\n",
       "   'Explanation': 'The cited work proposes a multiscale fusion method that the citing paper uses to align different granularity information from multiple modalities in multimodal sentiment analysis.'}],\n",
       " 'b17': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Oord et al., 2018)',\n",
       "   'Explanation': 'The cited work presents the InfoNCE loss as a method for measuring the lower bound of mutual information in the fusion process, which the citing paper incorporates into the noise-contrastive estimation framework to produce the final loss function.'}],\n",
       " 'b7': [{'Category': 'Supporting Evidence',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work by Hazarika et al. provides the metric set used to evaluate sentiment intensity predictions in the citing paper, serving as a foundational element for the research conducted.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work presents the MISA model, which the citing paper references for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work by Hazarika et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Hazarika et al., 2020)',\n",
       "   'Explanation': 'The cited work presents the MISA model that projects each modality to two distinct subspaces, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b3': [{'Category': 'Data Source',\n",
       "   'Citation': '(Devlin et al., 2018)',\n",
       "   'Explanation': 'The cited work by Devlin et al. (2018) is the source of the BERT-base model used in the citing paper for text input encoding and [CLS] embedding extraction.'}],\n",
       " 'b1': [{'Category': 'Data Source',\n",
       "   'Citation': '(Degottex et al., 2014)',\n",
       "   'Explanation': 'The cited work by Degottex et al. (2014) is the source of the COVAREP model used in the citing paper to extract audio features for sentiment analysis.'},\n",
       "  {'Category': 'Data Source',\n",
       "   'Citation': '(Degottex et al., 2014)',\n",
       "   'Explanation': 'The cited work by Degottex et al. (2014) is the source of the COVAREP model used in the citing paper to extract audio features for sentiment analysis.'}],\n",
       " 'b27': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the MulT model, which the citing paper adopts in their research for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2018)',\n",
       "   'Explanation': 'The cited work presents the MFM model, which the citing paper adopts in their research for text generation tasks.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2019)',\n",
       "   'Explanation': 'The cited work by Tsai et al. serves as a methodological basis for the comparison in the citing paper, as it provides a method for multimodal sentiment analysis that is used to compare against the performance of DBF.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2018)',\n",
       "   'Explanation': 'The cited work by Tsai et al. provides a method for multimodal sentiment analysis that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the MulT model with directional pairwise cross-attention, which the citing paper adopts in their research for multimodal sentiment analysis.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Tsai et al., 2018)',\n",
       "   'Explanation': 'The cited work presents the MFM model that factorizes representations into two sets of independent factors, which the citing paper may have used in their research for multimodal sentiment analysis.'}],\n",
       " 'b9': [{'Category': 'Data Source',\n",
       "   'Citation': '(Lewis et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the BART model, which the citing paper uses as a feature extractor in their research for text generation tasks.'}],\n",
       " 'b8': [{'Category': 'Data Source',\n",
       "   'Citation': '(Kay et al., 2017)',\n",
       "   'Explanation': 'The cited work provides the Kinetics dataset, which is used as a pre-training data source for the vision model in the citing paper.'}],\n",
       " 'b20': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Paszke et al., 2017)',\n",
       "   'Explanation': 'The cited work introduces the deep learning framework PyTorch, which the citing paper uses to implement the code for sentiment analysis and summarization experiments.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Paszke et al., 2017)',\n",
       "   'Explanation': 'The cited work also provides the deep learning framework PyTorch, which the citing paper uses to implement the code for sentiment analysis and summarization experiments.'}],\n",
       " 'b18': [{'Category': 'Methodological Basis',\n",
       "   'Citation': '(Palaskar et al., 2019)',\n",
       "   'Explanation': 'The cited work by Palaskar et al. provides a method for multimodal summarization that is used in the comparison in the citing paper.'},\n",
       "  {'Category': 'Methodological Basis',\n",
       "   'Citation': '(Palaskar et al., 2019)',\n",
       "   'Explanation': 'The cited work introduces the sequence-to-sequence multimodal fusion model with hierarchical attention, which serves as the basis for the DBF model in the citing paper.'}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[17]['grouped_citations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Building citation graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.1 Parsing annotated triplets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_dict = {\n",
    "    \"Supporting Evidence\": \"Is Evidence For\",\n",
    "    \"Methodological Basis\": \"Is Methodological Basis For\",\n",
    "    \"Theoretical Foundation\": \"Is Theoretical Foundation For\", \n",
    "    \"Data Source\": \"Is Data Source For\",\n",
    "    \"Extension or Continuation\": \"Is Extension or Continuation Of\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have grouped citation data; now we need to find the papers cited in the arXiv dataset by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7243/7243 [00:01<00:00, 4255.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_data['title'] = df_data['title'].str.lower()\n",
    "# titles = df_data['title'].tolist()\n",
    "title_dict = {title.lower(): arxiv_id for title, arxiv_id in zip(df_data['title'].tolist(), df_data['id'].to_list())}\n",
    "\n",
    "\n",
    "def search_paper_by_name(name):\n",
    "    # matches = df_data['title'].str.contains(name, case=False, na=False, regex=False)\n",
    "    # filtered_df = df_data[matches]\n",
    "    # if len(filtered_df) == 0:\n",
    "    #     return None\n",
    "    # return filtered_df.iloc[0]['id']\n",
    "    try:\n",
    "        return title_dict[name]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
    "\n",
    "    article_dict[\"arxiv_id\"] = search_paper_by_name(article_dict['title'].lower())\n",
    "\n",
    "    if \"grouped_citations\" in article_dict.keys():\n",
    "        article_dict[\"mapped_citation\"] = {}\n",
    "        for key,val in article_dict['grouped_citations'].items():\n",
    "            for ref in article_dict[\"references\"]:\n",
    "                if ref[\"ref_id\"] == key:\n",
    "                    title = ref[\"title\"]\n",
    "\n",
    "            title = title.lower()\n",
    "            arxiv_id = search_paper_by_name(title)\n",
    "            article_dict['mapped_citation'][key] = {\"title\": title, 'arxiv_id': arxiv_id, 'citation': val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b11': {'title': 'arcface: additive angular margin loss for deep face recognition',\n",
       "  'arxiv_id': '1801.07698',\n",
       "  'citation': [{'Citation': '[12]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide image-to-image retrieval approaches that the citing paper adopts in its research to conduct image retrieval.'}]},\n",
       " 'b12': {'title': 'spherereid: deep hypersphere manifold embedding for person re-identification',\n",
       "  'arxiv_id': '1807.00537',\n",
       "  'citation': [{'Citation': '[13]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide image-to-image retrieval approaches that the citing paper adopts in its research to conduct image retrieval.'}]},\n",
       " 'b13': {'title': \"learning visual features from product title for image retrieval (mm '20)\",\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[14]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide image-to-image retrieval approaches that the citing paper adopts in its research to conduct image retrieval.'}]},\n",
       " 'b49': {'title': 'mining hard samples globally and efficiently for person reidentification',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[50]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide image-to-image retrieval approaches that the citing paper adopts in its research to conduct image retrieval.'},\n",
       "   {'Citation': '[50]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited works on metric learning provide evidence that the global-wise classification method used in the citing paper can be a good choice for training the model due to its use of more negative samples in the training set and potential to learn a better metric.'}]},\n",
       " 'b54': {'title': 'cosface: large margin cosine loss for deep face recognition',\n",
       "  'arxiv_id': '1801.09414',\n",
       "  'citation': [{'Citation': '[55]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide image-to-image retrieval approaches that the citing paper adopts in its research to conduct image retrieval.'}]},\n",
       " 'b6': {'title': 'probabilistic embeddings for cross-modal retrieval',\n",
       "  'arxiv_id': '2101.05068',\n",
       "  'citation': [{'Citation': '[7]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works present text-to-image retrieval methods that the citing paper utilizes in its study of image retrieval.'}]},\n",
       " 'b18': {'title': 'cross-modal retrieval and synthesis (x-mrs): closing the modality gap in shared subspace learning',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[19]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works present text-to-image retrieval methods that the citing paper utilizes in its study of image retrieval.'}]},\n",
       " 'b28': {'title': \"cross-domain image retrieval with attention modeling (mm '17)\",\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[29]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works present text-to-image retrieval methods that the citing paper utilizes in its study of image retrieval.'}]},\n",
       " 'b56': {'title': 'point to rectangle matching for image text retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[57]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works present text-to-image retrieval methods that the citing paper utilizes in its study of image retrieval.'}]},\n",
       " 'b65': {'title': 'deep supervised cross-modal retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[66]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works present text-to-image retrieval methods that the citing paper utilizes in its study of image retrieval.'}]},\n",
       " 'b67': {'title': 'dual-path convolutional image-text embeddings with instance loss',\n",
       "  'arxiv_id': '1711.05535',\n",
       "  'citation': [{'Citation': '[68]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works present text-to-image retrieval methods that the citing paper utilizes in its study of image retrieval.'}]},\n",
       " 'b5': {'title': 'image search with text feedback by visiolinguistic attention learning',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[6]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works address the task of language-guided image retrieval, which the citing paper builds upon to further explore the field of image retrieval with a focus on interactive systems and the use of reference images and relative captions in the query.'},\n",
       "   {'Citation': '[6]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work VAL introduces a visual-linguistic attention learning framework that the citing paper uses to modify the visual features of the reference image in a language-guided image retrieval task.'},\n",
       "   {'Citation': '[6]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the basis for the image-text compositors used in the citing paper, with a focus on the residual form of the composition function.'},\n",
       "   {'Citation': '[6]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'},\n",
       "   {'Citation': '[6]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, VAL, is used as a reference for the design of the co-training strategy in the citing paper, which enables the sharing of knowledge among multiple compositors for improved performance in image retrieval tasks.'}]},\n",
       " 'b29': {'title': 'dual compositional learning in interactive image retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[30]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works address the task of language-guided image retrieval, which the citing paper builds upon to further explore the field of image retrieval with a focus on interactive systems and the use of reference images and relative captions in the query.'}]},\n",
       " 'b34': {'title': 'cosmo: content-style modulation for image retrieval with text feedback',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[35]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works address the task of language-guided image retrieval, which the citing paper builds upon to further explore the field of image retrieval with a focus on interactive systems and the use of reference images and relative captions in the query.'},\n",
       "   {'Citation': '[35]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work CoSMo introduces the content and style modulators that the citing paper utilizes to modify the visual features of the reference image in a language-guided image retrieval task.'},\n",
       "   {'Citation': '[35]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the basis for the image-text compositors used in the citing paper, with a focus on the residual form of the composition function.'},\n",
       "   {'Citation': '[35]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'},\n",
       "   {'Citation': '[35]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, CoSMo, is adopted as the image-text compositor in the citing paper to stabilize the training procedure of the content modulator.'},\n",
       "   {'Citation': '[35]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, CoSMo, is employed in the image-text compositor of the citing paper, providing a specific method for generating composed features in the model training process.'},\n",
       "   {'Citation': '[35]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited work is used as the baseline method, with the text encoder being replaced by RoBERTa. The mention of the baseline method and the change in the text processing approach highlights the reliance on external data and a pre-existing model in the study conducted in the citing paper.'}]},\n",
       " 'b52': {'title': 'composing text and image for image retrieval an empirical odyssey',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[53]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works address the task of language-guided image retrieval, which the citing paper builds upon to further explore the field of image retrieval with a focus on interactive systems and the use of reference images and relative captions in the query.'},\n",
       "   {'Citation': '[53]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work TIRG proposes a gating and residual module that the citing paper adopts to modify the visual features of the reference image in a language-guided image retrieval task.'},\n",
       "   {'Citation': '[53]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the basis for the image-text compositors used in the citing paper, with a focus on the residual form of the composition function.'},\n",
       "   {'Citation': '[53]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'},\n",
       "   {'Citation': '[53]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited work is the source of the Fashion200k dataset used in the evaluation of the citing paper.'}]},\n",
       " 'b57': {'title': 'comprehensive linguistic-visual composition network for image retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[58]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works address the task of language-guided image retrieval, which the citing paper builds upon to further explore the field of image retrieval with a focus on interactive systems and the use of reference images and relative captions in the query.'},\n",
       "   {'Citation': '[58]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work CLVC-Net uses local-wise and global-wise composition modules that the citing paper adopts to further improve the multi-modal fusion modules in a language-guided image retrieval task.'},\n",
       "   {'Citation': '[58]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the basis for the image-text compositors used in the citing paper, with a focus on the residual form of the composition function.'},\n",
       "   {'Citation': '[58]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'},\n",
       "   {'Citation': '[58]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, CLVC-Net, provides a method of local-wise and global-wise composition modules that the citing paper adopts in its own research to improve the learning process.'},\n",
       "   {'Citation': '[58]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, CLVC-Net, is used to compare the design of the co-training strategy in the citing paper, highlighting the differences in the use of global-wise and local-wise learning in the two approaches.'}]},\n",
       " 'b0': {'title': 'conditioned and composed image retrieval combining and partially fine-tuning clip-based features',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[1]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work CLIP4cir finetunes the CLIP text encoder and trains a combiner network to fuse the visual and textual features in a language-guided image retrieval task, which the citing paper may have also considered in their research.'}]},\n",
       " 'b26': {'title': 'cognitive and consensus processes in group recognition memory performance',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[27]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work provides the psychological finding that groups perform better than individuals on the memory task, which serves as the basis for the method developed in the citing paper to alleviate the triplet ambiguity by using a consensus module composed of compositors with diverse knowledge to make decisions during evaluation.'}]},\n",
       " 'b32': {'title': 'on information and sufficiency',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[33]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work introduces the Kullback Leibler divergence loss (KL loss) that is employed in the citing paper to encourage learning among the compositors and minimize their biases learned on noisy triplets, which is a methodological basis for the method developed in the paper.'}]},\n",
       " 'b36': {'title': 'feature pyramid networks for object detection',\n",
       "  'arxiv_id': '1612.03144',\n",
       "  'citation': [{'Citation': '[37]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the finding that the image features of high-resolution are semantically weak, while the image features of low-resolution are semantically strong, which motivates the use of two image-text compositors at different depths of the same image encoder in the method developed in the citing paper to ensure that the compositors possess distinct knowledge.'},\n",
       "   {'Citation': '[37]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the finding that the image features of high-resolution are semantically weak, while the image features of low-resolution are semantically strong, which inspires the development of a pyramid training paradigm for image-text compositors in the citing paper.'}]},\n",
       " 'b41': {'title': 'thinking fast and slow: efficient text-to-visual retrieval with transformers',\n",
       "  'arxiv_id': '2103.16553',\n",
       "  'citation': [{'Citation': '[42]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the finding that the image features of high-resolution are semantically weak, while the image features of low-resolution are semantically strong, which motivates the use of two image-text compositors at different depths of the same image encoder in the method developed in the citing paper to ensure that the compositors possess distinct knowledge.'},\n",
       "   {'Citation': '[42]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works provide the finding that the image features of high-resolution are semantically weak, while the image features of low-resolution are semantically strong, which inspires the development of a pyramid training paradigm for image-text compositors in the citing paper.'}]},\n",
       " 'b4': {'title': 'learning joint visual semantic matching embeddings for language-guided retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[5]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'}]},\n",
       " 'b16': {'title': 'image search with text feedback by deep hierarchical attention mutual information maximization',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[17]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'}]},\n",
       " 'b60': {'title': 'cross-modal joint prediction and alignment for composed query image retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[61]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'}]},\n",
       " 'b62': {'title': 'heterogeneous feature fusion and cross-modal alignment for composed image retrieval',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[63]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'}]},\n",
       " 'b64': {'title': 'progressive learning for image retrieval with hybrid-modality queries',\n",
       "  'arxiv_id': '2204.11212',\n",
       "  'citation': [{'Citation': '[65]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited works have made significant progress in the field of language-guided image retrieval, which serves as a foundational basis for the citing paper to develop a more effective and efficient approach for the task of image retrieval with flexible queries.'}]},\n",
       " 'b2': {'title': 'combining labeled and unlabeled data with co-training',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[3]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work introduces the concept of co-training, which the citing paper adopts in the context of language-guided image retrieval to acquire complementary information on two views of the data.'}]},\n",
       " 'b45': {'title': 'deep co-training for semi-supervised image recognition',\n",
       "  'arxiv_id': '1803.05984',\n",
       "  'citation': [{'Citation': '[46]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited work in image recognition is extended in the citing paper to explore the co-training technique in a new research field of language-guided image retrieval.'}]},\n",
       " 'b43': {'title': 'deep co-training for semi-supervised image segmentation',\n",
       "  'arxiv_id': '1903.11233',\n",
       "  'citation': [{'Citation': '[44]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited work in semantic segmentation is extended in the citing paper to further examine the co-training approach in a new research context of language-guided image retrieval.'}]},\n",
       " 'b40': {'title': 'taking a closer look at domain shift: category-level adversaries for semantics consistent domain adaptation',\n",
       "  'arxiv_id': '1809.09478',\n",
       "  'citation': [{'Citation': '[41]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited work in domain adaptation is extended in the citing paper to explore the co-training paradigm in a new research area of language-guided image retrieval.'}]},\n",
       " 'b47': {'title': 'maximum classifier discrepancy for unsupervised domain adaptation',\n",
       "  'arxiv_id': '1712.02560',\n",
       "  'citation': [{'Citation': '[48]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited work in domain adaptation is further extended in the citing paper to examine the co-training approach in a new research context of language-guided image retrieval.'}]},\n",
       " 'b66': {'title': 'unsupervised scene adaptation with memory regularization in vivo',\n",
       "  'arxiv_id': '1912.11164',\n",
       "  'citation': [{'Citation': '[67]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited work in domain adaptation is again extended in the citing paper to explore the co-training technique in a new research field of language-guided image retrieval.'}]},\n",
       " 'b17': {'title': 'multimodal compatibility modeling via exploring the consistent and complementary correlations',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[18]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work [18] explores the consistent and complementary correlations of multi-modal data, which the citing paper adopts to guide the consensus between compositors and rectify the single prediction in their research.'}]},\n",
       " 'b24': {'title': 'deep residual learning for image recognition',\n",
       "  'arxiv_id': '1512.03385',\n",
       "  'citation': [{'Citation': '[25]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, ResNet, is used as the image encoder in the Consensus Network, providing the method for extracting mid-level and high-level representations of input images.'},\n",
       "   {'Citation': '[25]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The image encoders for the Shoes and FashionIQ datasets are specified as ResNet-50, while the image encoder for the Fashion200k dataset is mentioned as ResNet-18. This information is provided to acknowledge the data source and the specific encoders used for each dataset in the study.'}]},\n",
       " 'b1': {'title': 'automatic attribute discovery and characterization from noisy web data',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[2]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited work is the source of the Shoes dataset used in the evaluation of the citing paper.'},\n",
       "   {'Citation': '[2]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': 'The cited work provides the Shoes dataset that is used in the experiments conducted in the citing paper to evaluate the performance of batch-based and global-wise methods in the context of triplet ambiguity.'}]},\n",
       " 'b59': {'title': 'fashion iq: a new dataset towards retrieving images by natural language feedback',\n",
       "  'arxiv_id': '1905.12794',\n",
       "  'citation': [{'Citation': '[60]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited work is the source of the FashionIQ dataset used in the evaluation of the citing paper.'}]},\n",
       " 'b15': {'title': 'long short-term memory. supervised sequence labelling with recurrent neural networks',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[16]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, LSTM, is replaced by RoBERTa in the text encoder of the baseline method, indicating a methodological change in the text processing approach.'}]},\n",
       " 'b38': {'title': 'roberta: a robustly optimized bert pretraining approach',\n",
       "  'arxiv_id': '1907.11692',\n",
       "  'citation': [{'Citation': '[39]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited work, RoBERTa, is mentioned as the text encoder in the baseline method. This information is provided to acknowledge the data source and the specific model used in the text processing approach.'}]},\n",
       " 'b31': {'title': 'adam: a method for stochastic optimization',\n",
       "  'arxiv_id': '1412.6980',\n",
       "  'citation': [{'Citation': '[32]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work by Adam provides the optimizer used in the citing paper for training the text encoder and other modules in the model.'}]},\n",
       " 'b25': {'title': 'in defense of the triplet loss for person re-identification',\n",
       "  'arxiv_id': '1703.07737',\n",
       "  'citation': [{'Citation': '[26]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited works on metric learning provide evidence that the global-wise classification method used in the citing paper can be a good choice for training the model due to its use of more negative samples in the training set and potential to learn a better metric.'}]},\n",
       " 'b55': {'title': 'cross-batch memory for embedding learning',\n",
       "  'arxiv_id': '1912.06798',\n",
       "  'citation': [{'Citation': '[56]',\n",
       "    'Category': 'Data Source',\n",
       "    'Explanation': 'The cited works on metric learning provide evidence that the global-wise classification method used in the citing paper can be a good choice for training the model due to its use of more negative samples in the training set and potential to learn a better metric.'}]},\n",
       " 'b3': {'title': 'a simple framework for contrastive learning of visual representations',\n",
       "  'arxiv_id': '2002.05709',\n",
       "  'citation': [{'Citation': '[4]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works on self-supervised learning in the citing paper extend the research by exploring new dimensions and variables in the training process, potentially leading to improved model performance.'}]},\n",
       " 'b23': {'title': 'momentum contrast for unsupervised visual representation learning',\n",
       "  'arxiv_id': '1911.05722',\n",
       "  'citation': [{'Citation': '[24]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The cited works on self-supervised learning in the citing paper extend the research by exploring new dimensions and variables in the training process, potentially leading to improved model performance.'}]},\n",
       " 'b27': {'title': 'long short-term memory',\n",
       "  'arxiv_id': None,\n",
       "  'citation': [{'Citation': '[28]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited work, which uses LSTM as a text encoder, is adopted as a baseline in the citing paper to improve the text encoding capabilities and achieve better results in capturing textual information.'}]}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_article[1024]['mapped_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../outputs/annotated_articles.json\", \"w\") as f:\n",
    "    json.dump(annotated_article, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b3': {'title': 'an image is worth 16x16 words: transformers for image recognition at scale',\n",
       "  'arxiv_id': '2010.11929',\n",
       "  'citation': [{'Citation': '[4]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b6': {'title': 'swin transformer v2: scaling up capacity and resolution',\n",
       "  'arxiv_id': '2111.09883',\n",
       "  'citation': [{'Citation': '[7]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The cited works introduce the concept of using image transformers as a backbone for video descriptor extraction, which the citing paper adopts for its video copy detection model.'}]},\n",
       " 'b0': {'title': 'a simple framework for contrastive learning of visual representations',\n",
       "  'arxiv_id': '2002.05709',\n",
       "  'citation': [{'Citation': '[1]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': \"The cited work, SimCLR, provides the method of combining with entropy loss for self-supervised learning, which is utilized in the training process of the citing paper's basic model.\"}]},\n",
       " 'b7': {'title': 'a self-supervised descriptor for image copy detection',\n",
       "  'arxiv_id': '2202.10261',\n",
       "  'citation': [{'Citation': '[8]',\n",
       "    'Category': 'Extension or Continuation',\n",
       "    'Explanation': 'The citing paper builds upon the SSCD method to train its basic model in a self-supervised manner, indicating an extension of the research conducted in the cited work.'}]},\n",
       " 'b9': {'title': 'spreading vectors for similarity search',\n",
       "  'arxiv_id': '1806.03198',\n",
       "  'citation': [{'Citation': '[10]',\n",
       "    'Category': 'Supporting Evidence',\n",
       "    'Explanation': \"The cited work proposes the entropy loss used in the training process of the citing paper's basic model, providing a foundational element for the loss function formulation.\"}]},\n",
       " 'b8': {'title': 'learning transferable visual models from natural language supervision',\n",
       "  'arxiv_id': '2103.00020',\n",
       "  'citation': [{'Citation': '[9]',\n",
       "    'Category': 'Methodological Basis',\n",
       "    'Explanation': 'The citing paper adopts the CLIP model to extract frame features without post-processing, which serves as the foundation for the edited video detection method proposed in the study.'}]},\n",
       " 'b5': {'title': 'roberta: a robustly optimized bert pretraining approach',\n",
       "  'arxiv_id': '1907.11692',\n",
       "  'citation': [{'Citation': '[6]',\n",
       "    'Category': 'Theoretical Foundation',\n",
       "    'Explanation': 'The citing paper leverages the RoBERTa model to process frame features extracted by CLIP, providing a theoretical framework for the binary classification approach used in identifying edited videos.'}]}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dict['mapped_citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a class for a paper node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperNode:\n",
    "    title: str\n",
    "    arxiv_id: str\n",
    "    \n",
    "    def __init__(self, title, arxiv_id):\n",
    "        self.title = title\n",
    "        self.arxiv_id = arxiv_id\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Title: {self.title},\\n Arxiv ID: {self.arxiv_id}\"\n",
    "\n",
    "class PaperEdge:\n",
    "    category: str\n",
    "    explanation: str\n",
    "    verbose = True\n",
    "\n",
    "    def __init__(self, category, explanation):\n",
    "        self.category = category\n",
    "        self.explanation = explanation\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.verbose:\n",
    "            return f\"Category: {self.category},\\n Explanation: {self.explanation}\"\n",
    "        else:\n",
    "            return f\"Category: {self.category}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7243/7243 [00:00<00:00, 29032.14it/s]\n"
     ]
    }
   ],
   "source": [
    "paper_dict = {}\n",
    "\n",
    "for article_dict in tqdm(annotated_article, total=len(annotated_article)):\n",
    "    paper_dict[article_dict['title'].lower()] = PaperNode(title=article_dict['title'], arxiv_id=article_dict['arxiv_id'])\n",
    "\n",
    "    if \"mapped_citation\" in article_dict.keys():\n",
    "        for key,val in article_dict['mapped_citation'].items():\n",
    "            title = val['title']\n",
    "            if title not in paper_dict.keys():\n",
    "                paper_node = PaperNode(title=val['title'], arxiv_id=val['arxiv_id'])\n",
    "                paper_dict[title] = paper_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112611"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PaperNode at 0x728c80c2d240>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_dict[\"make-a-video: text-to-video generation without text-video data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Laws and Multi-Valued Fluents\n"
     ]
    }
   ],
   "source": [
    "triplets = []\n",
    "error_count = 0\n",
    "\n",
    "for article_dict in annotated_article:\n",
    "    if \"mapped_citation\" not in article_dict.keys():\n",
    "        print(article_dict['title'])\n",
    "        continue\n",
    "    for key, val in article_dict['mapped_citation'].items():\n",
    "        title = val['title']\n",
    "        citation = val['citation']\n",
    "        \n",
    "        # Use a dictionary to group explanations by category\n",
    "        category_explanations = {}\n",
    "        for rel in citation:\n",
    "            # try:\n",
    "            if 'Category' in rel.keys() and 'Explanation' in rel.keys():\n",
    "                category = rel['Category']\n",
    "                explanation = rel['Explanation']\n",
    "                if category not in category_explanations:\n",
    "                    category_explanations[category] = []\n",
    "                category_explanations[category].append(explanation)\n",
    "            else:\n",
    "                error_count += 1\n",
    "\n",
    "\n",
    "        source_node = paper_dict[title]\n",
    "        target_node = paper_dict[article_dict['title'].lower()]\n",
    "\n",
    "        # Construct triplets with aggregated explanations for each category\n",
    "        if len(category_explanations.items()) > 0:\n",
    "            for category, explanations in category_explanations.items():\n",
    "                if category not in relationships_dict.keys():\n",
    "                    relationships_dict[category] = f\"Is {category} Of\"\n",
    "\n",
    "                aggregated_explanation = \"; \".join(set(explanations))  # Remove duplicates and join explanations\n",
    "                rel = PaperEdge(category=category, explanation=aggregated_explanation)\n",
    "                reverse_rel = PaperEdge(category=relationships_dict[category], explanation=aggregated_explanation)\n",
    "\n",
    "                # Add the relationship in both directions\n",
    "                triplets.append((source_node, rel, target_node))\n",
    "                triplets.append((target_node, reverse_rel, source_node))\n",
    "        else:\n",
    "            rel = PaperEdge(category=\"Unk\", explanation=\"Unk\")\n",
    "            triplets.append((source_node, rel, target_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586957"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Assuming 'triplets' is your list of relationships, \n",
    "# and each PaperNode object in the triplets has an 'arxiv_id' attribute\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for source_node, relationship, target_node in triplets:\n",
    "    # Add nodes if they are not already in the graph\n",
    "    if source_node.arxiv_id not in G:\n",
    "        G.add_node(source_node.title, title=str(source_node), arxiv_id=source_node.arxiv_id)\n",
    "    if target_node.arxiv_id not in G:\n",
    "        G.add_node(target_node.title, title=str(target_node), arxiv_id=target_node.arxiv_id)\n",
    "    \n",
    "    # Add edge with relationship details\n",
    "    G.add_edge(source_node.title, target_node.title, title=str(relationship), category=relationship.category, explanation=relationship.explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112610"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.2 Visualizing citation graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics related to cogvideo: ['T2TD: Text-3D Generation Model based on Prior Knowledge Guidance', 'Prompt-Free Diffusion: Taking \"Text\" out of Text-to-Image Diffusion Models', 'RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths', 'Cones 2: Customizable Image Synthesis with Multiple Subjects', 'JourneyDB: A Benchmark for Generative Image Understanding', 'Planting a SEED of Vision in Large Language Model', 'Enhancing Visually-Rich Document Understanding via Layout Structure Modeling', 'Likelihood-Based Text-to-Image Evaluation with Patch-Level Perceptual and Semantic Credit Assignment', 'DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment', 'AI-Generated Content (AIGC) for Various Data Modalities: A Survey', 'Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis', 'Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation', 'Looking at words and points with attention: a benchmark for text-to-shape coherence', 'OpenBA: An Open-Sourced 15B Bilingual Asymmetric Seq2Seq Model Pre-trained from Scratch', 'TextCLIP: Text-Guided Face Image Generation And Manipulation Without Adversarial Training', 'Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization']\n",
      "{'title': 'Title: make-a-video: text-to-video generation without text-video data,\\n Arxiv ID: 2209.14792', 'arxiv_id': '2209.14792'}\n"
     ]
    }
   ],
   "source": [
    "def find_connected_nodes(graph, node, relationship=None):\n",
    "    \"\"\"\n",
    "    Find nodes connected to the given node with an optional filter on the type of relationship.\n",
    "    \"\"\"\n",
    "    connected_nodes = []\n",
    "    for n, nbrs in graph.adj.items():\n",
    "        if n == node:\n",
    "            for nbr, eattr in nbrs.items():\n",
    "                if relationship is None or eattr['label'] == relationship:\n",
    "                    connected_nodes.append(nbr)\n",
    "    return connected_nodes\n",
    "\n",
    "# Function to search for a node by arxiv_id and return its details\n",
    "def find_nodes_by_arxiv_id(graph, arxiv_id):\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if data.get('arxiv_id') == arxiv_id:\n",
    "            return data  # or return data['paper_node'] to return the PaperNode object itself\n",
    "    return \"Paper not found in the graph.\"\n",
    "\n",
    "\n",
    "def find_shortest_path(graph, source, target):\n",
    "    \"\"\"\n",
    "    Find the shortest path between two nodes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=source, target=target)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "phenaki_related_topics = find_connected_nodes(G, 'cogview: mastering text-to-image generation via transformers')\n",
    "print(\"Topics related to cogvideo:\", phenaki_related_topics)\n",
    "\n",
    "# Example search\n",
    "search_result = find_nodes_by_arxiv_id(G, \"2209.14792\")\n",
    "print(search_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make-a-video: text-to-video generation without text-video data']\n"
     ]
    }
   ],
   "source": [
    "def find_nodes_by_keyword(graph, keyword):\n",
    "    \"\"\"\n",
    "    Find nodes that contain the given keyword in their name and retrieve their connected nodes and relationships.\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower()  # Convert keyword to lowercase for case-insensitive matching\n",
    "    matching_nodes = [node for node in graph.nodes if keyword in node.lower()]\n",
    "\n",
    "    # related_nodes = {}\n",
    "    # for node in matching_nodes:\n",
    "    #     connections = []\n",
    "    #     for neighbor, details in graph[node].items():\n",
    "    #         connections.append((neighbor, details['title'].split('\\n')[0]))\n",
    "    #     related_nodes[node] = connections\n",
    "\n",
    "    return matching_nodes\n",
    "\n",
    "# Example Usage\n",
    "keyword = \"make-a-video\"\n",
    "related_nodes = find_nodes_by_keyword(G, keyword)\n",
    "# for node, connections in phenaki_related.items():\n",
    "#     print(f\"Node: {node}\")\n",
    "#     for conn in connections:\n",
    "#         print(f\"  Connected to: {conn[0]} via {conn[1]}\")\n",
    "print(related_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Assuming G is your original graph\n",
    "# Step 1: Create the subgraph for \"Node1\" and its neighbors\n",
    "subgraph = nx.ego_graph(G, 'make-a-video: text-to-video generation without text-video data', radius=1, center=True, undirected=False)\n",
    "# Nodes to be removed because they have a degree of 1 in the full graph\n",
    "nodes_to_remove = [node for node in subgraph if subgraph.degree(node) < 3]\n",
    "\n",
    "# Remove the nodes from the ego graph\n",
    "subgraph.remove_nodes_from(nodes_to_remove)\n",
    "\n",
    "nt = Network(notebook=True, font_color='#10000000')\n",
    "nt.from_nx(subgraph)\n",
    "# nt.show(\"nx.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_retriever = index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a survey on video diffusion models\n",
      "frame by familiar frame: understanding replication in video diffusion models\n",
      "efficient video diffusion models via content-frame motion-latent decomposition\n",
      "stable video diffusion: scaling latent video diffusion models to large datasets\n",
      "preserve your own correlation: a noise prior for video diffusion models\n",
      "diffusion models: a comprehensive survey of methods and applications\n",
      "diffusion probabilistic modeling for video generation\n",
      "diffusion models for time series applications: a survey\n",
      "video diffusion models\n",
      "medm: mediating image diffusion models for video-to-video translation with temporal correspondence guidance\n"
     ]
    }
   ],
   "source": [
    "results = paper_retriever.retrieve(\"Give me some paper about Video diffusion models\")\n",
    "all_nodes = []\n",
    "for r in results:\n",
    "    title = r.text.split(\"\\n\")[0]\n",
    "    print(title)\n",
    "    nodes = find_nodes_by_keyword(G, title)\n",
    "    if len(nodes) > 0:\n",
    "        all_nodes += nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diffusion models: a comprehensive survey of methods and applications',\n",
       " 'diffusion probabilistic modeling for video generation',\n",
       " 'fleet. video diffusion models',\n",
       " 'latent video diffusion models for high-fidelity video generation with arbitrary lengths',\n",
       " 'video diffusion models',\n",
       " 'dreamix: video diffusion models are general video editors',\n",
       " 'latent video diffusion models for high-fidelity long video generation',\n",
       " 'Video Diffusion Models with Local-Global Context Guidance',\n",
       " 'llm-grounded video diffusion models',\n",
       " 'MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Generate ego graphs\n",
    "ego_graphs = [nx.ego_graph(G, node, radius=1) for node in nodes]\n",
    "\n",
    "# Combine all ego graphs into one graph\n",
    "combined_ego_graph = nx.compose_all(ego_graphs)\n",
    "\n",
    "# Draw the combined ego graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Define positions using a layout\n",
    "pos = nx.spring_layout(combined_ego_graph)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(combined_ego_graph, pos, node_color='orange')\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(combined_ego_graph, pos)\n",
    "\n",
    "# Draw node labels with transparency\n",
    "labels = {node: node for node in combined_ego_graph.nodes()}\n",
    "alpha_value = 0.0  # Set transparency for labels\n",
    "nx.draw_networkx_labels(combined_ego_graph, pos, labels, font_size=12, font_color='black', alpha=alpha_value)\n",
    "\n",
    "# Optionally add a title with transparency\n",
    "plt.title(\"Video diffusion graph\", alpha=0.5)  # Set transparency for title text\n",
    "\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Building Graph Query Engine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph query mechanism is designed to efficiently process user queries by leveraging LLM to retrieve information from a graph database. Here's a detailed breakdown of the process:\n",
    "\n",
    "- Query Parsing: Initially, when a user submits a query, the LLM parses the query to extract key elements, typically in the form of (paper_title, relationship) pairs. \n",
    "\n",
    "- Graph Database Querying: Once the necessary information is extracted, the next step involves querying the graph database. The process begins with locating the node corresponding to the 'paper_title'.\n",
    "\n",
    "- Connection Edge Retrieval: After identifying the relevant paper node, the system then searches for connection edges. It specifically looks for the edge that has the highest similarity score with the extracted 'relationship' text, using a vector search. This step ensures that the most relevant connections based on the user's query are identified.\n",
    "\n",
    "- Result Compilation: The final step involves returning the retrieved information to the LLM. The LLM then uses this data to generate a comprehensive answer tailored to the user's query, providing insights based on the connections and data found in the graph database.\n",
    "\n",
    "For a more visual understanding of this process, refer to the diagram below, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Graph-Mechanism.jpg\" alt=\"graph-search\" width=1500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4.1 Prepare data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relationships_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships_prompt = {\n",
    "    \"Is Evidence For\": \"The paper \\\"{source}\\\" provides supporting evidence for the paper \\\"{target}\\\". \\nExplanation: {explanation}\", \n",
    "    \"Supporting Evidence\": \"The paper \\\"{source}\\\" is supported by the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Is Methodological Basis For\": \"The paper \\\"{source}\\\" is a methodological basis for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Methodological Basis\": \"The paper \\\"{source}\\\" is based on the methodology of the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Is Theoretical Foundation For\": \"The paper \\\"{source}\\\" is a theoretical foundation for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Theoretical Foundation\": \"The paper \\\"{source}\\\" is based on the theoretical foundation of the paper \\\"{target}\\\". \\nExplanation: {explanation}\", \n",
    "    \"Is Data Source For\": \"The paper \\\"{source}\\\" uses the data from the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Data Source\": \"The paper \\\"{source}\\\" provides data for the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Is Extension or Continuation Of\": \"The paper \\\"{source}\\\" is an extension or continuation of the paper \\\"{target}\\\". \\nExplanation: {explanation}\", \n",
    "    \"Extension or Continuation\": \"The paper \\\"{source}\\\" is extended or continued by the paper \\\"{target}\\\". \\nExplanation: {explanation}\",\n",
    "    \"Unk\": \"The relationship bewteen the 2 papers \\\"{source}\\\", \\\"{target}\\\" is unkown {explanation}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in relationships_dict.items():\n",
    "    if key not in relationships_prompt.keys():\n",
    "        relationships_prompt[key] = \"The paper \\\"{source}\\\" provide \" + val + \" for paper \\\"{target}\\\". \\nExplanation: {explanation}\"\n",
    "        relationships_prompt[val] = \"The paper \\\"{source}\\\" is \" + val + \" of paper \\\"{target}\\\". \\nExplanation: {explanation}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"{source}\" provide Is Evaluation Protocol Of for paper \"{target}\". \n",
      "Explanation: {explanation}\n"
     ]
    }
   ],
   "source": [
    "print(relationships_prompt['Evaluation Protocol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586957/586957 [00:24<00:00, 24245.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='c1bef163-b20a-4334-b69f-1a12ab4782b9', embedding=None, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The paper \"active learning with statistical models\" is based on the methodology of the paper \"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings\". \\nExplanation: The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c769946e-a54a-4b53-8368-47f905eeb893', embedding=None, metadata={'title': 'Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings', 'arxiv_id': '2305.14576'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The paper \"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings\" is a methodological basis for the paper \"active learning with statistical models\". \\nExplanation: The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='85031755-296b-4f18-9e1b-3ca4ae45b189', embedding=None, metadata={'title': 'active learning literature survey', 'arxiv_id': None}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The paper \"active learning literature survey\" is based on the methodology of the paper \"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings\". \\nExplanation: The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a50744c1-f20d-4aa8-a236-7180bd690173', embedding=None, metadata={'title': 'Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings', 'arxiv_id': '2305.14576'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The paper \"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings\" is a methodological basis for the paper \"active learning literature survey\". \\nExplanation: The cited work provides a more in-depth discussion of active learning and its potential benefits in reducing labeling costs, which the citing paper further explores in the context of PLMs and low-resource settings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d9b82874-9e8a-4354-8583-8657598429e1', embedding=None, metadata={'title': 'two faces of active learning', 'arxiv_id': None}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='The paper \"two faces of active learning\" is based on the methodology of the paper \"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings\". \\nExplanation: The cited work highlights the importance of label complexity in active learning and the need to reduce it for efficient model training, which the citing paper addresses in its research on efficient finetuning methods for PLMs in low-resource settings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_nodes = []\n",
    "\n",
    "for triplet in tqdm(triplets, total=len(triplets)):\n",
    "    text_prompt = relationships_prompt[triplet[1].category].format(source=triplet[0].title, target=triplet[2].title, explanation=triplet[1].explanation)\n",
    "    triplet_nodes.append(Document(text=text_prompt, metadata={\"title\": triplet[0].title, \"arxiv_id\": triplet[0].arxiv_id}))\n",
    "    \n",
    "triplet_nodes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 586957/586957 [03:15<00:00, 3007.24it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 422.37it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.39it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.70it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 414.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 415.19it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.30it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 448.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.70it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.56it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 436.26it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 423.58it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 409.63it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 417.97it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.50it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 414.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.50it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 408.39it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.21it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.78it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.38it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 401.72it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 406.26it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.51it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 412.39it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.88it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.80it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 413.77it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 416.28it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 403.18it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 398.11it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.38it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 393.33it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 400.65it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.57it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 437.81it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 434.70it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.44it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.41it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 417.28it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.23it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 413.02it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 393.21it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.22it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.36it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 439.45it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.34it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.97it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 415.06it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 406.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 396.83it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 444.93it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 450.04it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.99it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.74it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 400.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 396.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 446.78it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.66it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.99it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.73it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.75it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 437.34it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.86it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 439.49it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 451.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 455.62it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 412.36it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.51it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.17it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 423.86it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 450.15it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 416.33it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.55it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.37it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.78it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.00it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.66it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 445.88it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.74it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.52it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 417.21it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.27it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.89it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.64it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.26it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 423.06it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 444.27it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 434.00it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.15it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.74it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 413.22it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 401.65it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.05it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.63it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 415.47it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 412.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.63it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.39it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 446.71it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 410.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.88it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.94it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.90it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 453.41it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 404.64it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.88it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 417.79it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 452.90it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 453.36it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.34it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.28it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.99it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 409.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.72it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 412.15it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 383.85it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.41it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 453.94it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.56it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 397.99it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.38it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 401.00it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 439.54it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.93it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.67it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 403.16it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.68it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.94it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.65it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 446.67it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 451.45it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 423.58it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.58it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.70it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.82it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 464.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.22it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 451.07it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 438.64it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 438.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 444.90it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 446.82it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 437.97it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 434.61it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 442.59it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 442.85it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 454.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.68it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.02it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 443.45it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 431.54it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.85it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 438.69it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.83it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.57it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.33it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 436.93it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.59it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 458.38it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 456.50it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 439.28it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 405.67it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 415.83it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.96it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.52it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.92it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.52it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.18it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.82it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.00it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.43it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.74it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 422.80it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.84it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.39it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 442.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.94it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 455.62it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 412.78it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 436.07it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.06it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.25it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 416.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.24it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.07it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.77it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 442.60it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.58it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.64it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.74it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.32it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 408.03it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 406.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 414.05it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 437.57it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.83it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.10it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 416.82it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.49it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.74it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.84it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.81it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 430.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.88it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 459.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.18it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 436.62it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.76it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.41it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 394.15it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 416.78it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.96it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 406.95it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 407.71it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 408.69it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.50it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.86it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 421.32it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 398.51it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 403.26it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 437.08it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.83it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 435.68it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 439.14it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.79it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 399.89it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 425.63it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.24it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 419.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.67it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 415.76it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 438.41it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.05it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:05<00:00, 401.24it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 427.55it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 442.33it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.14it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 424.79it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.65it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 432.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 447.15it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 422.01it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 436.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 411.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 423.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.53it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.36it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 438.90it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.96it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.68it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 443.24it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 459.00it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 418.91it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 454.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 428.29it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 420.99it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.56it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 416.70it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 446.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 441.64it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 453.48it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 443.58it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 429.94it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 434.69it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 440.50it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 448.78it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 426.98it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 448.06it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 433.12it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:04<00:00, 436.60it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1453/1453 [00:03<00:00, 454.50it/s]\n"
     ]
    }
   ],
   "source": [
    "## Extract embedding for the truplets relationships\n",
    "Settings.llm = None\n",
    "# Create embed model\n",
    "device_type = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/graph\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_graph\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    triplet_nodes, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the relationship from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/graph\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_graph\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "rel_index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "\n",
    "filters = MetadataFilters(filters=[\n",
    "    ExactMatchFilter(\n",
    "        key=\"title\", \n",
    "        value=\"active learning with statistical models\"\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rel_query_engine = rel_index.as_retriever(\n",
    "    similarity_top_k=10,\n",
    "    filters=filters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_res = graph_rel_query_engine.retrieve(\"is based on the methodology of the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='7a3970d0-9357-413c-8d21-3bf250dc4718', embedding=None, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7211851d-f489-440e-b679-4911344123d1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, hash='9149b3df731eac7265caae32f013a8a5b025af15b26ff6d1c28df5de7c71ead2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e3db80b2-937b-45a2-9623-762f58492bd9', node_type=<ObjectType.TEXT: '1'>, metadata={'title': 'REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS', 'arxiv_id': '2306.09205'}, hash='854b7a0b5649fef888bfda873e0c560208c755dec429339477b0e980d950f54b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='81ab7ee7-549b-43b6-aa9f-ba226df4fc04', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5983a9bf38b78872566a1b46a7d26b69ceed09df93413880eee8c98a7f6dbbf1')}, text='The paper \"active learning with statistical models\" is based on the methodology of the paper \"REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS\". \\nExplanation: The cited work on active learning provides the foundational basis for the development of intrinsic motivation in sequential decision-making in the citing paper.', start_char_idx=0, end_char_idx=325, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.48846321944846377),\n",
       " NodeWithScore(node=TextNode(id_='854da064-2997-4a01-8697-3e95afe064e1', embedding=None, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c1bef163-b20a-4334-b69f-1a12ab4782b9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, hash='a99586ea3008e98a89617a6241fcd455080d5c8c9831f5c9336d56f225e59624'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='efe9d188-4548-4874-baad-e5a1441f6c8e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='110b7397e65f4e57c3ac3950265621d00e1c03e46fd53480d1916f7b2c29a2d9')}, text='The paper \"active learning with statistical models\" is based on the methodology of the paper \"Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings\". \\nExplanation: The cited work introduces the concept of active learning as a potential solution to the challenge of data labeling in low-resource settings, which the citing paper builds upon in its research on efficient finetuning methods for PLMs.', start_char_idx=0, end_char_idx=431, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.4829036270278226),\n",
       " NodeWithScore(node=TextNode(id_='c02b2693-cf33-47bc-bdfe-0f45aa7c6157', embedding=None, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e4c281bc-a650-4762-9691-a036dc732491', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'active learning with statistical models', 'arxiv_id': 'cs/9603104'}, hash='9e6af43060158629e371596edf2dada96e2ba357964d764e0c16d5827d7db770'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b5bbe59b-bbfe-4eba-a1a0-0ee6d0cbf35d', node_type=<ObjectType.TEXT: '1'>, metadata={'title': 'Is GPT-4 a Good Data Analyst?', 'arxiv_id': '2305.15038'}, hash='d2caa4a0f87295fba94aa971ffe54b5b4b47480de00f3757ffd7d6371073f1ce'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6f805bcc-4f89-4fde-98b0-85d5c82206cf', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='2251d065cfc3b2c4c04066e724b5999ab35a7d311706ca9750d00f7cd1ef9174')}, text='The paper \"active learning with statistical models\" is supported by the paper \"Active Learning for Natural Language Generation\". \\nExplanation: The cited work introduces the concept of active learning as a machine learning approach to reduce annotation effort, which the citing paper builds upon to train models with less data.', start_char_idx=0, end_char_idx=326, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.47649322465142385)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4.2 Building the search logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses_json import DataClassJsonMixin\n",
    "from collections import defaultdict\n",
    "\n",
    "import fsspec\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    DEFAULT_PERSIST_DIR,\n",
    "    DEFAULT_PERSIST_FNAME,\n",
    "    GraphStore,\n",
    ")\n",
    "import ast\n",
    "import fsspec\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "DEFAULT_PERSIST_DIR = \"../DB/citation_graph\"\n",
    "DEFAULT_PERSIST_FNAME = \"graph_store.json\"\n",
    "\n",
    "\n",
    "class CitationGraphStoreData(DataClassJsonMixin):\n",
    "\n",
    "    \"\"\"Simple Graph Store Data container.\n",
    "\n",
    "    Args:\n",
    "        graph_dict (Optional[dict]): dict mapping subject to\n",
    "    \"\"\"\n",
    "\n",
    "    graph_dict: Dict[str, List[List[str]]] = defaultdict(list)\n",
    "    graph_index: VectorStoreIndex = rel_index\n",
    "\n",
    "\n",
    "    def find_nodes_by_keyword(self, keyword):\n",
    "        \"\"\"\n",
    "        Find all nodes that contain the given keyword in their name.\n",
    "        \"\"\"\n",
    "        keyword = keyword.lower()  # Convert keyword to lowercase for case-insensitive matching\n",
    "        return [node for node in self.graph_dict.keys() if keyword in node.title.lower()]\n",
    "\n",
    "\n",
    "    def get_rel_map(\n",
    "        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n",
    "    ) -> Dict[str, List[List[str]]]:\n",
    "        \"\"\"Get subjects' rel map in max depth.\"\"\"\n",
    "        if subjs is None:\n",
    "            subjs = list(self.graph_dict.keys())\n",
    "        rel_map = {}\n",
    "        for subj in subjs:\n",
    "            rel_map[subj] = self._get_rel_map(subj.title, depth=depth, limit=limit)\n",
    "        # TBD, truncate the rel_map in a spread way, now just truncate based\n",
    "        # on iteration order\n",
    "        rel_count = 0\n",
    "        return_map = {}\n",
    "        for subj in rel_map:\n",
    "            if rel_count + len(rel_map[subj]) > limit:\n",
    "                return_map[subj] = rel_map[subj][: limit - rel_count]\n",
    "                break\n",
    "            else:\n",
    "                return_map[subj] = rel_map[subj]\n",
    "                rel_count += len(rel_map[subj])\n",
    "        return return_map\n",
    "\n",
    "    def _get_rel_map(\n",
    "        self, keyword: str, depth: int = 2, limit: int = 30\n",
    "    ) -> List[List[str]]:\n",
    "        \"\"\"Get one subect's rel map in max depth.\"\"\"\n",
    "        if depth == 0:\n",
    "            return []\n",
    "        rel_map = []\n",
    "        rel_count = 0\n",
    "        subjs = self.find_nodes_by_keyword(keyword)\n",
    "\n",
    "        if len(subjs) > 0:\n",
    "            subj = subjs[0]\n",
    "            for rel, obj in self.graph_dict[subj]:\n",
    "                if rel_count >= limit:\n",
    "                    break\n",
    "                rel_map.append([subj, rel, obj])\n",
    "                rel_map += self._get_rel_map(obj, depth=depth - 1)\n",
    "                rel_count += 1\n",
    "        return rel_map\n",
    "    \n",
    "    def search_vector(self, queries):\n",
    "        # Example string that represents a tuple\n",
    "        final_res = []\n",
    "        for query_tuple in queries:\n",
    "            \n",
    "            # Converting string to tuple\n",
    "            # result_tuple = ast.literal_eval(query_str)\n",
    "            qr_title = query_tuple[0]\n",
    "            qr_rel = query_tuple[1]\n",
    "            node = self.find_nodes_by_keyword(qr_title)[0]\n",
    "\n",
    "            filters = MetadataFilters(filters=[\n",
    "                ExactMatchFilter(\n",
    "                    key=\"title\", \n",
    "                    value=node.title\n",
    "                )\n",
    "            ])\n",
    "            retriever = self.graph_index.as_retriever(\n",
    "                similarity_top_k=10,\n",
    "                filters=filters\n",
    "            )\n",
    "            res = retriever.retrieve(qr_rel)\n",
    "            final_res += [r.text for r in res] \n",
    "        \n",
    "        return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationGraphStore(GraphStore):\n",
    "    \"\"\"Simple Graph Store.\n",
    "\n",
    "    In this graph store, triplets are stored within a simple, in-memory dictionary.\n",
    "\n",
    "    Args:\n",
    "        simple_graph_store_data_dict (Optional[dict]): data dict\n",
    "            containing the triplets. See SimpleGraphStoreData\n",
    "            for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Optional[CitationGraphStoreData] = None,\n",
    "        fs: Optional[fsspec.AbstractFileSystem] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        self._data = data or CitationGraphStoreData()\n",
    "        self._fs = fs or fsspec.filesystem(\"file\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_persist_dir(\n",
    "        cls,\n",
    "        persist_dir: str = DEFAULT_PERSIST_DIR,\n",
    "        fs: Optional[fsspec.AbstractFileSystem] = None,\n",
    "    ) -> \"CitationGraphStore\":\n",
    "        \"\"\"Load from persist dir.\"\"\"\n",
    "        persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)\n",
    "        return cls.from_persist_path(persist_path, fs=fs)\n",
    "\n",
    "    @property\n",
    "    def client(self) -> None:\n",
    "        \"\"\"Get client.\n",
    "        Not applicable for this store.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    def get(self, subj: str) -> List[List[str]]:\n",
    "        \"\"\"Get triplets.\"\"\"\n",
    "        return self._data.graph_dict.get(subj, [])\n",
    "\n",
    "    def get_rel_map(\n",
    "        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n",
    "    ) -> Dict[str, List[List[str]]]:\n",
    "        \"\"\"Get depth-aware rel map.\"\"\"\n",
    "        return self._data.get_rel_map(subjs=subjs, depth=depth, limit=limit)\n",
    "\n",
    "    def upsert_triplet(self, subj: str, rel: str, obj: str) -> None:\n",
    "        \"\"\"Add triplet.\"\"\"\n",
    "        if subj not in self._data.graph_dict:\n",
    "            self._data.graph_dict[subj] = []\n",
    "        if (rel, obj) not in self._data.graph_dict[subj]:\n",
    "            self._data.graph_dict[subj].append([rel, obj])\n",
    "\n",
    "    def delete(self, subj: str, rel: str, obj: str) -> None:\n",
    "        \"\"\"Delete triplet.\"\"\"\n",
    "        if subj in self._data.graph_dict:\n",
    "            if (rel, obj) in self._data.graph_dict[subj]:\n",
    "                self._data.graph_dict[subj].remove([rel, obj])\n",
    "                if len(self._data.graph_dict[subj]) == 0:\n",
    "                    del self._data.graph_dict[subj]\n",
    "\n",
    "    def persist(\n",
    "        self,\n",
    "        persist_path: str = os.path.join(DEFAULT_PERSIST_DIR, DEFAULT_PERSIST_FNAME),\n",
    "        fs: Optional[fsspec.AbstractFileSystem] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Persist the SimpleGraphStore to a directory.\"\"\"\n",
    "        fs = fs or self._fs\n",
    "        dirpath = os.path.dirname(persist_path)\n",
    "        if not fs.exists(dirpath):\n",
    "            fs.makedirs(dirpath)\n",
    "\n",
    "        with fs.open(persist_path, \"w\") as f:\n",
    "            json.dump(self._data.to_dict(), f)\n",
    "\n",
    "    def get_schema(self, refresh: bool = False) -> str:\n",
    "        \"\"\"Get schema.\"\"\"\n",
    "        return \"CitationGraphStore\"\n",
    "\n",
    "    def query(self, query: str, param_map: Optional[Dict[str, Any]] = {}) -> Any:\n",
    "        response = []\n",
    "\n",
    "        pairs = ast.literal_eval(query)\n",
    "\n",
    "        # relmap = self._data.get_rel_map(subjs=pairs, depth=param_map.get(\"depth\") or 1, limit=param_map.get(\"limit\") or 30)\n",
    "        res = self._data.search_vector(pairs)\n",
    "\n",
    "        response.append(res)\n",
    "        return response\n",
    "    \n",
    "    @classmethod\n",
    "    def from_persist_path(\n",
    "        cls, persist_path: str, fs: Optional[fsspec.AbstractFileSystem] = None\n",
    "    ) -> \"CitationGraphStore\":\n",
    "        \"\"\"Create a SimpleGraphStore from a persist directory.\"\"\"\n",
    "        fs = fs or fsspec.filesystem(\"file\")\n",
    "        if not fs.exists(persist_path):\n",
    "            logger.warning(\n",
    "                f\"No existing {__name__} found at {persist_path}. \"\n",
    "                \"Initializing a new graph_store from scratch. \"\n",
    "            )\n",
    "            return cls()\n",
    "\n",
    "        logger.debug(f\"Loading {__name__} from {persist_path}.\")\n",
    "        with fs.open(persist_path, \"rb\") as f:\n",
    "            data_dict = json.load(f)\n",
    "            data = CitationGraphStoreData.from_dict(data_dict)\n",
    "        return cls(data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, save_dict: dict) -> \"CitationGraphStore\":\n",
    "        data = CitationGraphStoreData.from_dict(save_dict)\n",
    "        return cls(data)\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return self._data.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_graph_store = CitationGraphStore()\n",
    "for triplet in triplets:\n",
    "    citation_graph_store.upsert_triplet(triplet[0], triplet[1], triplet[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_res = citation_graph_store.query(\"(('make-a-video', 'methodological basis'), ('make-a-video', 'data sources'))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\". \\nExplanation: The cited work, Make-A-Video, serves as a methodological basis for the citing paper by providing a direct approach for text-to-video generation without the need for paired text-video data.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"*Ongoing work DRAGNUWA: FINE-GRAINED CONTROL IN VIDEO GENERATION BY INTEGRATING TEXT, IMAGE, AND TRAJECTORY\". \\nExplanation: The cited work by Singer et al. (2022) provides a method for text-to-video generation that the citing paper adopts to control the generation of videos based on text conditions.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance\". \\nExplanation: The cited works provide the basis for the text-to-video generation process, which the citing paper builds upon to simplify the process of creating videos with text prompts.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"GENERATING IMAGES WITH 3D ANNOTATIONS USING DIFFUSION MODELS\". \\nExplanation: The cited work by Singer et al. (2023) provides a detailed discussion on the use of diffusion models in generating high-resolution images, which the citing paper utilizes in their research on generating diverse images using appearance produced by diffusion models.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Probabilistic Adaptation of Text-to-Video Models\". \\nExplanation: The cited works provide the foundational text-to-video models that the citing paper builds upon in its research on generating highly realistic videos from text descriptions.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"AI-Generated Content (AIGC) for Various Data Modalities: A Survey\". \\nExplanation: The cited works on generation of molecules conditioned on SMILE structures serve as a data source for the citing paper, providing information on the generation of molecules in a specific context.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement\". \\nExplanation: The cited work, Make-a-video, is used as a data source for the training of the diffusion model in the citing paper. The data is used to pretrain the model and to provide a basis for the model chaining and image pretraining techniques employed in the study.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"MVDREAM: MULTI-VIEW DIFFUSION FOR 3D GENERATION\". \\nExplanation: The cited work is the source of the data used in the experiment to compare the different methods of embedding camera parameters in the video diffusion model.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"ControlVideo: Conditional Control for One-shot Text-driven Video Editing and Beyond\". \\nExplanation: The cited work provides a method of training temporal attention modules using extensive data, which the citing paper adopts to address the challenge of one-shot training in a similar manner.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is supported by the paper \"HeadSculpt: Crafting 3D Head Avatars with Text\". \\nExplanation: The cited works on text-to-2D generation for video sequence are mentioned as having made significant progress in the field, providing supporting evidence for the citing paper.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance\". \\nExplanation: The Make-A-Video is a data source for the comparison in the citing paper.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"AI-Generated Content (AIGC) for Various Data Modalities: A Survey\". \\nExplanation: The cited works on generation of molecules conditioned on SMILE structures serve as a data source for the citing paper, providing information on the generation of molecules in a specific context.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"TOKENFLOW: CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING\". \\nExplanation: The cited work provides a large video dataset for training text-to-video generation models, which the citing paper utilizes in their research to improve the quality of the models.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation\". \\nExplanation: The cited works on text-to-video in other modalities are data sources for the development of text-to-image generation in the citing paper.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing\". \\nExplanation: The cited work, Make-A-Video, is a data source for the citing paper, as it is a text-to-video generation approach that the citing paper utilizes in its research to generate video frames that correspond to input text.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance\". \\nExplanation: The cited works provide the basis for the text-to-video generation process, which the citing paper builds upon to simplify the process of creating videos with text prompts.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is supported by the paper \"HeadSculpt: Crafting 3D Head Avatars with Text\". \\nExplanation: The cited works on text-to-2D generation for video sequence are mentioned as having made significant progress in the field, providing supporting evidence for the citing paper.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"*Ongoing work DRAGNUWA: FINE-GRAINED CONTROL IN VIDEO GENERATION BY INTEGRATING TEXT, IMAGE, AND TRAJECTORY\". \\nExplanation: The cited work by Singer et al. (2022) provides a method for text-to-video generation that the citing paper adopts to control the generation of videos based on text conditions.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement\". \\nExplanation: The cited work, Make-a-video, is used as a data source for the training of the diffusion model in the citing paper. The data is used to pretrain the model and to provide a basis for the model chaining and image pretraining techniques employed in the study.',\n",
       "  'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Probabilistic Adaptation of Text-to-Video Models\". \\nExplanation: The cited works provide the foundational text-to-video models that the citing paper builds upon in its research on generating highly realistic videos from text descriptions.']]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=citation_graph_store)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=\"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\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "\n",
    "\n",
    "QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (\n",
    "    \"A question is provided below. Given the question, extract all the (paper, relationship) pars\"\n",
    "    \"from the text. Focus on extracting the keywords that we can use \"\n",
    "    \"to best lookup answers to the question. Avoid stopwords.\\n\"\n",
    "    \"Example: (vq-vae, continuation), (video diffusion models, data source)\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Provide pairs in the following comma-separated format: \"\n",
    "    \"Example 1: (('paper name', 'relationship'), ('paper name 2', 'relationship2')) \\n\"\n",
    "    \"Example 2: (('paper name', 'relationship'))\\n\"\n",
    ")\n",
    "\n",
    "QUERY_KEYWORD_EXTRACT_TEMPLATE = PromptTemplate(\n",
    "    QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,\n",
    "    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,\n",
    ")\n",
    "\n",
    "query_engine = KnowledgeGraphQueryEngine(storage_context=storage_context,\n",
    "                                          graph_query_synthesis_prompt=QUERY_KEYWORD_EXTRACT_TEMPLATE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33mGraph Store Query:\n",
      "(('make-a-video', 'methodological basis'), ('make-a-video', 'data sources'))\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33mGraph Store Response:\n",
      "[['The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models\". \\nExplanation: The cited work, Make-A-Video, serves as a methodological basis for the citing paper by providing a direct approach for text-to-video generation without the need for paired text-video data.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"*Ongoing work DRAGNUWA: FINE-GRAINED CONTROL IN VIDEO GENERATION BY INTEGRATING TEXT, IMAGE, AND TRAJECTORY\". \\nExplanation: The cited work by Singer et al. (2022) provides a method for text-to-video generation that the citing paper adopts to control the generation of videos based on text conditions.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance\". \\nExplanation: The cited works provide the basis for the text-to-video generation process, which the citing paper builds upon to simplify the process of creating videos with text prompts.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"GENERATING IMAGES WITH 3D ANNOTATIONS USING DIFFUSION MODELS\". \\nExplanation: The cited work by Singer et al. (2023) provides a detailed discussion on the use of diffusion models in generating high-resolution images, which the citing paper utilizes in their research on generating diverse images using appearance produced by diffusion models.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Probabilistic Adaptation of Text-to-Video Models\". \\nExplanation: The cited works provide the foundational text-to-video models that the citing paper builds upon in its research on generating highly realistic videos from text descriptions.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"AI-Generated Content (AIGC) for Various Data Modalities: A Survey\". \\nExplanation: The cited works on generation of molecules conditioned on SMILE structures serve as a data source for the citing paper, providing information on the generation of molecules in a specific context.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement\". \\nExplanation: The cited work, Make-a-video, is used as a data source for the training of the diffusion model in the citing paper. The data is used to pretrain the model and to provide a basis for the model chaining and image pretraining techniques employed in the study.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"MVDREAM: MULTI-VIEW DIFFUSION FOR 3D GENERATION\". \\nExplanation: The cited work is the source of the data used in the experiment to compare the different methods of embedding camera parameters in the video diffusion model.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"ControlVideo: Conditional Control for One-shot Text-driven Video Editing and Beyond\". \\nExplanation: The cited work provides a method of training temporal attention modules using extensive data, which the citing paper adopts to address the challenge of one-shot training in a similar manner.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is supported by the paper \"HeadSculpt: Crafting 3D Head Avatars with Text\". \\nExplanation: The cited works on text-to-2D generation for video sequence are mentioned as having made significant progress in the field, providing supporting evidence for the citing paper.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance\". \\nExplanation: The Make-A-Video is a data source for the comparison in the citing paper.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"AI-Generated Content (AIGC) for Various Data Modalities: A Survey\". \\nExplanation: The cited works on generation of molecules conditioned on SMILE structures serve as a data source for the citing paper, providing information on the generation of molecules in a specific context.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"TOKENFLOW: CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING\". \\nExplanation: The cited work provides a large video dataset for training text-to-video generation models, which the citing paper utilizes in their research to improve the quality of the models.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation\". \\nExplanation: The cited works on text-to-video in other modalities are data sources for the development of text-to-image generation in the citing paper.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing\". \\nExplanation: The cited work, Make-A-Video, is a data source for the citing paper, as it is a text-to-video generation approach that the citing paper utilizes in its research to generate video frames that correspond to input text.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance\". \\nExplanation: The cited works provide the basis for the text-to-video generation process, which the citing paper builds upon to simplify the process of creating videos with text prompts.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is supported by the paper \"HeadSculpt: Crafting 3D Head Avatars with Text\". \\nExplanation: The cited works on text-to-2D generation for video sequence are mentioned as having made significant progress in the field, providing supporting evidence for the citing paper.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"*Ongoing work DRAGNUWA: FINE-GRAINED CONTROL IN VIDEO GENERATION BY INTEGRATING TEXT, IMAGE, AND TRAJECTORY\". \\nExplanation: The cited work by Singer et al. (2022) provides a method for text-to-video generation that the citing paper adopts to control the generation of videos based on text conditions.', 'The paper \"make-a-video: text-to-video generation without text-video data\" provides data for the paper \"TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement\". \\nExplanation: The cited work, Make-a-video, is used as a data source for the training of the diffusion model in the citing paper. The data is used to pretrain the model and to provide a basis for the model chaining and image pretraining techniques employed in the study.', 'The paper \"make-a-video: text-to-video generation without text-video data\" is based on the methodology of the paper \"Probabilistic Adaptation of Text-to-Video Models\". \\nExplanation: The cited works provide the foundational text-to-video models that the citing paper builds upon in its research on generating highly realistic videos from text descriptions.']]\n",
      "\u001b[0m\u001b[1;3;32mFinal Response: The methodological basis for the paper 'make-a-video' includes utilizing approaches from various cited works such as \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models,\" \"*Ongoing work DRAGNUWA: FINE-GRAINED CONTROL IN VIDEO GENERATION BY INTEGRATING TEXT, IMAGE, AND TRAJECTORY,\" \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance,\" \"GENERATING IMAGES WITH 3D ANNOTATIONS USING DIFFUSION MODELS,\" and \"Probabilistic Adaptation of Text-to-Video Models.\" Additionally, the paper draws data from sources like \"AI-Generated Content (AIGC) for Various Data Modalities: A Survey,\" \"TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement,\" \"MVDREAM: MULTI-VIEW DIFFUSION FOR 3D GENERATION,\" \"TOKENFLOW: CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING,\" \"Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation,\" and \"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing.\"\n",
      "\u001b[0mThe methodological basis for the paper 'make-a-video' includes utilizing approaches from various cited works such as \"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models,\" \"*Ongoing work DRAGNUWA: FINE-GRAINED CONTROL IN VIDEO GENERATION BY INTEGRATING TEXT, IMAGE, AND TRAJECTORY,\" \"Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance,\" \"GENERATING IMAGES WITH 3D ANNOTATIONS USING DIFFUSION MODELS,\" and \"Probabilistic Adaptation of Text-to-Video Models.\" Additionally, the paper draws data from sources like \"AI-Generated Content (AIGC) for Various Data Modalities: A Survey,\" \"TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement,\" \"MVDREAM: MULTI-VIEW DIFFUSION FOR 3D GENERATION,\" \"TOKENFLOW: CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING,\" \"Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation,\" and \"CoDeF: Content Deformation Fields for Temporally Consistent Video Processing.\"\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"what are the methodological basis and data sources used in the paper 'make-a-video'?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Basic Data Science Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/Data-Science-Search.jpg\" alt=\"ds-search\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Download Wikipedia Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data science questions, I will use the source from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Pre-compile the regular expression pattern for better performance\n",
    "BRACES_PATTERN = re.compile(r'\\{.*?\\}|\\}')\n",
    "\n",
    "def remove_braces_and_content(text):\n",
    "    \"\"\"Remove all occurrences of curly braces and their content from the given text\"\"\"\n",
    "    return BRACES_PATTERN.sub('', text)\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"Clean the input string.\"\"\"\n",
    "    \n",
    "    # Remove extra spaces by splitting the string by spaces and joining back together\n",
    "    cleaned_string = ' '.join(input_string.split())\n",
    "    \n",
    "    # Remove consecutive carriage return characters until there are no more consecutive occurrences\n",
    "    cleaned_string = re.sub(r'\\r+', '\\r', cleaned_string)\n",
    "    \n",
    "    # Remove all occurrences of curly braces and their content from the cleaned string\n",
    "    cleaned_string = remove_braces_and_content(cleaned_string)\n",
    "    \n",
    "    # Return the cleaned string\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikipedia_pages(wiki_wiki, category_name):\n",
    "    \"\"\"Extract all references from a category on Wikipedia\"\"\"\n",
    "    \n",
    "    # Get the Wikipedia page corresponding to the provided category name\n",
    "    category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "    \n",
    "    # Initialize an empty list to store page titles\n",
    "    pages = []\n",
    "    \n",
    "    # Check if the category exists\n",
    "    if category.exists():\n",
    "        # Iterate through each article in the category and append its title to the list\n",
    "        for article in category.categorymembers.values():\n",
    "            pages.append(article.title)\n",
    "    \n",
    "    # Return the list of page titles\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_wikipedia_pages(categories):\n",
    "    \"\"\"Retrieve Wikipedia pages from a list of categories and extract their content\"\"\"\n",
    "    \n",
    "    # Create a Wikipedia object\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('Kaggle Data Science Assistant with Gemma', 'en')\n",
    "    \n",
    "    # Initialize lists to store explored categories and Wikipedia pages\n",
    "    explored_categories = []\n",
    "    wikipedia_pages = []\n",
    "\n",
    "    # Iterate through each category\n",
    "    print(\"- Processing Wikipedia categories:\")\n",
    "    for category_name in categories:\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Get the Wikipedia page corresponding to the category\n",
    "        category = wiki_wiki.page(\"Category:\" + category_name)\n",
    "        \n",
    "        # Extract Wikipedia pages from the category and extend the list\n",
    "        wikipedia_pages.extend(extract_wikipedia_pages(wiki_wiki, category_name))\n",
    "        \n",
    "        # Add the explored category to the list\n",
    "        explored_categories.append(category_name)\n",
    "\n",
    "    # Extract subcategories and remove duplicate categories\n",
    "    categories_to_explore = [item.replace(\"Category:\", \"\") for item in wikipedia_pages if \"Category:\" in item]\n",
    "    wikipedia_pages = list(set([item for item in wikipedia_pages if \"Category:\" not in item]))\n",
    "    \n",
    "    # Explore subcategories recursively\n",
    "    while categories_to_explore:\n",
    "        category_name = categories_to_explore.pop()\n",
    "        print(f\"\\tExploring {category_name} on Wikipedia\")\n",
    "        \n",
    "        # Extract more references from the subcategory\n",
    "        more_refs = extract_wikipedia_pages(wiki_wiki, category_name)\n",
    "\n",
    "        # Iterate through the references\n",
    "        for ref in more_refs:\n",
    "            # Check if the reference is a category\n",
    "            if \"Category:\" in ref:\n",
    "                new_category = ref.replace(\"Category:\", \"\")\n",
    "                # Add the new category to the explored categories list\n",
    "                if new_category not in explored_categories:\n",
    "                    explored_categories.append(new_category)\n",
    "            else:\n",
    "                # Add the reference to the Wikipedia pages list\n",
    "                if ref not in wikipedia_pages:\n",
    "                    wikipedia_pages.append(ref)\n",
    "\n",
    "    # Initialize a list to store extracted texts\n",
    "    extracted_texts = []\n",
    "    \n",
    "    # Iterate through each Wikipedia page\n",
    "    print(\"- Processing Wikipedia pages:\")\n",
    "    for page_title in tqdm(wikipedia_pages, total=len(wikipedia_pages)):\n",
    "        # Get the Wikipedia page\n",
    "        page = wiki_wiki.page(page_title)\n",
    "\n",
    "        # Append the page title and summary to the extracted texts list\n",
    "        if len(page.summary) > len(page.title):\n",
    "            extracted_texts.append(page.title + \" : \" + clean_string(page.summary))\n",
    "        \n",
    "        # Iterate through the sections in the page\n",
    "        for section in page.sections:\n",
    "            # Append the page title and section text to the extracted texts list\n",
    "            if len(section.text) > len(page.title):\n",
    "                extracted_texts.append(page.title + \" : \" + clean_string(section.text))\n",
    "                \n",
    "    # Return the extracted texts\n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Processing Wikipedia categories:\n",
      "\tExploring Machine_learning on Wikipedia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExploring Data_science on Wikipedia\n",
      "\tExploring Statistics on Wikipedia\n",
      "\tExploring Deep_learning on Wikipedia\n",
      "\tExploring Artificial_intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence stubs on Wikipedia\n",
      "\tExploring Works created using artificial intelligence on Wikipedia\n",
      "\tExploring Virtual assistants on Wikipedia\n",
      "\tExploring Turing tests on Wikipedia\n",
      "\tExploring AI software on Wikipedia\n",
      "\tExploring Rule engines on Wikipedia\n",
      "\tExploring Artificial intelligence publications on Wikipedia\n",
      "\tExploring Philosophy of artificial intelligence on Wikipedia\n",
      "\tExploring Artificial intelligence people on Wikipedia\n",
      "\tExploring Open-source artificial intelligence on Wikipedia\n",
      "\tExploring Non-fiction books about Artificial intelligence on Wikipedia\n",
      "\tExploring Neural networks on Wikipedia\n",
      "\tExploring Multi-agent systems on Wikipedia\n",
      "\tExploring Mindâ€“body problem on Wikipedia\n",
      "\tExploring Machine learning on Wikipedia\n",
      "\tExploring Artificial intelligence laboratories on Wikipedia\n",
      "\tExploring Knowledge representation on Wikipedia\n",
      "\tExploring History of artificial intelligence on Wikipedia\n",
      "\tExploring Generative artificial intelligence on Wikipedia\n",
      "\tExploring Game artificial intelligence on Wikipedia\n",
      "\tExploring Fuzzy logic on Wikipedia\n",
      "\tExploring Fiction about artificial intelligence on Wikipedia\n",
      "\tExploring Existential risk from artificial general intelligence on Wikipedia\n",
      "\tExploring Evolutionary computation on Wikipedia\n",
      "\tExploring Artificial intelligence entertainment on Wikipedia\n",
      "\tExploring Distributed artificial intelligence on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computer vision on Wikipedia\n",
      "\tExploring Artificial intelligence competitions on Wikipedia\n",
      "\tExploring AI companies on Wikipedia\n",
      "\tExploring Cognitive architecture on Wikipedia\n",
      "\tExploring Cloud robotics on Wikipedia\n",
      "\tExploring Chatbots on Wikipedia\n",
      "\tExploring Automated reasoning on Wikipedia\n",
      "\tExploring Artificial intelligence associations on Wikipedia\n",
      "\tExploring Artificial intelligence templates on Wikipedia\n",
      "\tExploring Artificial immune systems on Wikipedia\n",
      "\tExploring Artificial intelligence art on Wikipedia\n",
      "\tExploring Argument technology on Wikipedia\n",
      "\tExploring Applications of artificial intelligence on Wikipedia\n",
      "\tExploring Ambient intelligence on Wikipedia\n",
      "\tExploring AI accelerators on Wikipedia\n",
      "\tExploring Affective computing on Wikipedia\n",
      "\tExploring Text-to-image generation on Wikipedia\n",
      "\tExploring Google DeepMind on Wikipedia\n",
      "\tExploring Deepfakes on Wikipedia\n",
      "\tExploring Deep learning software on Wikipedia\n",
      "\tExploring Statistics stubs on Wikipedia\n",
      "\tExploring Statistical concepts on Wikipedia\n",
      "\tExploring Statistical software on Wikipedia\n",
      "\tExploring Statistical methods on Wikipedia\n",
      "\tExploring Statistical data on Wikipedia\n",
      "\tExploring Subfields of statistics on Wikipedia\n",
      "\tExploring Statistics profession and organizations on Wikipedia\n",
      "\tExploring Statistics-related lists on Wikipedia\n",
      "\tExploring Statisticians on Wikipedia\n",
      "\tExploring Data scientists on Wikipedia\n",
      "\tExploring Unsupervised learning on Wikipedia\n",
      "\tExploring Support vector machines on Wikipedia\n",
      "\tExploring Supervised learning on Wikipedia\n",
      "\tExploring Structured prediction on Wikipedia\n",
      "\tExploring Statistical natural language processing on Wikipedia\n",
      "\tExploring Semisupervised learning on Wikipedia\n",
      "\tExploring Natural language processing researchers on Wikipedia\n",
      "\tExploring Machine learning researchers on Wikipedia\n",
      "\tExploring Reinforcement learning on Wikipedia\n",
      "\tExploring Ontology learning (computer science) on Wikipedia\n",
      "\tExploring Markov models on Wikipedia\n",
      "\tExploring Machine learning task on Wikipedia\n",
      "\tExploring Machine learning algorithms on Wikipedia\n",
      "\tExploring Loss functions on Wikipedia\n",
      "\tExploring Log-linear models on Wikipedia\n",
      "\tExploring Learning in computer vision on Wikipedia\n",
      "\tExploring Latent variable models on Wikipedia\n",
      "\tExploring Kernel methods for machine learning on Wikipedia\n",
      "\tExploring Inductive logic programming on Wikipedia\n",
      "\tExploring Genetic programming on Wikipedia\n",
      "\tExploring Evolutionary algorithms on Wikipedia\n",
      "\tExploring Ensemble learning on Wikipedia\n",
      "\tExploring Dimension reduction on Wikipedia\n",
      "\tExploring Datasets in machine learning on Wikipedia\n",
      "\tExploring Data mining and machine learning software on Wikipedia\n",
      "\tExploring Signal processing conferences on Wikipedia\n",
      "\tExploring Artificial intelligence conferences on Wikipedia\n",
      "\tExploring Computational learning theory on Wikipedia\n",
      "\tExploring Cluster analysis on Wikipedia\n",
      "\tExploring Classification algorithms on Wikipedia\n",
      "\tExploring Blockmodeling on Wikipedia\n",
      "\tExploring Bayesian networks on Wikipedia\n",
      "\tExploring Artificial neural networks on Wikipedia\n",
      "\tExploring Applied machine learning on Wikipedia\n",
      "- Processing Wikipedia pages:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3448/3448 [22:26<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16232 Wikipedia pages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\"Machine_learning\", \"Data_science\", \"Statistics\", \"Deep_learning\", \"Artificial_intelligence\"]\n",
    "extracted_texts = get_wikipedia_pages(categories)\n",
    "print(\"Found\", len(extracted_texts), \"Wikipedia pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_documents = [Document(text=extracted_text, doc_id=str(i)) for i, extracted_text in enumerate(extracted_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"../DB/wiki\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_wiki\")\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16232/16232 [00:08<00:00, 1830.75it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 175.13it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:10<00:00, 188.60it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 175.40it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 177.25it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:10<00:00, 202.03it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:09<00:00, 209.87it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 185.12it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:11<00:00, 182.42it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 197.81it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    wiki_documents, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Loading from vector store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "import torch\n",
    "\n",
    "\n",
    "Settings.llm = None # Set this to none to make the index only do retrieval\n",
    "device_type = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\", cache_folder=\"../models\", device=device_type) # must be the same as the previous stage\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"../DB/wiki\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"gemma_assistant_wiki\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# load the vectorstore\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_science_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "Outline of regression analysis : Regression analysis Linear regression\n",
      "\n",
      "Regression diagnostic : Regression diagnostics have often been developed or were initially proposed in the context of linear regression or, more particularly, ordinary least squares. This means that many formally defined diagnostics are only available for these contexts.\n",
      "\n",
      "Linear predictor function : In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as \"weights\".\n",
      "\n",
      "Outline of regression analysis : General linear model Ordinary least squares Generalized least squares Simple linear regression Trend estimation Ridge regression Polynomial regression Segmented regression Nonlinear regression\n",
      "\n",
      "Outline of regression analysis : Least squares Linear least squares (mathematics) Non-linear least squares Least absolute deviations Curve fitting Smoothing Cross-sectional study\n",
      "\n",
      "Partial least squares regression : Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between 2 matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized). Partial least squares was introduced by the Swedish statistician Herman O. A. Wold, who then developed it with his son, Svante Wold. An alternative term for PLS is projection to latent structures, but the term partial least squares is still dominant in many areas. Although the original applications were in the social sciences, PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience, and anthropology.\n",
      "\n",
      "Linear predictor function : Linear model Linear regression == References ==\n",
      "\n",
      "Outline of regression analysis : The following outline is provided as an overview of and topical guide to regression analysis: Regression analysis â€“ use of statistical techniques for learning about the relationship between one or more dependent variables (Y) and one or more independent variables (X).\n",
      "\n",
      "Ecological regression : Ecological regression is a statistical technique which runs regression on aggregates, often used in political science and history to estimate group voting behavior from aggregate data.For example, if counties have a known Democratic vote (in percentage) D, and a known percentage of Catholics, C, then running a linear regression of dependent variable D against independent variable C will give D = a + bC. If the regression gives D = .22 + .45C for example, then the estimated Catholic vote (C = 1) is 67% Democratic and the non-Catholic vote (C = 0) is 22% Democratic. The technique has been often used in litigation brought under the Voting Rights Act of 1965 to see how blacks and whites voted.\n",
      "\n",
      "Proper linear model : In statistics, a proper linear model is a linear regression model in which the weights given to the predictor variables are chosen in such a way as to optimize the relationship between the prediction and the criterion. Simple regression analysis is the most common example of a proper linear model. Unit-weighted regression is the most common example of an improper linear model.\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What is linear regression\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(data_science_query_engine.query(\"What is linear regression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Python Code Assistant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Define a code intepreter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import regex\n",
    "import pickle\n",
    "import traceback\n",
    "import copy\n",
    "import datetime\n",
    "import dateutil.relativedelta\n",
    "import multiprocess\n",
    "from multiprocess import Pool\n",
    "from typing import Any, Dict, Optional\n",
    "from pebble import ProcessPool\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import TimeoutError\n",
    "from functools import partial\n",
    "from timeout_decorator import timeout\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "class GenericRuntime:\n",
    "    GLOBAL_DICT = {}\n",
    "    LOCAL_DICT = None\n",
    "    HEADERS = []\n",
    "    def __init__(self):\n",
    "        self._global_vars = copy.copy(self.GLOBAL_DICT)\n",
    "        self._local_vars = copy.copy(self.LOCAL_DICT) if self.LOCAL_DICT else None\n",
    "\n",
    "        for c in self.HEADERS:\n",
    "            self.exec_code(c)\n",
    "\n",
    "    def exec_code(self, code_piece: str) -> None:\n",
    "        if regex.search(r'(\\s|^)?input\\(', code_piece) or regex.search(r'(\\s|^)?os.system\\(', code_piece):\n",
    "            raise RuntimeError()\n",
    "        exec(code_piece, self._global_vars)\n",
    "        \n",
    "    def eval_code(self, expr: str) -> Any:\n",
    "        return eval(expr, self._global_vars)\n",
    "    \n",
    "    def inject(self, var_dict: Dict[str, Any]) -> None:\n",
    "        for k, v in var_dict.items():\n",
    "            self._global_vars[k] = v\n",
    "    \n",
    "    @property\n",
    "    def answer(self):\n",
    "        return self._global_vars['answer']\n",
    "\n",
    "class DateRuntime(GenericRuntime):\n",
    "    GLOBAL_DICT = {\n",
    "        'datetime': datetime.datetime, \n",
    "        'timedelta': dateutil.relativedelta.relativedelta,\n",
    "        'relativedelta': dateutil.relativedelta.relativedelta\n",
    "    }\n",
    "\n",
    "\n",
    "class CustomDict(dict):\n",
    "    def __iter__(self):\n",
    "        return list(super().__iter__()).__iter__()\n",
    "\n",
    "class ColorObjectRuntime(GenericRuntime):\n",
    "    GLOBAL_DICT = {'dict': CustomDict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hello world!', 'Done')\n"
     ]
    }
   ],
   "source": [
    "class PythonExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        runtime: Optional[Any] = None,\n",
    "        get_answer_symbol: Optional[str] = None,\n",
    "        get_answer_expr: Optional[str] = None,\n",
    "        get_answer_from_stdout: bool = False,\n",
    "        timeout_length: int = 5,\n",
    "    ) -> None:\n",
    "        self.runtime = runtime if runtime else GenericRuntime()\n",
    "        self.answer_symbol = get_answer_symbol\n",
    "        self.answer_expr = get_answer_expr\n",
    "        self.get_answer_from_stdout = get_answer_from_stdout\n",
    "        self.pool = Pool(multiprocess.cpu_count())\n",
    "        self.timeout_length = timeout_length\n",
    "\n",
    "    def process_generation_to_code(self, gens: str):\n",
    "        return [g.split('\\n') for g in gens]\n",
    "\n",
    "    @staticmethod\n",
    "    def execute(\n",
    "        code,\n",
    "        get_answer_from_stdout = None,\n",
    "        runtime = None,\n",
    "        answer_symbol = None,\n",
    "        answer_expr = None,\n",
    "        timeout_length = 10,\n",
    "    ):\n",
    "        try:\n",
    "            if get_answer_from_stdout:\n",
    "                program_io = io.StringIO()\n",
    "                with redirect_stdout(program_io):\n",
    "                    timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                program_io.seek(0)\n",
    "                result = program_io.read()\n",
    "            elif answer_symbol:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                result = runtime._global_vars[answer_symbol]\n",
    "            elif answer_expr:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code))\n",
    "                result = timeout(timeout_length)(runtime.eval_code)(answer_expr)\n",
    "            else:\n",
    "                timeout(timeout_length)(runtime.exec_code)('\\n'.join(code[:-1]))\n",
    "                result = timeout(timeout_length)(runtime.eval_code)(code[-1])\n",
    "            report = \"Done\"\n",
    "            str(result)\n",
    "            pickle.dumps(result) # serialization check\n",
    "        except:\n",
    "            result = ''\n",
    "            report = traceback.format_exc().split('\\n')[-2]\n",
    "        return result, report\n",
    "\n",
    "    def apply(self, code):\n",
    "        return self.batch_apply([code])[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate(s, max_length=400):\n",
    "        half = max_length // 2\n",
    "        if len(s) > max_length:\n",
    "            s = s[:half] + \"...\" + s[-half:]\n",
    "        return s\n",
    "\n",
    "    def batch_apply(self, batch_code):\n",
    "        all_code_snippets = self.process_generation_to_code(batch_code)\n",
    "\n",
    "        timeout_cnt = 0\n",
    "        all_exec_results = []\n",
    "        with ProcessPool(max_workers=min(len(all_code_snippets), os.cpu_count())) as pool:\n",
    "            executor = partial(\n",
    "                self.execute,\n",
    "                get_answer_from_stdout=self.get_answer_from_stdout,\n",
    "                runtime=self.runtime,\n",
    "                answer_symbol=self.answer_symbol,\n",
    "                answer_expr=self.answer_expr,\n",
    "                timeout_length=self.timeout_length, # this timeout not work\n",
    "            )\n",
    "            future = pool.map(executor, all_code_snippets, timeout=self.timeout_length)\n",
    "            iterator = future.result()\n",
    "\n",
    "            if len(all_code_snippets) > 100:  \n",
    "                progress_bar = tqdm(total=len(all_code_snippets), desc=\"Execute\")  \n",
    "            else:  \n",
    "                progress_bar = None \n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    result = next(iterator)\n",
    "                    all_exec_results.append(result)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except TimeoutError as error:\n",
    "                    print(error)\n",
    "                    all_exec_results.append((\"\", \"Timeout Error\"))\n",
    "                    timeout_cnt += 1\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    exit()\n",
    "                if progress_bar is not None:\n",
    "                    progress_bar.update(1) \n",
    "            \n",
    "            if progress_bar is not None:\n",
    "                progress_bar.close() \n",
    "\n",
    "        batch_results = []\n",
    "        for code, (res, report) in zip(all_code_snippets, all_exec_results):\n",
    "            # post processing\n",
    "            res, report = str(res).strip(), str(report).strip()\n",
    "            res, report = self.truncate(res), self.truncate(report)\n",
    "            batch_results.append((res, report))\n",
    "        return batch_results\n",
    "\n",
    "\n",
    "def test():\n",
    "    batch_code = [\n",
    "\"\"\"\n",
    "print(\"Hello world!\")\n",
    "\"\"\"\n",
    "    ]\n",
    "\n",
    "    executor = PythonExecutor(get_answer_from_stdout=True)\n",
    "    predictions = executor.apply(batch_code[0])\n",
    "    print(predictions)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Combine all of them together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Define Router Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"unsloth/gemma-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "#     cache_dir = \"../models\",\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model) \n",
    "\n",
    "# llm = HuggingFaceLLM(model=model, tokenizer=tokenizer)\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", api_key=\"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\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert output to JSON: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m ds_tool \u001b[38;5;241m=\u001b[39m QueryEngineTool\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[1;32m      6\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39mdata_science_query_engine,\n\u001b[1;32m      7\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUseful for answering data science concepts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m     11\u001b[0m     selector\u001b[38;5;241m=\u001b[39mLLMSingleSelector\u001b[38;5;241m.\u001b[39mfrom_defaults(),\n\u001b[1;32m     12\u001b[0m     query_engine_tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is linear regression?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:211\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/base/base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/query_engine/router_query_engine.py:169\u001b[0m, in \u001b[0;36mRouterQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    167\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    168\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[0;32m--> 169\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39minds) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    172\u001b[0m             responses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/base/base_selector.py:87\u001b[0m, in \u001b[0;36mBaseSelector.select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m     85\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [_wrap_choice(choice) \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m choices]\n\u001b[1;32m     86\u001b[0m query_bundle \u001b[38;5;241m=\u001b[39m _wrap_query(query)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/selectors/llm_selectors.py:118\u001b[0m, in \u001b[0;36mLLMSingleSelector._select\u001b[0;34m(self, choices, query)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# parse output\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m parse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _structured_output_to_selector_result(parse)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/llama_index/core/output_parsers/selection.py:97\u001b[0m, in \u001b[0;36mSelectionOutputParser.parse\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m     94\u001b[0m     json_obj \u001b[38;5;241m=\u001b[39m [json_obj]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_obj:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert output to JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m json_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(json_obj)\n\u001b[1;32m    100\u001b[0m answers \u001b[38;5;241m=\u001b[39m [Answer\u001b[38;5;241m.\u001b[39mfrom_dict(json_dict) \u001b[38;5;28;01mfor\u001b[39;00m json_dict \u001b[38;5;129;01min\u001b[39;00m json_output]\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert output to JSON: ''"
     ]
    }
   ],
   "source": [
    "paper_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=paper_query_engine,\n",
    "    description=\"Useful for search for papers\",\n",
    ")\n",
    "ds_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_science_query_engine,\n",
    "    description=\"Useful for answering data science concepts\",\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        paper_tool,\n",
    "        ds_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "print(query_engine.query(\"What is linear regression?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
